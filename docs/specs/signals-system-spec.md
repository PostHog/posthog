# Signals System Specification

## Overview

The signals system allows PostHog products to emit "signals" (e.g., "experiment reached significance") that get clustered together. When sufficient related signals accumulate, a **Report** is created and a deep research run is triggered via the AI chat agent to analyze and summarize the findings.

## Core Concepts

### Signal (ClickHouse only)

Signals are stored in the `document_embeddings` table in ClickHouse - NOT in Django. They use the existing embedding infrastructure.

**Storage Schema** (in `document_embeddings`):
- `team_id`: int
- `product`: "signals"
- `document_type`: "signal"
- `document_id`: UUID of the signal
- `model_name`: embedding model used
- `rendering`: "plain"
- `timestamp`: when the signal was created
- `content`: plaintext description of the signal (what gets embedded)
- `metadata`: JSON containing:
  - `source_product`: string (e.g., "experiments", "feature_flags", "web_analytics")
  - `source_type`: string (e.g., "experiment_significance", "funnel_drop_off")
  - `source_id`: string (e.g., experiment ID)
  - `weight`: float (0.0-1.0) - importance/confidence of the signal
  - `report_id`: UUID | null - linked report once assigned
  - `extra`: dict - product-specific metadata

### Report (Django)

Reports aggregate signals and track the lifecycle of research runs.

**Lifecycle States:**
```
potential → candidate → in_progress → ready
    │           │           │
    └───────────┴───────────┴──→ failed (terminal)
```

- **potential**: Report exists, collecting signals, below weight threshold
- **candidate**: Weight threshold met, waiting in queue for research run (may wait hours/days)
- **in_progress**: Research run actively processing
- **ready**: Research complete, report available
- **failed**: Research run failed (can be retried)

### ReportArtefact (Django)

Simple artifact storage for report outputs.

## Django Models

### Report

```python
from django.contrib.postgres.fields import ArrayField

class Report(UUIDModel):
    class Status(models.TextChoices):
        POTENTIAL = "potential"
        CANDIDATE = "candidate"
        IN_PROGRESS = "in_progress"
        READY = "ready"
        FAILED = "failed"

    team = models.ForeignKey(Team, on_delete=models.CASCADE)
    status = models.CharField(max_length=20, choices=Status.choices, default=Status.POTENTIAL)
    
    # Centroid for clustering - vector embedding (mean of all signal embeddings)
    # Updated incrementally: new_centroid = old_centroid + (new_embedding - old_centroid) / new_count
    # This is mathematically equivalent to the mean but avoids storing all embeddings
    centroid = ArrayField(models.FloatField(), help_text="Embedding centroid vector for signal clustering")
    
    # Aggregated weight from all signals (sum of signal weights)
    total_weight = models.FloatField(default=0.0)
    # Number of signals in this report (used for centroid calculation)
    signal_count = models.IntegerField(default=0)
    
    # Research run tracking
    conversation = models.ForeignKey("ee.Conversation", null=True, on_delete=models.SET_NULL)
    # How many signals were in the report when research was last kicked off
    # Used to detect if new signals arrived since last run
    signals_at_run = models.IntegerField(default=0)
    
    # Metadata - title and summary are generated by the LLM during the matching step
    # when it determines whether this is a novel signal cluster or part of an existing report
    title = models.TextField(null=True, blank=True)
    summary = models.TextField(null=True, blank=True)
    # Error message if research failed - shown to user and used for debugging
    error = models.TextField(null=True, blank=True)
    
    # Timestamps
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    promoted_at = models.DateTimeField(null=True, help_text="When moved to candidate")
    last_run_at = models.DateTimeField(null=True, help_text="When research run was last kicked off")
```

### ReportArtefact

```python
# TODO: Consider S3 storage for large artefacts. Options:
# 1. Store content inline (current approach) - simple, works for small artefacts
# 2. Store in S3 with URL reference - better for large files
# 3. Support both with a `storage_type` field ("inline" | "s3") and `s3_key` field
# For now, using inline storage. Revisit if artefacts become large.

class ReportArtefact(UUIDModel):
    team = models.ForeignKey(Team, on_delete=models.CASCADE)
    report = models.ForeignKey(Report, on_delete=models.CASCADE, related_name="artefacts")
    type = models.CharField(max_length=100)  # e.g., "notebook", "visualization", "summary"
    content = models.BinaryField()
    created_at = models.DateTimeField(auto_now_add=True)
```

## Signal Emission Interface

Products emit signals via a simple async function:

```python
async def emit_signal(
    team_id: int,
    source_product: str,      # e.g., "experiments"
    source_type: str,         # e.g., "experiment_significance"
    source_id: str,           # e.g., experiment UUID
    description: str,         # plaintext, will be embedded
    weight: float = 0.5,      # 0.0-1.0
    extra: dict | None = None # product-specific metadata
) -> str:  # returns signal UUID
```

This is a **Temporal workflow** (`workflow_id = f"{team_id}:{source_type}:{source_id}"` for idempotency). The flow:
1. Generates UUID for signal
2. Calls `generate_embedding()` synchronously to get embedding in-memory for immediate use
3. Proceeds with matching/assignment using the in-memory embedding
4. **After** matching completes and we know the `report_id`, emits to `KAFKA_DOCUMENT_EMBEDDINGS_INPUT_TOPIC` for async ClickHouse storage (so metadata includes correct `report_id`)

## Signal Processing Flow

### 1. Signal Ingestion

When a new signal arrives:

```python
async def process_new_signal(
    team_id: int,
    signal_id: str,
    embedding: list[float],
    weight: float,
    metadata: dict
) -> Report | None:
```

### 2. Report Matching

Find existing reports whose centroid is close enough:

```python
async def find_matching_report(
    team_id: int,
    signal: SignalData,  # full signal including description, embedding, metadata, etc.
) -> Report | None:
```

**LLM-based matching:**

Rather than using a naive cosine similarity threshold, we use an LLM to determine if a new signal belongs to an existing report. This is a one-shot call, not agentic. **The LLM always runs**, even if there are zero candidates - it handles both "match to existing" and "create new report" cases.

1. **ClickHouse lookup**: Find the 10 nearest signals that already have a report
   - Query `document_embeddings FINAL` for signals WITH a `report_id` set
   - Order by `cosineDistance(embedding, query_embedding)` ascending
   - Return top 10 candidates (may be empty)

2. **LLM decides**: Pass candidates (possibly empty) + the new signal to an LLM
   - Ask: "Which of these signals (if any) is describing the same thing as the new signal?"
   - **If match found**: LLM returns the matching signal's index → use that signal's report
   - **If no match**: LLM returns "none" + generates a title/summary for a new report

> **Note**: We currently assume no ingestion lag between the workflow and ClickHouse. In the future, we'll add a constraint that only one "new signal" workflow can run per team at a time, which will eliminate race conditions where duplicate reports could be created due to lag. For now, we accept this limitation.

```python
async def find_matching_report(
    team_id: int,
    signal: SignalData,  # full signal including description, embedding, metadata, etc.
) -> tuple[Report | None, str, str]:
    """
    Returns: (matching_report, title, summary)
    - If match found: (report, existing_title, existing_summary)
    - If no match: (None, new_title, new_summary) for creating a new report
    """
    # 1. ClickHouse: get 10 nearest signals WITH reports
    candidate_signals = await get_nearest_assigned_signals_ch(team_id, signal.embedding, limit=10)
    
    # 2. LLM decides - always runs, even with empty candidates
    match, title, summary = await llm_match_signal(signal, candidate_signals)
    
    if match:
        report = await Report.objects.aget(id=match.report_id)
        return (report, report.title, report.summary)
    
    return (None, title, summary)


async def llm_match_signal(
    signal: SignalData,
    candidates: list[SignalCandidate]  # may be empty
) -> tuple[SignalCandidate | None, str, str]:
    """
    One-shot LLM call to determine if new signal matches any candidate.
    Always runs, even with empty candidates list.
    
    Returns:
    - If match found: (matching_candidate, "", "") - will use existing report's title/summary
    - If no match: (None, new_title, new_summary) - for creating a new report
    """
    pass
```

### 3. Report Creation/Update

```python
async def assign_signal_to_report(
    team_id: int,
    signal_id: str,
    embedding: list[float],
    weight: float,
    metadata: dict,
    existing_report: Report | None
) -> Report:
```

If no matching report:
- Create new `Report` with `status=potential`
- Set `centroid = embedding` (first signal IS the centroid)
- Set `signal_count = 1`, `total_weight = weight`

If matching report:
- Update signal's metadata with `report_id` in ClickHouse
- **Update centroid using incremental mean formula:**
  ```python
  # Mathematically equivalent to mean of all embeddings, but O(1) space
  new_count = report.signal_count + 1
  report.centroid = [
      old_c + (new_e - old_c) / new_count
      for old_c, new_e in zip(report.centroid, embedding)
  ]
  report.signal_count = new_count
  ```
- Add signal weight to `total_weight`
- **Run promotion check** (see below)

### 4. Promotion Check

```python
async def check_report_promotion(
    report: Report,
    weight_threshold: float = 1.0  # configurable
) -> bool:
```

If `total_weight >= weight_threshold` and `status == potential`:
- Promote to `candidate`
- Set `promoted_at = now()`

## Research Run Integration

### Triggering Research

Candidate reports are processed by a **cron job** that runs continuously (e.g., every 5 minutes):

```python
# Environment variable for configuration
SIGNAL_REPORT_CANDIDATE_WAIT_HOURS = float(os.getenv("SIGNAL_REPORT_CANDIDATE_WAIT_HOURS", "3"))

async def process_candidate_reports() -> list[Report]:
    """
    Cron job to find and process candidate reports.
    Run every 5 minutes via Celery beat.
    """
```

Finds reports where:
- `status == candidate`
- `promoted_at < now() - SIGNAL_REPORT_CANDIDATE_WAIT_HOURS`

For each, triggers research run and sets:
- `status = in_progress`
- `last_run_at = now()`
- `signals_at_run = signal_count`

### Research Workflow

Uses existing `ChatAgentWorkflow` infrastructure:

```python
@dataclass
class SignalResearchWorkflowInputs:
    team_id: int
    report_id: UUID
    signal_ids: list[str]  # fetched from document_embeddings
```

The workflow:
1. Creates a new `Conversation` with `type=DEEP_RESEARCH`
2. Constructs a research prompt from signal descriptions and metadata
3. Runs the chat agent with research-focused system prompt
4. Stores results in `ReportArtefact`
5. Updates `Report.status = ready`

### Temporal Workflow

```python
@workflow.defn(name="signal-research")
class SignalResearchWorkflow(AgentBaseWorkflow):
    @workflow.run
    async def run(self, inputs: SignalResearchWorkflowInputs) -> None:
        # 1. Fetch signals from ClickHouse
        # 2. Build research context
        # 3. Execute chat agent activity
        # 4. Store artifacts
        # 5. Update report status
```

## ClickHouse Queries

Note: Always use `FINAL` modifier for signal queries to handle ReplacingMergeTree deduplication.

### Find Nearest Assigned Signals (for LLM matching)

```sql
SELECT 
    document_id,
    content,
    JSONExtractString(metadata, 'report_id') as report_id,
    JSONExtractString(metadata, 'source_product') as source_product,
    JSONExtractString(metadata, 'source_type') as source_type,
    cosineDistance(embedding, {query_embedding}) as distance
FROM document_embeddings FINAL
WHERE team_id = {team_id}
  AND product = 'signals'
  AND document_type = 'signal'
  AND JSONExtractString(metadata, 'report_id') != ''  -- only signals already assigned to a report
ORDER BY distance ASC
LIMIT 10
```

### Fetch Signals for Report (use FINAL)

```sql
SELECT 
    document_id,
    content,
    metadata,
    timestamp
FROM document_embeddings FINAL
WHERE team_id = {team_id}
  AND product = 'signals'
  AND document_type = 'signal'
  AND JSONExtractString(metadata, 'report_id') = {report_id}
ORDER BY timestamp ASC
```

## Configuration

```python
# Environment variables with defaults
SIGNAL_WEIGHT_THRESHOLD = float(os.getenv("SIGNAL_WEIGHT_THRESHOLD", "1.0"))
SIGNAL_REPORT_CANDIDATE_WAIT_HOURS = float(os.getenv("SIGNAL_REPORT_CANDIDATE_WAIT_HOURS", "3"))
SIGNAL_EMBEDDING_MODEL = os.getenv("SIGNAL_EMBEDDING_MODEL", "text-embedding-3-small-1536")
SIGNAL_MATCHING_LLM_MODEL = os.getenv("SIGNAL_MATCHING_LLM_MODEL", "gpt-4o-mini")  # for signal/report matching
```

## Example Usage

### Experiments Product

```python
# In experiments/signals.py
async def on_experiment_significance(experiment: Experiment, variant: str, p_value: float):
    await emit_signal(
        team_id=experiment.team_id,
        source_product="experiments",
        source_type="significance_reached",
        source_id=str(experiment.id),
        description=f"Experiment '{experiment.name}' reached statistical significance. "
                    f"Variant '{variant}' shows p-value of {p_value:.4f}. "
                    f"Goal: {experiment.goal_description}",
        weight=0.8 if p_value < 0.01 else 0.5,
        extra={
            "experiment_name": experiment.name,
            "variant": variant,
            "p_value": p_value,
        }
    )
```

### Web Analytics Product

```python
async def on_significant_traffic_change(team_id: int, page: str, change_pct: float):
    await emit_signal(
        team_id=team_id,
        source_product="web_analytics",
        source_type="traffic_anomaly",
        source_id=f"page:{page}",
        description=f"Significant traffic change detected on '{page}': {change_pct:+.1f}% "
                    f"compared to baseline.",
        weight=min(abs(change_pct) / 100, 1.0),
        extra={"page": page, "change_pct": change_pct}
    )
```

## Open Questions

(None remaining - all resolved in Q&A below)

## Q&A / Decisions

**Q: Should potential reports expire if they never reach the weight threshold?**

A: No - reports should never expire. The associated signals will eventually be deleted from ClickHouse after 3 months (per the TTL), but we keep the report around so it can match and accumulate new signals that come along later. The report's centroid serves as a "cluster anchor" for future related signals.

**Q: Should we deduplicate signals from the same source (e.g., experiment emits significance signal twice with slightly different p-values)?**

A: No deduplication - let signals stack. Repeated signals from the same source add to the weight of the report, which is the desired behavior. If something keeps firing signals, that's a stronger indication it deserves research attention.

**Q: If a report is `ready` (research complete) and new related signals come in, what happens?**

A: New signals should still be added to sufficiently similar reports regardless of status (including `ready`). The UI will display these new signals, and users will have the option to manually re-run the research if they want updated analysis. No automatic re-run.

**Q: Should we auto-retry failed reports?**

A: No - leave failed reports for manual intervention. The `error` field captures what went wrong for debugging, and users can manually trigger a re-run when ready.

**Q: How/when should users be notified about ready reports?**

A: Out of scope for v1. We'll revisit notifications (in-app, email, Slack, etc.) later.

**Q: How do we construct the research prompt from multiple signals?**

A: The agent receives the report UUID and uses a tool to fetch all related signals. The tool queries the `document_embeddings` table using the metadata column to select all signals where `metadata.report_id` matches. This lets the agent pull signal data as needed rather than stuffing everything into the initial prompt.

**Q: How does the LLM matching step work for clustering signals?**

A: The LLM is given the cosine distances between the new signal and all candidate signals, along with their sources and descriptions. It's instructed that the *meaning* of the signals must be very similar, not just the category. For example, two "experiment significance" signals from different experiments should NOT match - they need to be about the same underlying thing. Exact prompt to be determined during implementation.

**Q: What's the expected range/meaning for signal weights?**

A: Weights are 0.0-1.0, and the threshold for promotion to candidate is 1.0. The semantics: a weight of 1.0 means "this alone should trigger research." Product teams calibrate accordingly:
- Experiments reaching significance: emit weight ~1.0 (always want research)
- New error tracking exception: emit weight ~0.1 (needs to stack up before research)
- Error frequency spike: might emit 0.5-1.0 depending on severity

Guidance to product teams: "If your signal hits weight 1.0, it becomes a candidate for research."

## Implementation Plan

### Phase 1: Django Modelling

- [ ] Create `Report` model with all fields (status, centroid, weights, timestamps, etc.)
- [ ] Create `ReportArtefact` model (uuid, team, type, content)
- [ ] Create and run migrations

### Phase 2: Signal Creation Flow

- [ ] Implement `emit_signal()` function - the interface products will call
- [ ] Integrate with embedding worker to generate embeddings
- [ ] Implement `get_nearest_assigned_signals_ch()` - ClickHouse query for similar signals
- [ ] Implement `llm_match_signal()` - one-shot LLM matching
- [ ] Implement `find_matching_report()` - orchestrates the two-phase lookup
- [ ] Implement `assign_signal_to_report()` - creates or updates report, updates centroid
- [ ] Implement `check_report_promotion()` - promotes to candidate if weight >= 1.0

### Phase 3: Research Workflow

- [ ] Create Celery beat task to find candidate reports ready for processing
- [ ] Create `SignalResearchWorkflow` Temporal workflow
- [ ] Implement tool for agent to fetch signals by report ID
- [ ] Wire up workflow to create Conversation with `type=DEEP_RESEARCH`
- [ ] Handle success: update report status to `ready`, store artefacts
- [ ] Handle failure: update report status to `failed`, store error

### Out of Scope (Future)

- UI for viewing reports and signals
- User notifications
- Manual re-run triggering
- Admin/management commands
- Research workflow details (to be specified during Phase 3 implementation)

## Dependencies

- Existing `document_embeddings` ClickHouse table
- Existing embedding worker API (`emit_embedding_request`)
- Existing `ChatAgentWorkflow` Temporal workflow
- Existing `Conversation` model