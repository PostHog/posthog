#
# Docker image for the LLM Analytics Temporal worker.
#
# Includes ML dependencies (transformers, ONNX Runtime, torch) that are too
# large (~2GB) for the shared PostHog image. These deps live in the `sentiment`
# dependency group in pyproject.toml and are installed via `uv sync --group sentiment`.
#

# Same as pyproject.toml so that uv can pick it up and doesn't need to download a different Python version.
FROM ghcr.io/astral-sh/uv:0.9.9 AS uv

FROM python:3.12.12-slim-bookworm
SHELL ["/bin/bash", "-e", "-o", "pipefail", "-c"]

# Copy uv
COPY --from=uv /uv /uvx /bin/

# Set working directory
WORKDIR /code

# uv settings for Docker builds
ENV UV_COMPILE_BYTECODE=1
ENV UV_LINK_MODE=copy
ENV UV_PROJECT_ENVIRONMENT=/python-runtime

# Install system dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    "build-essential" \
    "git" \
    "libpq-dev" \
    "libxmlsec1" \
    "libxmlsec1-dev" \
    "libffi-dev" \
    "zlib1g-dev" \
    "pkg-config" \
    && \
    rm -rf /var/lib/apt/lists/*

# Install Python dependencies including sentiment ML deps
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,source=uv.lock,target=uv.lock \
    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \
    uv sync --locked --no-dev --no-install-project --group sentiment --no-binary-package lxml --no-binary-package xmlsec

# Copy project files
COPY manage.py manage.py
COPY common/esbuilder common/esbuilder
COPY common/hogvm common/hogvm/
COPY common/migration_utils common/migration_utils/
COPY posthog posthog/
COPY products/ products/
COPY ee ee/
COPY bin/temporal-django-worker bin/temporal-django-worker

ENV PATH=/python-runtime/bin:$PATH \
    PYTHONPATH=/python-runtime \
    POSTHOG_SENTIMENT_MODEL_CACHE=/opt/posthog-sentiment-model

# Pre-bake the sentiment ONNX model so workers don't download from HuggingFace on startup.
# Downloads the PyTorch model, exports to ONNX (~60MB), saves tokenizer + model to disk.
RUN python -c "
import os, torch
from transformers import AutoTokenizer
from optimum.onnxruntime import ORTModelForSequenceClassification

model_name = 'cardiffnlp/twitter-roberta-base-sentiment-latest'
cache_dir = os.environ['POSTHOG_SENTIMENT_MODEL_CACHE']
os.makedirs(cache_dir, exist_ok=True)

tokenizer = AutoTokenizer.from_pretrained(model_name)

# PyTorch 2.9+ defaults torch.onnx.export to dynamo=True which breaks optimum
original_export = torch.onnx.export
def export_with_dynamo_disabled(*args, **kwargs):
    kwargs.setdefault('dynamo', False)
    return original_export(*args, **kwargs)
torch.onnx.export = export_with_dynamo_disabled

model = ORTModelForSequenceClassification.from_pretrained(model_name, export=True)
torch.onnx.export = original_export

model.save_pretrained(cache_dir)
tokenizer.save_pretrained(cache_dir)
print(f'Model baked into {cache_dir}')
"

RUN chmod +x bin/temporal-django-worker

CMD ["bin/temporal-django-worker"]
