"""
Evaluations for survey response summarization (Gemini-based).

Tests the quality of summaries generated by the summarization module,
not the Max AI survey analysis tool (which has separate evals).
"""

import pytest

from django.conf import settings

from autoevals.llm import LLMClassifier
from braintrust import EvalCase, Score

from products.surveys.backend.summarization import format_as_markdown, summarize_responses
from products.surveys.backend.summarization.llm.schema import SurveySummaryResponse

from ..base import MaxPublicEval


def _check_output_success(output, scorer_name: str) -> Score | None:
    """Common validation for scorer outputs. Returns Score on failure, None on success."""
    if not output.get("success", False):
        return Score(
            name=scorer_name,
            score=0,
            metadata={"reason": "Summarization failed", "error": output.get("error", "Unknown error")},
        )
    return None


class ThemeAccuracyScorer(LLMClassifier):
    """Evaluates whether extracted themes accurately represent the survey responses."""

    def __init__(self, **kwargs):
        super().__init__(
            name="theme_accuracy",
            prompt_template="""
Evaluate whether the extracted themes accurately represent the survey responses.

Survey Question: {{input.question}}

Survey Responses:
{{input.responses}}

Extracted Themes:
{{output.themes}}

Evaluation Criteria:
1. **Grounded**: Are all themes directly supported by actual responses?
2. **Complete**: Are the major patterns in responses captured?
3. **No Hallucination**: Are there any themes not supported by the data?
4. **Specificity**: Are themes specific enough to be useful, not generic?

Rate the theme accuracy:
- excellent: All themes grounded in data, captures all major patterns, no hallucination
- good: Themes mostly accurate, may miss minor patterns
- partial: Some accurate themes but missing major patterns or has minor hallucinations
- poor: Significant hallucination or misses most patterns
""".strip(),
            choice_scores={"excellent": 1.0, "good": 0.75, "partial": 0.4, "poor": 0.0},
            model="gpt-4.1",
            **kwargs,
        )

    async def _run_eval_async(self, output, expected=None, **kwargs):
        if error_score := _check_output_success(output, self._name()):
            return error_score
        return await super()._run_eval_async(output, expected, **kwargs)

    def _run_eval_sync(self, output, expected=None, **kwargs):
        if error_score := _check_output_success(output, self._name()):
            return error_score
        return super()._run_eval_sync(output, expected, **kwargs)


class FrequencyClassificationScorer(LLMClassifier):
    """Evaluates whether frequency labels (common/moderate/rare) are accurate."""

    def __init__(self, **kwargs):
        super().__init__(
            name="frequency_classification",
            prompt_template="""
Evaluate whether the frequency classifications (common, moderate, rare) are accurate.

Total Responses: {{input.response_count}}

Survey Responses:
{{input.responses}}

Themes with Frequencies:
{{output.themes}}

Frequency Guidelines:
- common: Theme appears in majority of responses (>40%)
- moderate: Theme appears in notable portion (15-40%)
- rare: Theme appears in few responses (<15%)

Evaluation Criteria:
1. Count how many responses support each theme
2. Check if frequency label matches the actual proportion
3. Consider that themes may overlap in responses

Rate the frequency accuracy:
- accurate: All frequency labels correctly reflect response proportions
- mostly_accurate: Minor frequency misclassifications
- inaccurate: Major frequency misclassifications
""".strip(),
            choice_scores={"accurate": 1.0, "mostly_accurate": 0.6, "inaccurate": 0.0},
            model="gpt-4.1",
            **kwargs,
        )

    async def _run_eval_async(self, output, expected=None, **kwargs):
        if error_score := _check_output_success(output, self._name()):
            return error_score
        return await super()._run_eval_async(output, expected, **kwargs)

    def _run_eval_sync(self, output, expected=None, **kwargs):
        if error_score := _check_output_success(output, self._name()):
            return error_score
        return super()._run_eval_sync(output, expected, **kwargs)


class InsightQualityScorer(LLMClassifier):
    """Evaluates whether the key insight is actionable and relevant."""

    def __init__(self, **kwargs):
        super().__init__(
            name="insight_quality",
            prompt_template="""
Evaluate the quality of the key insight generated from survey analysis.

Survey Question: {{input.question}}
Survey Responses: {{input.responses}}
Extracted Themes: {{output.themes}}
Key Insight: {{output.key_insight}}

Evaluation Criteria:
1. **Actionable**: Does the insight suggest concrete next steps?
2. **Relevant**: Is it based on the actual themes and responses?
3. **Valuable**: Would a product team find this insight useful?
4. **Not Generic**: Is it specific to this data, not boilerplate advice?

Rate the insight quality:
- excellent: Highly actionable, specific to the data, valuable for decision-making
- good: Actionable and relevant, could be slightly more specific
- adequate: Somewhat useful but generic or not clearly actionable
- poor: Generic boilerplate, not grounded in data, or not actionable
""".strip(),
            choice_scores={"excellent": 1.0, "good": 0.75, "adequate": 0.4, "poor": 0.0},
            model="gpt-4.1",
            **kwargs,
        )

    async def _run_eval_async(self, output, expected=None, **kwargs):
        if error_score := _check_output_success(output, self._name()):
            return error_score
        return await super()._run_eval_async(output, expected, **kwargs)

    def _run_eval_sync(self, output, expected=None, **kwargs):
        if error_score := _check_output_success(output, self._name()):
            return error_score
        return super()._run_eval_sync(output, expected, **kwargs)


class OverviewQualityScorer(LLMClassifier):
    """Evaluates whether the overview accurately summarizes sentiment and main points."""

    def __init__(self, **kwargs):
        super().__init__(
            name="overview_quality",
            prompt_template="""
Evaluate whether the overview accurately captures the overall sentiment and main points.

Survey Question: {{input.question}}
Survey Responses: {{input.responses}}
Overview: {{output.overview}}

Evaluation Criteria:
1. **Sentiment Accuracy**: Does it correctly characterize overall sentiment (positive/negative/mixed)?
2. **Main Points**: Does it mention the most important patterns?
3. **Conciseness**: Is it appropriately brief while covering key points?
4. **No Hallucination**: Is everything stated supported by the responses?

Rate the overview quality:
- excellent: Accurate sentiment, covers main points, concise, no hallucination
- good: Mostly accurate with minor omissions
- adequate: Captures some points but misses sentiment or major patterns
- poor: Inaccurate sentiment, hallucinations, or misses most content
""".strip(),
            choice_scores={"excellent": 1.0, "good": 0.75, "adequate": 0.4, "poor": 0.0},
            model="gpt-4.1",
            **kwargs,
        )

    async def _run_eval_async(self, output, expected=None, **kwargs):
        if error_score := _check_output_success(output, self._name()):
            return error_score
        return await super()._run_eval_async(output, expected, **kwargs)

    def _run_eval_sync(self, output, expected=None, **kwargs):
        if error_score := _check_output_success(output, self._name()):
            return error_score
        return super()._run_eval_sync(output, expected, **kwargs)


async def run_summarization(input_data: dict) -> dict:
    """Task function that runs the summarization and returns structured output."""
    # Check for Gemini API key
    if not settings.GEMINI_API_KEY:
        return {"success": False, "error": "GEMINI_API_KEY not configured"}

    try:
        result: SurveySummaryResponse = summarize_responses(
            question_text=input_data["question"],
            responses=input_data["responses"],
        )
        return {
            "success": True,
            "overview": result.overview,
            "themes": [
                {"theme": t.theme, "description": t.description, "frequency": t.frequency} for t in result.themes
            ],
            "key_insight": result.key_insight,
            "markdown": format_as_markdown(result),
        }
    except Exception as e:
        return {"success": False, "error": str(e)}


# Test datasets
POSITIVE_FEEDBACK_RESPONSES = [
    "Love the new dashboard - so much cleaner and faster",
    "The UI improvements are fantastic, much easier to navigate now",
    "Great performance boost in the latest update",
    "Dashboard loads instantly now, huge improvement",
    "Navigation is intuitive, found everything I needed quickly",
    "The redesign looks professional and modern",
    "Loading times are amazing compared to before",
    "Clean interface, easy to use even for new users",
    "Performance has been rock solid lately",
    "Love how responsive everything feels now",
]

NEGATIVE_FEEDBACK_RESPONSES = [
    "App crashes every time I try to export data",
    "Export feature is completely broken for large datasets",
    "Lost my work twice due to crashes during export",
    "Performance is terrible when working with big files",
    "The export crashes are making this unusable for our team",
    "Waiting 10+ minutes for exports that used to take seconds",
    "Stability issues are really hurting our workflow",
    "Can't rely on the export feature anymore",
    "Multiple crashes per day, very frustrating",
    "Export functionality needs urgent fixing",
]

MIXED_FEEDBACK_RESPONSES = [
    "Love the new features but the mobile app is buggy",
    "Great desktop experience, terrible on mobile",
    "Desktop version is excellent, mobile needs work",
    "Features are powerful but mobile crashes often",
    "Works great on my laptop, unusable on my phone",
    "New functionality is impressive, mobile support lacking",
    "Desktop UI is intuitive, mobile UX is confusing",
    "Powerful features, mobile responsiveness is poor",
]

FEATURE_REQUEST_RESPONSES = [
    "Need dark mode desperately, working late kills my eyes",
    "Please add dark mode, essential for night work",
    "Dark theme would be amazing for evening sessions",
    "Would love Slack integration for notifications",
    "Slack integration would save us so much time",
    "Need to connect this with our Slack workspace",
    "Dark mode + Slack integration are my top requests",
    "Better integrations needed, especially Slack",
]


@pytest.mark.django_db
async def eval_survey_summarization(set_up_evals, pytestconfig):
    """Evaluation for Gemini-based survey summarization quality."""
    await MaxPublicEval(
        experiment_name="survey_summarization",
        task=run_summarization,
        scores=[
            ThemeAccuracyScorer(),
            FrequencyClassificationScorer(),
            InsightQualityScorer(),
            OverviewQualityScorer(),
        ],
        data=[
            EvalCase(
                input={
                    "question": "What do you like about our product?",
                    "responses": POSITIVE_FEEDBACK_RESPONSES,
                    "response_count": len(POSITIVE_FEEDBACK_RESPONSES),
                },
                expected={
                    "sentiment": "positive",
                    "expected_themes": ["UI/Design", "Performance"],
                },
                metadata={"test_type": "positive_feedback"},
            ),
            EvalCase(
                input={
                    "question": "What issues have you experienced?",
                    "responses": NEGATIVE_FEEDBACK_RESPONSES,
                    "response_count": len(NEGATIVE_FEEDBACK_RESPONSES),
                },
                expected={
                    "sentiment": "negative",
                    "expected_themes": ["Export Issues", "Crashes/Stability", "Performance"],
                },
                metadata={"test_type": "negative_feedback"},
            ),
            EvalCase(
                input={
                    "question": "How would you describe your experience?",
                    "responses": MIXED_FEEDBACK_RESPONSES,
                    "response_count": len(MIXED_FEEDBACK_RESPONSES),
                },
                expected={
                    "sentiment": "mixed",
                    "expected_themes": ["Desktop Positive", "Mobile Issues"],
                },
                metadata={"test_type": "mixed_feedback"},
            ),
            EvalCase(
                input={
                    "question": "What features would you like to see?",
                    "responses": FEATURE_REQUEST_RESPONSES,
                    "response_count": len(FEATURE_REQUEST_RESPONSES),
                },
                expected={
                    "sentiment": "neutral",
                    "expected_themes": ["Dark Mode", "Slack Integration"],
                },
                metadata={"test_type": "feature_requests"},
            ),
        ],
        pytestconfig=pytestconfig,
    )
