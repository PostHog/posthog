import time
import uuid
from collections.abc import AsyncGenerator
from typing import cast

from django.conf import settings
from django.core.exceptions import ValidationError
from django.http import StreamingHttpResponse

import pydantic
import structlog
from asgiref.sync import async_to_sync as asgi_async_to_sync
from drf_spectacular.utils import extend_schema
from loginas.utils import is_impersonated_session
from prometheus_client import Histogram
from rest_framework import exceptions, serializers, status
from rest_framework.decorators import action
from rest_framework.exceptions import Throttled
from rest_framework.mixins import ListModelMixin, RetrieveModelMixin
from rest_framework.request import Request
from rest_framework.response import Response
from rest_framework.viewsets import GenericViewSet

from posthog.schema import AgentMode, AssistantMessage, HumanMessage, MaxBillingContext

from posthog.api.routing import TeamAndOrgViewSetMixin
from posthog.exceptions import Conflict, QuotaLimitExceeded
from posthog.exceptions_capture import capture_exception
from posthog.models.user import User
from posthog.rate_limit import (
    AIBurstRateThrottle,
    AIResearchBurstRateThrottle,
    AIResearchSustainedRateThrottle,
    AISustainedRateThrottle,
)
from posthog.temporal.ai.chat_agent import (
    CHAT_AGENT_STREAM_MAX_LENGTH,
    CHAT_AGENT_WORKFLOW_TIMEOUT,
    ChatAgentWorkflow,
    ChatAgentWorkflowInputs,
)
from posthog.temporal.ai.research_agent import (
    RESEARCH_AGENT_STREAM_MAX_LENGTH,
    RESEARCH_AGENT_WORKFLOW_TIMEOUT,
    ResearchAgentWorkflow,
    ResearchAgentWorkflowInputs,
)

from ee.billing.quota_limiting import QuotaLimitingCaches, QuotaResource, is_team_limited
from ee.hogai.api.serializers import ConversationSerializer
from ee.hogai.chat_agent import AssistantGraph
from ee.hogai.core.executor import AgentExecutor
from ee.hogai.queue import ConversationQueueMessage, ConversationQueueStore, QueueFullError, build_queue_message
from ee.hogai.stream.redis_stream import get_conversation_stream_key
from ee.hogai.utils.aio import async_to_sync
from ee.hogai.utils.sse import AssistantSSESerializer
from ee.hogai.utils.types import PartialAssistantState
from ee.models.assistant import Conversation

logger = structlog.get_logger(__name__)

RESEARCH_RATE_LIMIT_MESSAGE = (
    "You've reached the usage limit for Research mode, which is currently in beta "
    "with limited capacity. Please try again {retry_after}, or switch to a regular "
    "conversation for continued access."
)

STREAM_ITERATION_LATENCY_HISTOGRAM = Histogram(
    "posthog_ai_stream_iteration_latency_seconds",
    "Time between iterations in the async stream loop",
    buckets=[0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0, float("inf")],
)


class MessageMinimalSerializer(serializers.Serializer):
    """Serializer for appending a message to an existing conversation without triggering AI processing."""

    content = serializers.CharField(required=True, max_length=10000)


class MessageSerializer(MessageMinimalSerializer):
    content = serializers.CharField(
        required=True,
        allow_null=True,  # Null content means we're resuming streaming or continuing previous generation
        max_length=40000,  # Roughly 10k tokens
    )
    conversation = serializers.UUIDField(
        required=True
    )  # this either retrieves an existing conversation or creates a new one
    contextual_tools = serializers.DictField(required=False, child=serializers.JSONField())
    ui_context = serializers.JSONField(required=False)
    billing_context = serializers.JSONField(required=False)
    trace_id = serializers.UUIDField(required=True)
    session_id = serializers.CharField(required=False)
    agent_mode = serializers.ChoiceField(required=False, choices=[mode.value for mode in AgentMode])
    resume_payload = serializers.JSONField(required=False, allow_null=True)

    def validate(self, attrs):
        data = attrs
        if data["content"] is not None:
            try:
                message = HumanMessage.model_validate(
                    {
                        "content": data["content"],
                        "ui_context": data.get("ui_context"),
                        "trace_id": str(data["trace_id"]) if data.get("trace_id") else None,
                    }
                )
            except pydantic.ValidationError:
                if settings.DEBUG:
                    raise
                raise serializers.ValidationError("Invalid message content.")
            data["message"] = message
        else:
            # NOTE: If content is empty, it means we're resuming streaming or continuing generation with only the contextual_tools potentially different
            # Because we intentionally don't add a HumanMessage, we are NOT updating ui_context here
            data["message"] = None
        billing_context = data.get("billing_context")
        if billing_context:
            try:
                billing_context = MaxBillingContext.model_validate(billing_context)
                data["billing_context"] = billing_context
            except pydantic.ValidationError as e:
                capture_exception(e)
                # billing data relies on a lot of legacy code, this might break and we don't want to block the conversation
                data["billing_context"] = None
        if agent_mode := data.get("agent_mode"):
            try:
                data["agent_mode"] = AgentMode(agent_mode)
            except ValueError:
                raise serializers.ValidationError("Invalid agent mode.")
        return data


class QueueMessageSerializer(serializers.Serializer):
    content = serializers.CharField(required=True, allow_blank=False, max_length=40000)
    contextual_tools = serializers.DictField(required=False, child=serializers.JSONField())
    ui_context = serializers.JSONField(required=False)
    billing_context = serializers.JSONField(required=False)
    agent_mode = serializers.ChoiceField(required=False, choices=[mode.value for mode in AgentMode])

    def validate(self, attrs):
        data = attrs
        try:
            HumanMessage.model_validate(
                {
                    "content": data["content"],
                    "ui_context": data.get("ui_context"),
                }
            )
        except pydantic.ValidationError:
            raise serializers.ValidationError("Invalid message content.")

        billing_context = data.get("billing_context")
        if billing_context:
            try:
                parsed_context = MaxBillingContext.model_validate(billing_context)
                data["billing_context"] = parsed_context.model_dump()
            except pydantic.ValidationError as e:
                capture_exception(e)
                data["billing_context"] = None

        if agent_mode := data.get("agent_mode"):
            try:
                data["agent_mode"] = AgentMode(agent_mode).value
            except ValueError:
                raise serializers.ValidationError("Invalid agent mode.")

        return data


class QueueMessageUpdateSerializer(serializers.Serializer):
    content = serializers.CharField(required=True, allow_blank=False, max_length=40000)


@extend_schema(tags=["max"])
class ConversationViewSet(TeamAndOrgViewSetMixin, ListModelMixin, RetrieveModelMixin, GenericViewSet):
    scope_object = "conversation"
    serializer_class = ConversationSerializer
    queryset = Conversation.objects.all()
    lookup_url_kwarg = "conversation"

    def _queue_conversation_id(self) -> str:
        if not self.lookup_url_kwarg:
            raise exceptions.ValidationError("Conversation not provided")
        conversation_id = self.kwargs.get(self.lookup_url_kwarg)
        if not conversation_id:
            raise exceptions.ValidationError("Conversation not provided")
        return str(conversation_id)

    def _ensure_queue_access(self, request: Request, conversation_id: str) -> Response | None:
        try:
            conversation = Conversation.objects.get(id=conversation_id)
        except Conversation.DoesNotExist:
            return Response({"error": "Conversation not found"}, status=status.HTTP_404_NOT_FOUND)
        if conversation.user != request.user or conversation.team != self.team:
            return Response({"error": "Cannot access other users' conversations"}, status=status.HTTP_403_FORBIDDEN)
        return None

    def _queue_response(self, queue_store: ConversationQueueStore, queue: list[ConversationQueueMessage]) -> Response:
        return Response({"messages": queue, "max_queue_messages": queue_store.max_messages})

    def safely_get_queryset(self, queryset):
        # Only single retrieval of a specific conversation is allowed for other users' conversations (if ID known)
        if self.action != "retrieve":
            queryset = queryset.filter(user=self.request.user)
        # For listing or single retrieval, conversations must be from the assistant and have a title
        if self.action in ("list", "retrieve"):
            queryset = queryset.filter(
                title__isnull=False,
                type__in=[Conversation.Type.DEEP_RESEARCH, Conversation.Type.ASSISTANT, Conversation.Type.SLACK],
            )
            # Hide internal conversations from customers, but show them to support agents during impersonation
            if not is_impersonated_session(self.request):
                queryset = queryset.filter(is_internal=False)
            queryset = queryset.order_by("-updated_at")
        return queryset

    def get_throttles(self):
        # For create action, throttling is handled in check_throttles() for conditional logic
        if self.action == "create":
            return []
        return super().get_throttles()

    def _is_research_request(self, request: Request) -> bool:
        """Check if the request is for a research conversation."""
        # Check if it's a new conversation with research mode
        agent_mode = request.data.get("agent_mode")
        if agent_mode == AgentMode.RESEARCH or agent_mode == AgentMode.RESEARCH.value:
            return True

        # Check if it's an existing deep research conversation
        conversation_id = request.data.get("conversation")
        if conversation_id:
            try:
                conversation = Conversation.objects.get(id=conversation_id)
                if conversation.type == Conversation.Type.DEEP_RESEARCH:
                    return True
            except (Conversation.DoesNotExist, ValidationError):
                # DoesNotExist or ValidationError (invalid UUID) - not a research conversation
                pass

        return False

    def check_throttles(self, request: Request):
        # Only apply custom throttling for create action
        if self.action != "create":
            return super().check_throttles(request)

        # Skip throttling in local development
        if settings.DEBUG:
            return

        # Determine which throttles to apply based on request type
        is_research = self._is_research_request(request)

        if is_research:
            throttles = [AIResearchBurstRateThrottle(), AIResearchSustainedRateThrottle()]
        else:
            # Skip throttling for paying customers
            if self.organization.customer_id:
                return
            throttles = [AIBurstRateThrottle(), AISustainedRateThrottle()]

        for throttle in throttles:
            if not throttle.allow_request(request, self):
                wait = throttle.wait()
                if wait is not None:
                    if wait < 60:
                        retry_after = f"in {int(wait)} seconds"
                    elif wait < 3600:
                        retry_after = f"in {int(wait / 60)} minutes"
                    else:
                        retry_after = "later today"
                else:
                    retry_after = "later"

                if is_research:
                    detail = RESEARCH_RATE_LIMIT_MESSAGE.format(retry_after=retry_after)
                else:
                    detail = f"You've reached PostHog AI's usage limit for the moment. Please try again {retry_after}."

                raise Throttled(wait=wait, detail=detail)

    def get_serializer_class(self):
        if self.action == "create":
            return MessageSerializer
        if self.action == "append_message":
            return MessageMinimalSerializer
        return super().get_serializer_class()

    def get_serializer_context(self):
        context = super().get_serializer_context()
        context["team"] = self.team
        context["user"] = cast(User, self.request.user)
        return context

    def create(self, request: Request, *args, **kwargs):
        """
        Unified endpoint that handles both conversation creation and streaming.

        - If message is provided: Start new conversation processing
        - If no message: Stream from existing conversation
        """

        if is_team_limited(self.team.api_token, QuotaResource.AI_CREDITS, QuotaLimitingCaches.QUOTA_LIMITER_CACHE_KEY):
            raise QuotaLimitExceeded(
                "Your organization reached its AI credit usage limit. Increase the limits in Billing settings, or ask an org admin to do so."
            )

        serializer = self.get_serializer(data=request.data)
        serializer.is_valid(raise_exception=True)
        conversation_id = serializer.validated_data["conversation"]

        has_message = serializer.validated_data.get("content") is not None
        is_deep_research = serializer.validated_data.get("agent_mode") == AgentMode.RESEARCH

        is_new_conversation = False
        # Safely set the lookup kwarg for potential error handling
        if self.lookup_url_kwarg:
            self.kwargs[self.lookup_url_kwarg] = conversation_id
        try:
            conversation = Conversation.objects.get(id=conversation_id)
            if conversation.user != request.user or conversation.team != self.team:
                return Response(
                    {"error": "Cannot access other users' conversations"}, status=status.HTTP_400_BAD_REQUEST
                )
        except Conversation.DoesNotExist:
            # Conversation doesn't exist, create it if we have a message
            if not has_message:
                return Response(
                    {"error": "Cannot stream from non-existent conversation"}, status=status.HTTP_400_BAD_REQUEST
                )
            # Use frontend-provided conversation ID
            # Mark conversation as internal if created during an impersonated session (support agents)
            is_impersonated = is_impersonated_session(request)
            conversation_type = Conversation.Type.DEEP_RESEARCH if is_deep_research else Conversation.Type.ASSISTANT
            conversation = Conversation.objects.create(
                user=cast(User, request.user),
                team=self.team,
                id=conversation_id,
                type=conversation_type,
                is_internal=is_impersonated,
            )
            is_new_conversation = True

        is_idle = conversation.status == Conversation.Status.IDLE
        has_message = serializer.validated_data.get("message") is not None
        has_resume_payload = serializer.validated_data.get("resume_payload") is not None
        if conversation.type == Conversation.Type.DEEP_RESEARCH:
            is_deep_research = True

        if has_message and not is_idle:
            raise Conflict("Cannot resume streaming with a new message")
        # If the frontend is trying to resume streaming for a finished conversation, return a conflict error
        if not has_message and conversation.status == Conversation.Status.IDLE and not has_resume_payload:
            raise exceptions.ValidationError("Cannot continue streaming from an idle conversation")

        workflow_inputs: ChatAgentWorkflowInputs | ResearchAgentWorkflowInputs
        workflow_class: type[ChatAgentWorkflow] | type[ResearchAgentWorkflow]
        if is_deep_research:
            workflow_inputs = ResearchAgentWorkflowInputs(
                team_id=self.team_id,
                user_id=cast(User, request.user).pk,  # Use pk instead of id for User model
                conversation_id=conversation.id,
                stream_key=get_conversation_stream_key(conversation.id),
                message=serializer.validated_data["message"].model_dump() if has_message else None,
                is_new_conversation=is_new_conversation,
                trace_id=serializer.validated_data["trace_id"],
                session_id=request.headers.get("X-POSTHOG-SESSION-ID"),  # Relies on posthog-js __add_tracing_headers
                billing_context=serializer.validated_data.get("billing_context"),
                is_agent_billable=False,
                resume_payload=serializer.validated_data.get("resume_payload"),
            )
            workflow_class = ResearchAgentWorkflow
            timeout = RESEARCH_AGENT_WORKFLOW_TIMEOUT
            max_length = RESEARCH_AGENT_STREAM_MAX_LENGTH
        else:
            is_impersonated = is_impersonated_session(request)
            is_agent_billable = not is_impersonated
            workflow_inputs = ChatAgentWorkflowInputs(
                team_id=self.team_id,
                user_id=cast(User, request.user).pk,  # Use pk instead of id for User model
                conversation_id=conversation.id,
                stream_key=get_conversation_stream_key(conversation.id),
                message=serializer.validated_data["message"].model_dump() if has_message else None,
                contextual_tools=serializer.validated_data.get("contextual_tools"),
                is_new_conversation=is_new_conversation,
                trace_id=serializer.validated_data["trace_id"],
                session_id=request.headers.get("X-POSTHOG-SESSION-ID"),  # Relies on posthog-js __add_tracing_headers
                billing_context=serializer.validated_data.get("billing_context"),
                agent_mode=serializer.validated_data.get("agent_mode"),
                use_checkpointer=True,
                is_agent_billable=is_agent_billable,
                resume_payload=serializer.validated_data.get("resume_payload"),
            )
            workflow_class = ChatAgentWorkflow
            timeout = CHAT_AGENT_WORKFLOW_TIMEOUT
            max_length = CHAT_AGENT_STREAM_MAX_LENGTH

        async def async_stream(
            workflow_inputs: ChatAgentWorkflowInputs | ResearchAgentWorkflowInputs,
        ) -> AsyncGenerator[bytes, None]:
            serializer = AssistantSSESerializer()
            stream_manager = AgentExecutor(conversation, timeout=timeout, max_length=max_length)
            last_iteration_time = time.time()
            async for chunk in stream_manager.astream(workflow_class, workflow_inputs):
                chunk_received_time = time.time()
                STREAM_ITERATION_LATENCY_HISTOGRAM.observe(chunk_received_time - last_iteration_time)
                last_iteration_time = chunk_received_time

                event = await serializer.dumps(chunk)
                yield event.encode("utf-8")

        return StreamingHttpResponse(
            (
                async_stream(workflow_inputs)
                if settings.SERVER_GATEWAY_INTERFACE == "ASGI"
                else async_to_sync(lambda: async_stream(workflow_inputs))
            ),
            content_type="text/event-stream",
        )

    @action(detail=True, methods=["GET", "POST"], url_path="queue")
    def queue(self, request: Request, *args, **kwargs):
        conversation_id = self._queue_conversation_id()
        error_response = self._ensure_queue_access(request, conversation_id)
        if error_response:
            return error_response

        queue_store = ConversationQueueStore(conversation_id)

        if request.method == "GET":
            return self._queue_response(queue_store, queue_store.list())

        serializer = QueueMessageSerializer(data=request.data)
        serializer.is_valid(raise_exception=True)
        message = build_queue_message(
            content=serializer.validated_data["content"],
            contextual_tools=serializer.validated_data.get("contextual_tools"),
            ui_context=serializer.validated_data.get("ui_context"),
            billing_context=serializer.validated_data.get("billing_context"),
            agent_mode=serializer.validated_data.get("agent_mode"),
            session_id=request.headers.get("X-POSTHOG-SESSION-ID"),
        )

        try:
            queue = queue_store.enqueue(message)
        except QueueFullError:
            return Response(
                {
                    "error": "queue_full",
                    "detail": "Only two messages can be queued at a time.",
                },
                status=status.HTTP_409_CONFLICT,
            )

        return self._queue_response(queue_store, queue)

    @action(detail=True, methods=["PATCH", "DELETE"], url_path=r"queue/(?P<queue_id>[^/.]+)")
    def queue_item(self, request: Request, queue_id: str, *args, **kwargs):
        conversation_id = self._queue_conversation_id()
        error_response = self._ensure_queue_access(request, conversation_id)
        if error_response:
            return error_response

        queue_store = ConversationQueueStore(conversation_id)
        queue = queue_store.list()
        queue_index = next((index for index, item in enumerate(queue) if item.get("id") == queue_id), None)

        if queue_index is None:
            return Response({"detail": "Queue message not found."}, status=status.HTTP_404_NOT_FOUND)

        if request.method == "PATCH":
            serializer = QueueMessageUpdateSerializer(data=request.data)
            serializer.is_valid(raise_exception=True)
            queue = queue_store.update(queue_id, serializer.validated_data["content"])
        else:
            queue = queue_store.delete(queue_id)

        return self._queue_response(queue_store, queue)

    @action(detail=True, methods=["POST"], url_path="queue/clear")
    def clear_queue(self, request: Request, *args, **kwargs):
        conversation_id = self._queue_conversation_id()
        error_response = self._ensure_queue_access(request, conversation_id)
        if error_response:
            return error_response
        queue_store = ConversationQueueStore(conversation_id)
        return self._queue_response(queue_store, queue_store.clear())

    @action(detail=True, methods=["PATCH"])
    def cancel(self, request: Request, *args, **kwargs):
        conversation = self.get_object()

        if conversation.status in [Conversation.Status.CANCELING, Conversation.Status.IDLE]:
            return Response(status=status.HTTP_204_NO_CONTENT)

        async def cancel_workflow():
            agent_executor = AgentExecutor(conversation)
            await agent_executor.cancel_workflow()

        try:
            asgi_async_to_sync(cancel_workflow)()
        except Exception as e:
            logger.exception("Failed to cancel conversation", conversation_id=conversation.id, error=str(e))
            return Response({"error": "Failed to cancel conversation"}, status=status.HTTP_422_UNPROCESSABLE_ENTITY)

        return Response(status=status.HTTP_204_NO_CONTENT)

    @action(detail=True, methods=["POST"], url_path="append_message")
    def append_message(self, request: Request, *args, **kwargs):
        """
        Appends a message to an existing conversation without triggering AI processing.
        This is used for client-side generated messages that need to be persisted
        (e.g., support ticket confirmation messages).
        """
        conversation = self.get_object()

        serializer = self.get_serializer(data=request.data)
        serializer.is_valid(raise_exception=True)

        content = serializer.validated_data["content"]
        message = AssistantMessage(content=content, id=str(uuid.uuid4()))

        async def append_to_state():
            user = cast(User, request.user)
            graph = AssistantGraph(self.team, user).compile_full_graph()
            # Empty checkpoint_ns targets the root graph (not subgraphs)
            config = {"configurable": {"thread_id": str(conversation.id), "checkpoint_ns": ""}}
            await graph.aupdate_state(
                config,
                PartialAssistantState(messages=[message]),
            )

        try:
            asgi_async_to_sync(append_to_state)()
        except Exception as e:
            logger.exception("Failed to append message to conversation", conversation_id=conversation.id, error=str(e))
            return Response({"error": "Failed to append message"}, status=status.HTTP_422_UNPROCESSABLE_ENTITY)

        return Response({"id": message.id}, status=status.HTTP_201_CREATED)
