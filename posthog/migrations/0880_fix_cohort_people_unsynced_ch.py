# Generated by Django 4.2.16 on 2025-10-14 20:57

from django.db import migrations

import structlog

logger = structlog.get_logger(__name__)

BATCH_SIZE = 10000
PROD_US_CUTOFF = "2025-10-09 00:00:00"


def sync_cohort_people_from_clickhouse(apps, schema_editor):
    """
    Sync cohort people records from ClickHouse person_static_cohort table
    to PostgreSQL posthog_cohortpeople table for records inserted after Oct 9, 2025 00:00 UTC.

    EU environments are skipped because the original fix was already deployed there
    before the persons table separation occurred, making this migration unnecessary.
    """
    import os

    from django.conf import settings
    from django.db import connections

    from posthog.clickhouse.client import sync_execute

    # Skip migration during tests
    if settings.TEST:
        logger.info("Skipping cohort people sync during tests")
        return

    # Skip migration for EU environment
    site_url = getattr(settings, "SITE_URL", os.getenv("SITE_URL", ""))
    if site_url == "https://eu.posthog.com":
        logger.info("Skipping cohort people sync for EU environment")
        return

    # Determine the correct database connection for persons
    if "persons_db_reader" in connections:
        persons_db = "persons_db_reader"
    elif "replica" in connections:
        persons_db = "replica"
    else:
        persons_db = "default"

    # Use US cutoff for all non-EU environments
    cutoff_timestamp = PROD_US_CUTOFF
    logger.info(f"Starting cohort people sync for records after {cutoff_timestamp}")

    try:
        # Process ClickHouse data in batches and handle each batch completely
        total_inserted = 0
        processed_cohort_ids = set()
        offset = 0

        while True:
            batch_query = """
                SELECT DISTINCT person_id, cohort_id, team_id, _timestamp
                FROM person_static_cohort
                WHERE _timestamp >= %(cutoff_timestamp)s
                ORDER BY _timestamp, person_id, cohort_id
                LIMIT %(batch_size)s OFFSET %(offset)s
            """

            clickhouse_batch = sync_execute(
                batch_query, {"cutoff_timestamp": cutoff_timestamp, "batch_size": BATCH_SIZE, "offset": offset}
            )

            if not clickhouse_batch:
                break

            logger.info(f"Processing ClickHouse batch: {len(clickhouse_batch)} records (offset: {offset})")

            # Extract unique person UUIDs from this batch
            ch_person_uuids = list({str(record[0]) for record in clickhouse_batch})

            if not ch_person_uuids:
                offset += BATCH_SIZE
                continue

            # Use the correct database connection
            person_connection = connections[persons_db]

            # Get person ID mappings from persons database for this batch
            person_uuid_to_id = {}
            with person_connection.cursor() as cursor:
                cursor.execute("SELECT uuid, id FROM posthog_person WHERE uuid = ANY(%s)", [ch_person_uuids])
                for uuid, person_id in cursor.fetchall():
                    person_uuid_to_id[str(uuid)] = person_id

            if not person_uuid_to_id:
                logger.info(f"No matching persons found for batch at offset {offset}")
                offset += BATCH_SIZE
                continue

            logger.info(f"Found {len(person_uuid_to_id)} person ID mappings for batch")

            # Build the cohort data mapping from this ClickHouse batch
            cohort_data: dict[str, set[int]] = {}
            for record in clickhouse_batch:
                person_uuid = str(record[0])
                cohort_id = record[1]
                processed_cohort_ids.add(cohort_id)
                if person_uuid not in cohort_data:
                    cohort_data[person_uuid] = set()
                cohort_data[person_uuid].add(cohort_id)

            # Get existing cohortpeople records to avoid duplicates for this batch
            existing_pairs = set()
            with person_connection.cursor() as cursor:
                person_ids = list(person_uuid_to_id.values())
                if person_ids:
                    cursor.execute(
                        """
                        SELECT cohort_id, person_id
                        FROM posthog_cohortpeople
                        WHERE person_id = ANY(%s)
                    """,
                        [person_ids],
                    )
                    for cohort_id, person_id in cursor.fetchall():
                        existing_pairs.add((cohort_id, person_id))

            # Prepare bulk insert data for this batch, excluding existing records
            insert_values = []
            for person_uuid, cohort_ids in cohort_data.items():
                if person_uuid not in person_uuid_to_id:
                    continue
                person_id = person_uuid_to_id[person_uuid]
                for cohort_id in cohort_ids:
                    if (cohort_id, person_id) not in existing_pairs:
                        insert_values.append((cohort_id, person_id))

            if insert_values:
                # Insert this batch using Django ORM
                from posthog.models.cohort.cohort import CohortPeople

                # Create CohortPeople objects for bulk_create
                cohort_people_objects = [
                    CohortPeople(cohort_id=cohort_id, person_id=person_id, version=0)
                    for cohort_id, person_id in insert_values
                ]

                # Use bulk_create with ignore_conflicts
                created_objects = CohortPeople.objects.using(persons_db).bulk_create(
                    cohort_people_objects, ignore_conflicts=True
                )
                batch_inserted = len(created_objects)
                total_inserted += batch_inserted
                logger.info(f"Batch successful: {batch_inserted} records inserted")
            else:
                logger.info(f"No new records to insert for batch at offset {offset}")

            offset += BATCH_SIZE

        logger.info(f"Cohort people sync completed: {total_inserted} records inserted")

        # Trigger cohort recalculation for all processed cohorts
        if processed_cohort_ids:
            logger.info(
                f"Triggering recalculation for {len(processed_cohort_ids)} cohorts: {list(processed_cohort_ids)}"
            )
            try:
                from posthog.models.cohort.cohort import Cohort
                from posthog.tasks.calculate_cohort import increment_version_and_enqueue_calculate_cohort

                for cohort in Cohort.objects.filter(id__in=processed_cohort_ids).iterator():
                    try:
                        increment_version_and_enqueue_calculate_cohort(cohort, initiating_user=None)
                        logger.info(f"Triggered recalculation for cohort {cohort.id}")
                    except Exception as e:
                        logger.exception(f"Failed to trigger recalculation for cohort {cohort.id}: {e}")

                logger.info("Completed cohort recalculation triggers")
            except ImportError as e:
                logger.exception(f"Could not import cohort calculation functions: {e}")
            except Exception as e:
                logger.exception(f"Failed to trigger cohort recalculations: {e}")

    except Exception as e:
        logger.exception("Error syncing cohort people", error=str(e))
        raise


def reverse_sync_cohort_people_from_clickhouse(apps, schema_editor):
    """
    Reverse migration is a no-op since we're syncing historical data.
    Removing synced data would be destructive and not necessary.
    """
    pass


class Migration(migrations.Migration):
    dependencies = [
        ("posthog", "0879_migrate_error_tracking_models"),
    ]

    operations = [
        migrations.RunPython(
            sync_cohort_people_from_clickhouse,
            reverse_sync_cohort_people_from_clickhouse,
            elidable=True,
        ),
    ]
