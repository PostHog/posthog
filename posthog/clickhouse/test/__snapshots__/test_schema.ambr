# name: test_create_kafka_events_with_disabled_protobuf
  '
  
  CREATE TABLE IF NOT EXISTS kafka_events_json ON CLUSTER 'posthog'
  (
      uuid UUID,
      event VARCHAR,
      properties VARCHAR CODEC(ZSTD(3)),
      timestamp DateTime64(6, 'UTC'),
      team_id Int64,
      distinct_id VARCHAR,
      elements_chain VARCHAR,
      created_at DateTime64(6, 'UTC'),
      person_id UUID,
      person_created_at DateTime64,
      person_properties VARCHAR Codec(ZSTD(3)),
      group0_properties VARCHAR Codec(ZSTD(3)),
      group1_properties VARCHAR Codec(ZSTD(3)),
      group2_properties VARCHAR Codec(ZSTD(3)),
      group3_properties VARCHAR Codec(ZSTD(3)),
      group4_properties VARCHAR Codec(ZSTD(3)),
      group0_created_at DateTime64,
      group1_created_at DateTime64,
      group2_created_at DateTime64,
      group3_created_at DateTime64,
      group4_created_at DateTime64
      
      
      
  ) ENGINE = Kafka('kafka:9092', 'clickhouse_events_json_test', 'group1', 'JSONEachRow')
  
      SETTINGS kafka_skip_broken_messages = 100
  
  '
---
# name: test_create_kafka_table_with_different_kafka_host[kafka_app_metrics]
  '
  
  CREATE TABLE kafka_app_metrics ON CLUSTER 'posthog'
  (
      team_id Int64,
      timestamp DateTime64(6, 'UTC'),
      plugin_config_id Int64,
      category LowCardinality(String),
      job_id String,
      successes Int64,
      successes_on_retry Int64,
      failures Int64,
      error_uuid UUID,
      error_type String,
      error_details String CODEC(ZSTD(3))
  )
  ENGINE=Kafka('test.kafka.broker:9092', 'clickhouse_app_metrics_test', 'group1', 'JSONEachRow')
  
  '
---
# name: test_create_kafka_table_with_different_kafka_host[kafka_events_dead_letter_queue]
  '
  
  CREATE TABLE IF NOT EXISTS kafka_events_dead_letter_queue ON CLUSTER 'posthog'
  (
      id UUID,
      event_uuid UUID,
      event VARCHAR,
      properties VARCHAR,
      distinct_id VARCHAR,
      team_id Int64,
      elements_chain VARCHAR,
      created_at DateTime64(6, 'UTC'),
      ip VARCHAR,
      site_url VARCHAR,
      now DateTime64(6, 'UTC'),
      raw_payload VARCHAR,
      error_timestamp DateTime64(6, 'UTC'),
      error_location VARCHAR,
      error VARCHAR,
      tags Array(VARCHAR)
      
  ) ENGINE = Kafka('test.kafka.broker:9092', 'events_dead_letter_queue_test', 'group1', 'JSONEachRow')
   SETTINGS kafka_skip_broken_messages=1000
  '
---
# name: test_create_kafka_table_with_different_kafka_host[kafka_events_json]
  '
  
  CREATE TABLE IF NOT EXISTS kafka_events_json ON CLUSTER 'posthog'
  (
      uuid UUID,
      event VARCHAR,
      properties VARCHAR CODEC(ZSTD(3)),
      timestamp DateTime64(6, 'UTC'),
      team_id Int64,
      distinct_id VARCHAR,
      elements_chain VARCHAR,
      created_at DateTime64(6, 'UTC'),
      person_id UUID,
      person_created_at DateTime64,
      person_properties VARCHAR Codec(ZSTD(3)),
      group0_properties VARCHAR Codec(ZSTD(3)),
      group1_properties VARCHAR Codec(ZSTD(3)),
      group2_properties VARCHAR Codec(ZSTD(3)),
      group3_properties VARCHAR Codec(ZSTD(3)),
      group4_properties VARCHAR Codec(ZSTD(3)),
      group0_created_at DateTime64,
      group1_created_at DateTime64,
      group2_created_at DateTime64,
      group3_created_at DateTime64,
      group4_created_at DateTime64
      
      
      
  ) ENGINE = Kafka('test.kafka.broker:9092', 'clickhouse_events_json_test', 'group1', 'JSONEachRow')
  
      SETTINGS kafka_skip_broken_messages = 100
  
  '
---
# name: test_create_kafka_table_with_different_kafka_host[kafka_groups]
  '
  
  CREATE TABLE IF NOT EXISTS kafka_groups ON CLUSTER 'posthog'
  (
      group_type_index UInt8,
      group_key VARCHAR,
      created_at DateTime64,
      team_id Int64,
      group_properties VARCHAR
      
  ) ENGINE = Kafka('test.kafka.broker:9092', 'clickhouse_groups_test', 'group1', 'JSONEachRow')
  
  '
---
# name: test_create_kafka_table_with_different_kafka_host[kafka_ingestion_warnings]
  '
  
  CREATE TABLE IF NOT EXISTS kafka_ingestion_warnings ON CLUSTER 'posthog'
  (
      team_id Int64,
      source LowCardinality(VARCHAR),
      type VARCHAR,
      details VARCHAR CODEC(ZSTD(3)),
      timestamp DateTime64(6, 'UTC')
      
  ) ENGINE = Kafka('test.kafka.broker:9092', 'clickhouse_ingestion_warnings_test', 'group1', 'JSONEachRow')
  
  '
---
# name: test_create_kafka_table_with_different_kafka_host[kafka_performance_events]
  '
  
  CREATE TABLE IF NOT EXISTS kafka_performance_events ON CLUSTER 'posthog'
  (
      uuid UUID,
  session_id String,
  window_id String,
  pageview_id String,
  distinct_id String,
  timestamp DateTime64,
  time_origin DateTime64(3, 'UTC'),
  entry_type LowCardinality(String),
  name String,
  team_id Int64,
  current_url String,
  start_time Float64,
  duration Float64,
  redirect_start Float64,
  redirect_end Float64,
  worker_start Float64,
  fetch_start Float64,
  domain_lookup_start Float64,
  domain_lookup_end Float64,
  connect_start Float64,
  secure_connection_start Float64,
  connect_end Float64,
  request_start Float64,
  response_start Float64,
  response_end Float64,
  decoded_body_size Int64,
  encoded_body_size Int64,
  initiator_type LowCardinality(String),
  next_hop_protocol LowCardinality(String),
  render_blocking_status LowCardinality(String),
  response_status Int64,
  transfer_size Int64,
  largest_contentful_paint_element String,
  largest_contentful_paint_render_time Float64,
  largest_contentful_paint_load_time Float64,
  largest_contentful_paint_size Float64,
  largest_contentful_paint_id String,
  largest_contentful_paint_url String,
  dom_complete Float64,
  dom_content_loaded_event Float64,
  dom_interactive Float64,
  load_event_end Float64,
  load_event_start Float64,
  redirect_count Int64,
  navigation_type LowCardinality(String),
  unload_event_end Float64,
  unload_event_start Float64
      
  , _timestamp DateTime
  , _offset UInt64
  , _partition UInt64
  
  ) ENGINE = Kafka('test.kafka.broker:9092', 'clickhouse_performance_events_test', 'group1', 'JSONEachRow')
  
  '
---
# name: test_create_kafka_table_with_different_kafka_host[kafka_person]
  '
  
  CREATE TABLE IF NOT EXISTS kafka_person ON CLUSTER 'posthog'
  (
      id UUID,
      created_at DateTime64,
      team_id Int64,
      properties VARCHAR,
      is_identified Int8,
      is_deleted Int8 DEFAULT 0,
      version UInt64
      
  ) ENGINE = Kafka('test.kafka.broker:9092', 'clickhouse_person_test', 'group1', 'JSONEachRow')
  
  '
---
# name: test_create_kafka_table_with_different_kafka_host[kafka_person_distinct_id2]
  '
  
  CREATE TABLE IF NOT EXISTS kafka_person_distinct_id2 ON CLUSTER 'posthog'
  (
      team_id Int64,
      distinct_id VARCHAR,
      person_id UUID,
      is_deleted Int8,
      version Int64 DEFAULT 1
      
  ) ENGINE = Kafka('test.kafka.broker:9092', 'clickhouse_person_distinct_id_test', 'group1', 'JSONEachRow')
  
  '
---
# name: test_create_kafka_table_with_different_kafka_host[kafka_person_distinct_id]
  '
  
  CREATE TABLE kafka_person_distinct_id ON CLUSTER 'posthog'
  (
      distinct_id VARCHAR,
      person_id UUID,
      team_id Int64,
      _sign Nullable(Int8),
      is_deleted Nullable(Int8)
  ) ENGINE = Kafka('test.kafka.broker:9092', 'clickhouse_person_unique_id_test', 'group1', 'JSONEachRow')
  
  '
---
# name: test_create_kafka_table_with_different_kafka_host[kafka_plugin_log_entries]
  '
  
  CREATE TABLE IF NOT EXISTS kafka_plugin_log_entries ON CLUSTER 'posthog'
  (
      id UUID,
      team_id Int64,
      plugin_id Int64,
      plugin_config_id Int64,
      timestamp DateTime64(6, 'UTC'),
      source VARCHAR,
      type VARCHAR,
      message VARCHAR,
      instance_id UUID
      
  ) ENGINE = Kafka('test.kafka.broker:9092', 'plugin_log_entries_test', 'group1', 'JSONEachRow')
  
  '
---
# name: test_create_kafka_table_with_different_kafka_host[kafka_session_recording_events]
  '
  
  CREATE TABLE IF NOT EXISTS kafka_session_recording_events ON CLUSTER 'posthog'
  (
      uuid UUID,
      timestamp DateTime64(6, 'UTC'),
      team_id Int64,
      distinct_id VARCHAR,
      session_id VARCHAR,
      window_id VARCHAR,
      snapshot_data VARCHAR,
      created_at DateTime64(6, 'UTC')
      
      
  ) ENGINE = Kafka('test.kafka.broker:9092', 'clickhouse_session_recording_events_test', 'group1', 'JSONEachRow')
  
  '
---
# name: test_create_table_query[app_metrics]
  '
  
  CREATE TABLE app_metrics ON CLUSTER 'posthog'
  (
      team_id Int64,
      timestamp DateTime64(6, 'UTC'),
      plugin_config_id Int64,
      category LowCardinality(String),
      job_id String,
      successes SimpleAggregateFunction(sum, Int64),
      successes_on_retry SimpleAggregateFunction(sum, Int64),
      failures SimpleAggregateFunction(sum, Int64),
      error_uuid UUID,
      error_type String,
      error_details String CODEC(ZSTD(3))
      
  , _timestamp DateTime
  , _offset UInt64
  , _partition UInt64
  
  )
  ENGINE=Distributed('posthog', 'posthog_test', 'sharded_app_metrics', rand())
  
  '
---
# name: test_create_table_query[app_metrics_mv]
  '
  
  CREATE MATERIALIZED VIEW app_metrics_mv ON CLUSTER 'posthog'
  TO posthog_test.sharded_app_metrics
  AS SELECT
  team_id,
  timestamp,
  plugin_config_id,
  category,
  job_id,
  successes,
  successes_on_retry,
  failures,
  error_uuid,
  error_type,
  error_details
  FROM posthog_test.kafka_app_metrics
  
  '
---
# name: test_create_table_query[cohortpeople]
  '
  
  CREATE TABLE IF NOT EXISTS cohortpeople ON CLUSTER 'posthog'
  (
      person_id UUID,
      cohort_id Int64,
      team_id Int64,
      sign Int8,
      version UInt64
  ) ENGINE = CollapsingMergeTree(sign)
  Order By (team_id, cohort_id, person_id, version)
  
  
  '
---
# name: test_create_table_query[events]
  '
  
  CREATE TABLE IF NOT EXISTS events ON CLUSTER 'posthog'
  (
      uuid UUID,
      event VARCHAR,
      properties VARCHAR CODEC(ZSTD(3)),
      timestamp DateTime64(6, 'UTC'),
      team_id Int64,
      distinct_id VARCHAR,
      elements_chain VARCHAR,
      created_at DateTime64(6, 'UTC'),
      person_id UUID,
      person_created_at DateTime64,
      person_properties VARCHAR Codec(ZSTD(3)),
      group0_properties VARCHAR Codec(ZSTD(3)),
      group1_properties VARCHAR Codec(ZSTD(3)),
      group2_properties VARCHAR Codec(ZSTD(3)),
      group3_properties VARCHAR Codec(ZSTD(3)),
      group4_properties VARCHAR Codec(ZSTD(3)),
      group0_created_at DateTime64,
      group1_created_at DateTime64,
      group2_created_at DateTime64,
      group3_created_at DateTime64,
      group4_created_at DateTime64
      
      , $group_0 VARCHAR COMMENT 'column_materializer::$group_0'
      , $group_1 VARCHAR COMMENT 'column_materializer::$group_1'
      , $group_2 VARCHAR COMMENT 'column_materializer::$group_2'
      , $group_3 VARCHAR COMMENT 'column_materializer::$group_3'
      , $group_4 VARCHAR COMMENT 'column_materializer::$group_4'
      , $window_id VARCHAR COMMENT 'column_materializer::$window_id'
      , $session_id VARCHAR COMMENT 'column_materializer::$session_id'
  
      
  , _timestamp DateTime
  , _offset UInt64
  
      
  ) ENGINE = Distributed('posthog', 'posthog_test', 'events', sipHash64(distinct_id))
  
  '
---
# name: test_create_table_query[events_dead_letter_queue]
  '
  
  CREATE TABLE IF NOT EXISTS events_dead_letter_queue ON CLUSTER 'posthog'
  (
      id UUID,
      event_uuid UUID,
      event VARCHAR,
      properties VARCHAR,
      distinct_id VARCHAR,
      team_id Int64,
      elements_chain VARCHAR,
      created_at DateTime64(6, 'UTC'),
      ip VARCHAR,
      site_url VARCHAR,
      now DateTime64(6, 'UTC'),
      raw_payload VARCHAR,
      error_timestamp DateTime64(6, 'UTC'),
      error_location VARCHAR,
      error VARCHAR,
      tags Array(VARCHAR)
      
      
  , _timestamp DateTime
  , _offset UInt64
  
      , INDEX kafka_timestamp_minmax_events_dead_letter_queue _timestamp TYPE minmax GRANULARITY 3
      
  ) ENGINE = ReplacingMergeTree(_timestamp)
  ORDER BY (id, event_uuid, distinct_id, team_id)
  
  SETTINGS index_granularity=512
  
  '
---
# name: test_create_table_query[events_dead_letter_queue_mv]
  '
  
  CREATE MATERIALIZED VIEW IF NOT EXISTS events_dead_letter_queue_mv ON CLUSTER 'posthog'
  TO posthog_test.events_dead_letter_queue
  AS SELECT
  id,
  event_uuid,
  event,
  properties,
  distinct_id,
  team_id,
  elements_chain,
  created_at,
  ip,
  site_url,
  now,
  raw_payload,
  error_timestamp,
  error_location,
  error,
  tags,
  _timestamp,
  _offset
  FROM posthog_test.kafka_events_dead_letter_queue
  
  '
---
# name: test_create_table_query[events_json_mv]
  '
  
  CREATE MATERIALIZED VIEW events_json_mv ON CLUSTER 'posthog'
  TO posthog_test.events
  AS SELECT
  uuid,
  event,
  properties,
  timestamp,
  team_id,
  distinct_id,
  elements_chain,
  created_at,
  person_id,
  person_created_at,
  person_properties,
  group0_properties,
  group1_properties,
  group2_properties,
  group3_properties,
  group4_properties,
  group0_created_at,
  group1_created_at,
  group2_created_at,
  group3_created_at,
  group4_created_at,
  _timestamp,
  _offset
  FROM posthog_test.kafka_events_json
  
  '
---
# name: test_create_table_query[groups]
  '
  
  CREATE TABLE IF NOT EXISTS groups ON CLUSTER 'posthog'
  (
      group_type_index UInt8,
      group_key VARCHAR,
      created_at DateTime64,
      team_id Int64,
      group_properties VARCHAR
      
  , _timestamp DateTime
  , _offset UInt64
  
  ) ENGINE = ReplacingMergeTree(_timestamp)
  Order By (team_id, group_type_index, group_key)
  
  
  '
---
# name: test_create_table_query[groups_mv]
  '
  
  CREATE MATERIALIZED VIEW groups_mv ON CLUSTER 'posthog'
  TO posthog_test.groups
  AS SELECT
  group_type_index,
  group_key,
  created_at,
  team_id,
  group_properties,
  _timestamp,
  _offset
  FROM posthog_test.kafka_groups
  
  '
---
# name: test_create_table_query[ingestion_warnings]
  '
  
  CREATE TABLE IF NOT EXISTS ingestion_warnings ON CLUSTER 'posthog'
  (
      team_id Int64,
      source LowCardinality(VARCHAR),
      type VARCHAR,
      details VARCHAR CODEC(ZSTD(3)),
      timestamp DateTime64(6, 'UTC')
      
  , _timestamp DateTime
  , _offset UInt64
  , _partition UInt64
  
  ) ENGINE = Distributed('posthog', 'posthog_test', 'sharded_ingestion_warnings', rand())
  
  '
---
# name: test_create_table_query[ingestion_warnings_mv]
  '
  
  CREATE MATERIALIZED VIEW ingestion_warnings_mv ON CLUSTER 'posthog'
  TO posthog_test.ingestion_warnings
  AS SELECT
  team_id,
  source,
  type,
  details,
  timestamp,
  _timestamp,
  _offset,
  _partition
  FROM posthog_test.kafka_ingestion_warnings
  
  '
---
# name: test_create_table_query[kafka_app_metrics]
  '
  
  CREATE TABLE kafka_app_metrics ON CLUSTER 'posthog'
  (
      team_id Int64,
      timestamp DateTime64(6, 'UTC'),
      plugin_config_id Int64,
      category LowCardinality(String),
      job_id String,
      successes Int64,
      successes_on_retry Int64,
      failures Int64,
      error_uuid UUID,
      error_type String,
      error_details String CODEC(ZSTD(3))
  )
  ENGINE=Kafka('kafka:9092', 'clickhouse_app_metrics_test', 'group1', 'JSONEachRow')
  
  '
---
# name: test_create_table_query[kafka_events_dead_letter_queue]
  '
  
  CREATE TABLE IF NOT EXISTS kafka_events_dead_letter_queue ON CLUSTER 'posthog'
  (
      id UUID,
      event_uuid UUID,
      event VARCHAR,
      properties VARCHAR,
      distinct_id VARCHAR,
      team_id Int64,
      elements_chain VARCHAR,
      created_at DateTime64(6, 'UTC'),
      ip VARCHAR,
      site_url VARCHAR,
      now DateTime64(6, 'UTC'),
      raw_payload VARCHAR,
      error_timestamp DateTime64(6, 'UTC'),
      error_location VARCHAR,
      error VARCHAR,
      tags Array(VARCHAR)
      
  ) ENGINE = Kafka('kafka:9092', 'events_dead_letter_queue_test', 'group1', 'JSONEachRow')
   SETTINGS kafka_skip_broken_messages=1000
  '
---
# name: test_create_table_query[kafka_events_json]
  '
  
  CREATE TABLE IF NOT EXISTS kafka_events_json ON CLUSTER 'posthog'
  (
      uuid UUID,
      event VARCHAR,
      properties VARCHAR CODEC(ZSTD(3)),
      timestamp DateTime64(6, 'UTC'),
      team_id Int64,
      distinct_id VARCHAR,
      elements_chain VARCHAR,
      created_at DateTime64(6, 'UTC'),
      person_id UUID,
      person_created_at DateTime64,
      person_properties VARCHAR Codec(ZSTD(3)),
      group0_properties VARCHAR Codec(ZSTD(3)),
      group1_properties VARCHAR Codec(ZSTD(3)),
      group2_properties VARCHAR Codec(ZSTD(3)),
      group3_properties VARCHAR Codec(ZSTD(3)),
      group4_properties VARCHAR Codec(ZSTD(3)),
      group0_created_at DateTime64,
      group1_created_at DateTime64,
      group2_created_at DateTime64,
      group3_created_at DateTime64,
      group4_created_at DateTime64
      
      
      
  ) ENGINE = Kafka('kafka:9092', 'clickhouse_events_json_test', 'group1', 'JSONEachRow')
  
      SETTINGS kafka_skip_broken_messages = 100
  
  '
---
# name: test_create_table_query[kafka_groups]
  '
  
  CREATE TABLE IF NOT EXISTS kafka_groups ON CLUSTER 'posthog'
  (
      group_type_index UInt8,
      group_key VARCHAR,
      created_at DateTime64,
      team_id Int64,
      group_properties VARCHAR
      
  ) ENGINE = Kafka('kafka:9092', 'clickhouse_groups_test', 'group1', 'JSONEachRow')
  
  '
---
# name: test_create_table_query[kafka_ingestion_warnings]
  '
  
  CREATE TABLE IF NOT EXISTS kafka_ingestion_warnings ON CLUSTER 'posthog'
  (
      team_id Int64,
      source LowCardinality(VARCHAR),
      type VARCHAR,
      details VARCHAR CODEC(ZSTD(3)),
      timestamp DateTime64(6, 'UTC')
      
  ) ENGINE = Kafka('kafka:9092', 'clickhouse_ingestion_warnings_test', 'group1', 'JSONEachRow')
  
  '
---
# name: test_create_table_query[kafka_performance_events]
  '
  
  CREATE TABLE IF NOT EXISTS kafka_performance_events ON CLUSTER 'posthog'
  (
      uuid UUID,
  session_id String,
  window_id String,
  pageview_id String,
  distinct_id String,
  timestamp DateTime64,
  time_origin DateTime64(3, 'UTC'),
  entry_type LowCardinality(String),
  name String,
  team_id Int64,
  current_url String,
  start_time Float64,
  duration Float64,
  redirect_start Float64,
  redirect_end Float64,
  worker_start Float64,
  fetch_start Float64,
  domain_lookup_start Float64,
  domain_lookup_end Float64,
  connect_start Float64,
  secure_connection_start Float64,
  connect_end Float64,
  request_start Float64,
  response_start Float64,
  response_end Float64,
  decoded_body_size Int64,
  encoded_body_size Int64,
  initiator_type LowCardinality(String),
  next_hop_protocol LowCardinality(String),
  render_blocking_status LowCardinality(String),
  response_status Int64,
  transfer_size Int64,
  largest_contentful_paint_element String,
  largest_contentful_paint_render_time Float64,
  largest_contentful_paint_load_time Float64,
  largest_contentful_paint_size Float64,
  largest_contentful_paint_id String,
  largest_contentful_paint_url String,
  dom_complete Float64,
  dom_content_loaded_event Float64,
  dom_interactive Float64,
  load_event_end Float64,
  load_event_start Float64,
  redirect_count Int64,
  navigation_type LowCardinality(String),
  unload_event_end Float64,
  unload_event_start Float64
      
  , _timestamp DateTime
  , _offset UInt64
  , _partition UInt64
  
  ) ENGINE = Kafka('kafka:9092', 'clickhouse_performance_events_test', 'group1', 'JSONEachRow')
  
  '
---
# name: test_create_table_query[kafka_person]
  '
  
  CREATE TABLE IF NOT EXISTS kafka_person ON CLUSTER 'posthog'
  (
      id UUID,
      created_at DateTime64,
      team_id Int64,
      properties VARCHAR,
      is_identified Int8,
      is_deleted Int8 DEFAULT 0,
      version UInt64
      
  ) ENGINE = Kafka('kafka:9092', 'clickhouse_person_test', 'group1', 'JSONEachRow')
  
  '
---
# name: test_create_table_query[kafka_person_distinct_id2]
  '
  
  CREATE TABLE IF NOT EXISTS kafka_person_distinct_id2 ON CLUSTER 'posthog'
  (
      team_id Int64,
      distinct_id VARCHAR,
      person_id UUID,
      is_deleted Int8,
      version Int64 DEFAULT 1
      
  ) ENGINE = Kafka('kafka:9092', 'clickhouse_person_distinct_id_test', 'group1', 'JSONEachRow')
  
  '
---
# name: test_create_table_query[kafka_person_distinct_id]
  '
  
  CREATE TABLE kafka_person_distinct_id ON CLUSTER 'posthog'
  (
      distinct_id VARCHAR,
      person_id UUID,
      team_id Int64,
      _sign Nullable(Int8),
      is_deleted Nullable(Int8)
  ) ENGINE = Kafka('kafka:9092', 'clickhouse_person_unique_id_test', 'group1', 'JSONEachRow')
  
  '
---
# name: test_create_table_query[kafka_plugin_log_entries]
  '
  
  CREATE TABLE IF NOT EXISTS kafka_plugin_log_entries ON CLUSTER 'posthog'
  (
      id UUID,
      team_id Int64,
      plugin_id Int64,
      plugin_config_id Int64,
      timestamp DateTime64(6, 'UTC'),
      source VARCHAR,
      type VARCHAR,
      message VARCHAR,
      instance_id UUID
      
  ) ENGINE = Kafka('kafka:9092', 'plugin_log_entries_test', 'group1', 'JSONEachRow')
  
  '
---
# name: test_create_table_query[kafka_session_recording_events]
  '
  
  CREATE TABLE IF NOT EXISTS kafka_session_recording_events ON CLUSTER 'posthog'
  (
      uuid UUID,
      timestamp DateTime64(6, 'UTC'),
      team_id Int64,
      distinct_id VARCHAR,
      session_id VARCHAR,
      window_id VARCHAR,
      snapshot_data VARCHAR,
      created_at DateTime64(6, 'UTC')
      
      
  ) ENGINE = Kafka('kafka:9092', 'clickhouse_session_recording_events_test', 'group1', 'JSONEachRow')
  
  '
---
# name: test_create_table_query[performance_events]
  '
  
  CREATE TABLE IF NOT EXISTS performance_events ON CLUSTER 'posthog'
  (
      uuid UUID,
  session_id String,
  window_id String,
  pageview_id String,
  distinct_id String,
  timestamp DateTime64,
  time_origin DateTime64(3, 'UTC'),
  entry_type LowCardinality(String),
  name String,
  team_id Int64,
  current_url String,
  start_time Float64,
  duration Float64,
  redirect_start Float64,
  redirect_end Float64,
  worker_start Float64,
  fetch_start Float64,
  domain_lookup_start Float64,
  domain_lookup_end Float64,
  connect_start Float64,
  secure_connection_start Float64,
  connect_end Float64,
  request_start Float64,
  response_start Float64,
  response_end Float64,
  decoded_body_size Int64,
  encoded_body_size Int64,
  initiator_type LowCardinality(String),
  next_hop_protocol LowCardinality(String),
  render_blocking_status LowCardinality(String),
  response_status Int64,
  transfer_size Int64,
  largest_contentful_paint_element String,
  largest_contentful_paint_render_time Float64,
  largest_contentful_paint_load_time Float64,
  largest_contentful_paint_size Float64,
  largest_contentful_paint_id String,
  largest_contentful_paint_url String,
  dom_complete Float64,
  dom_content_loaded_event Float64,
  dom_interactive Float64,
  load_event_end Float64,
  load_event_start Float64,
  redirect_count Int64,
  navigation_type LowCardinality(String),
  unload_event_end Float64,
  unload_event_start Float64
      
  , _timestamp DateTime
  , _offset UInt64
  , _partition UInt64
  
  ) ENGINE = Distributed('posthog', 'posthog_test', 'sharded_performance_events', sipHash64(session_id))
  
  '
---
# name: test_create_table_query[performance_events_mv]
  '
  
  CREATE MATERIALIZED VIEW performance_events_mv ON CLUSTER 'posthog'
  TO posthog_test.writeable_performance_events
  AS SELECT
  uuid, session_id, window_id, pageview_id, distinct_id, timestamp, time_origin, entry_type, name, team_id, current_url, start_time, duration, redirect_start, redirect_end, worker_start, fetch_start, domain_lookup_start, domain_lookup_end, connect_start, secure_connection_start, connect_end, request_start, response_start, response_end, decoded_body_size, encoded_body_size, initiator_type, next_hop_protocol, render_blocking_status, response_status, transfer_size, largest_contentful_paint_element, largest_contentful_paint_render_time, largest_contentful_paint_load_time, largest_contentful_paint_size, largest_contentful_paint_id, largest_contentful_paint_url, dom_complete, dom_content_loaded_event, dom_interactive, load_event_end, load_event_start, redirect_count, navigation_type, unload_event_end, unload_event_start
  ,_timestamp, _offset, _partition
  FROM posthog_test.kafka_performance_events
  
  '
---
# name: test_create_table_query[person]
  '
  
  CREATE TABLE IF NOT EXISTS person ON CLUSTER 'posthog'
  (
      id UUID,
      created_at DateTime64,
      team_id Int64,
      properties VARCHAR,
      is_identified Int8,
      is_deleted Int8 DEFAULT 0,
      version UInt64
      
      
  , _timestamp DateTime
  , _offset UInt64
  
      , INDEX kafka_timestamp_minmax_person _timestamp TYPE minmax GRANULARITY 3
      
  ) ENGINE = ReplacingMergeTree(version)
  Order By (team_id, id)
  
  
  '
---
# name: test_create_table_query[person_distinct_id2]
  '
  
  CREATE TABLE IF NOT EXISTS person_distinct_id2 ON CLUSTER 'posthog'
  (
      team_id Int64,
      distinct_id VARCHAR,
      person_id UUID,
      is_deleted Int8,
      version Int64 DEFAULT 1
      
      
  , _timestamp DateTime
  , _offset UInt64
  
      , _partition UInt64
      , INDEX kafka_timestamp_minmax_person_distinct_id2 _timestamp TYPE minmax GRANULARITY 3
      
  ) ENGINE = ReplacingMergeTree(version)
  
      ORDER BY (team_id, distinct_id)
      SETTINGS index_granularity = 512
      
  '
---
# name: test_create_table_query[person_distinct_id2_mv]
  '
  
  CREATE MATERIALIZED VIEW person_distinct_id2_mv ON CLUSTER 'posthog'
  TO posthog_test.person_distinct_id2
  AS SELECT
  team_id,
  distinct_id,
  person_id,
  is_deleted,
  version,
  _timestamp,
  _offset,
  _partition
  FROM posthog_test.kafka_person_distinct_id2
  
  '
---
# name: test_create_table_query[person_distinct_id]
  '
  
  CREATE TABLE IF NOT EXISTS person_distinct_id ON CLUSTER 'posthog'
  (
      distinct_id VARCHAR,
      person_id UUID,
      team_id Int64,
      _sign Int8 DEFAULT 1,
      is_deleted Int8 ALIAS if(_sign==-1, 1, 0)
      
  , _timestamp DateTime
  , _offset UInt64
  
  ) ENGINE = CollapsingMergeTree(_sign)
  Order By (team_id, distinct_id, person_id)
  
  
  '
---
# name: test_create_table_query[person_distinct_id_mv]
  '
  
  CREATE MATERIALIZED VIEW person_distinct_id_mv ON CLUSTER 'posthog'
  TO posthog_test.person_distinct_id
  AS SELECT
  distinct_id,
  person_id,
  team_id,
  coalesce(_sign, if(is_deleted==0, 1, -1)) AS _sign,
  _timestamp,
  _offset
  FROM posthog_test.kafka_person_distinct_id
  
  '
---
# name: test_create_table_query[person_mv]
  '
  
  CREATE MATERIALIZED VIEW person_mv ON CLUSTER 'posthog'
  TO posthog_test.person
  AS SELECT
  id,
  created_at,
  team_id,
  properties,
  is_identified,
  is_deleted,
  version,
  _timestamp,
  _offset
  FROM posthog_test.kafka_person
  
  '
---
# name: test_create_table_query[person_static_cohort]
  '
  
  CREATE TABLE IF NOT EXISTS person_static_cohort ON CLUSTER 'posthog'
  (
      id UUID,
      person_id UUID,
      cohort_id Int64,
      team_id Int64
      
  , _timestamp DateTime
  , _offset UInt64
  
  ) ENGINE = ReplacingMergeTree(_timestamp)
  Order By (team_id, cohort_id, person_id, id)
  
  
  '
---
# name: test_create_table_query[plugin_log_entries]
  '
  
  CREATE TABLE IF NOT EXISTS plugin_log_entries ON CLUSTER 'posthog'
  (
      id UUID,
      team_id Int64,
      plugin_id Int64,
      plugin_config_id Int64,
      timestamp DateTime64(6, 'UTC'),
      source VARCHAR,
      type VARCHAR,
      message VARCHAR,
      instance_id UUID
      
  , _timestamp DateTime
  , _offset UInt64
  
  ) ENGINE = ReplacingMergeTree(_timestamp)
  PARTITION BY plugin_id ORDER BY (team_id, id)
  
  SETTINGS index_granularity=512
  
  '
---
# name: test_create_table_query[plugin_log_entries_mv]
  '
  
  CREATE MATERIALIZED VIEW plugin_log_entries_mv ON CLUSTER 'posthog'
  TO posthog_test.plugin_log_entries
  AS SELECT
  id,
  team_id,
  plugin_id,
  plugin_config_id,
  timestamp,
  source,
  type,
  message,
  instance_id,
  _timestamp,
  _offset
  FROM posthog_test.kafka_plugin_log_entries
  
  '
---
# name: test_create_table_query[session_recording_events]
  '
  
  CREATE TABLE IF NOT EXISTS session_recording_events ON CLUSTER 'posthog'
  (
      uuid UUID,
      timestamp DateTime64(6, 'UTC'),
      team_id Int64,
      distinct_id VARCHAR,
      session_id VARCHAR,
      window_id VARCHAR,
      snapshot_data VARCHAR,
      created_at DateTime64(6, 'UTC')
      , has_full_snapshot Int8 COMMENT 'column_materializer::has_full_snapshot', events_summary Array(String) COMMENT 'column_materializer::events_summary', click_count Int8 COMMENT 'column_materializer::click_count', keypress_count Int8 COMMENT 'column_materializer::keypress_count', timestamps_summary Array(DateTime64(6, 'UTC')) COMMENT 'column_materializer::timestamps_summary', first_event_timestamp Nullable(DateTime64(6, 'UTC')) COMMENT 'column_materializer::first_event_timestamp', last_event_timestamp Nullable(DateTime64(6, 'UTC')) COMMENT 'column_materializer::last_event_timestamp', urls Array(String) COMMENT 'column_materializer::urls'
      
  , _timestamp DateTime
  , _offset UInt64
  
  ) ENGINE = Distributed('posthog', 'posthog_test', 'session_recording_events', sipHash64(distinct_id))
  
  '
---
# name: test_create_table_query[session_recording_events_mv]
  '
  
  CREATE MATERIALIZED VIEW session_recording_events_mv ON CLUSTER 'posthog'
  TO posthog_test.session_recording_events
  AS SELECT
  uuid,
  timestamp,
  team_id,
  distinct_id,
  session_id,
  window_id,
  snapshot_data,
  created_at,
  _timestamp,
  _offset
  FROM posthog_test.kafka_session_recording_events
  
  '
---
# name: test_create_table_query[sharded_app_metrics]
  '
  
  CREATE TABLE sharded_app_metrics ON CLUSTER 'posthog'
  (
      team_id Int64,
      timestamp DateTime64(6, 'UTC'),
      plugin_config_id Int64,
      category LowCardinality(String),
      job_id String,
      successes SimpleAggregateFunction(sum, Int64),
      successes_on_retry SimpleAggregateFunction(sum, Int64),
      failures SimpleAggregateFunction(sum, Int64),
      error_uuid UUID,
      error_type String,
      error_details String CODEC(ZSTD(3))
      
  , _timestamp DateTime
  , _offset UInt64
  , _partition UInt64
  
  )
  ENGINE = AggregatingMergeTree()
  PARTITION BY toYYYYMM(timestamp)
  ORDER BY (team_id, plugin_config_id, job_id, category, toStartOfHour(timestamp), error_type, error_uuid)
  
  '
---
# name: test_create_table_query[sharded_events]
  '
  
  CREATE TABLE IF NOT EXISTS events ON CLUSTER 'posthog'
  (
      uuid UUID,
      event VARCHAR,
      properties VARCHAR CODEC(ZSTD(3)),
      timestamp DateTime64(6, 'UTC'),
      team_id Int64,
      distinct_id VARCHAR,
      elements_chain VARCHAR,
      created_at DateTime64(6, 'UTC'),
      person_id UUID,
      person_created_at DateTime64,
      person_properties VARCHAR Codec(ZSTD(3)),
      group0_properties VARCHAR Codec(ZSTD(3)),
      group1_properties VARCHAR Codec(ZSTD(3)),
      group2_properties VARCHAR Codec(ZSTD(3)),
      group3_properties VARCHAR Codec(ZSTD(3)),
      group4_properties VARCHAR Codec(ZSTD(3)),
      group0_created_at DateTime64,
      group1_created_at DateTime64,
      group2_created_at DateTime64,
      group3_created_at DateTime64,
      group4_created_at DateTime64
      
      , $group_0 VARCHAR MATERIALIZED replaceRegexpAll(JSONExtractRaw(properties, '$group_0'), '^"|"$', '') COMMENT 'column_materializer::$group_0'
      , $group_1 VARCHAR MATERIALIZED replaceRegexpAll(JSONExtractRaw(properties, '$group_1'), '^"|"$', '') COMMENT 'column_materializer::$group_1'
      , $group_2 VARCHAR MATERIALIZED replaceRegexpAll(JSONExtractRaw(properties, '$group_2'), '^"|"$', '') COMMENT 'column_materializer::$group_2'
      , $group_3 VARCHAR MATERIALIZED replaceRegexpAll(JSONExtractRaw(properties, '$group_3'), '^"|"$', '') COMMENT 'column_materializer::$group_3'
      , $group_4 VARCHAR MATERIALIZED replaceRegexpAll(JSONExtractRaw(properties, '$group_4'), '^"|"$', '') COMMENT 'column_materializer::$group_4'
      , $window_id VARCHAR MATERIALIZED replaceRegexpAll(JSONExtractRaw(properties, '$window_id'), '^"|"$', '') COMMENT 'column_materializer::$window_id'
      , $session_id VARCHAR MATERIALIZED replaceRegexpAll(JSONExtractRaw(properties, '$session_id'), '^"|"$', '') COMMENT 'column_materializer::$session_id'
  
      
  , _timestamp DateTime
  , _offset UInt64
  
      
      , PROJECTION fast_max_kafka_timestamp_events (SELECT max(_timestamp))
      , INDEX kafka_timestamp_minmax_events _timestamp TYPE minmax GRANULARITY 3
      
  ) ENGINE = ReplacingMergeTree(_timestamp)
  PARTITION BY toYYYYMM(timestamp)
  ORDER BY (team_id, toDate(timestamp), event, cityHash64(distinct_id), cityHash64(uuid))
  SAMPLE BY cityHash64(distinct_id)
  
  
  '
---
# name: test_create_table_query[sharded_ingestion_warnings]
  '
  
  CREATE TABLE IF NOT EXISTS sharded_ingestion_warnings ON CLUSTER 'posthog'
  (
      team_id Int64,
      source LowCardinality(VARCHAR),
      type VARCHAR,
      details VARCHAR CODEC(ZSTD(3)),
      timestamp DateTime64(6, 'UTC')
      
  , _timestamp DateTime
  , _offset UInt64
  , _partition UInt64
  
  ) ENGINE = MergeTree()
  PARTITION BY toYYYYMMDD(timestamp)
  ORDER BY (team_id, toHour(timestamp), type, source, timestamp)
  
  '
---
# name: test_create_table_query[sharded_performance_events]
  '
  
  CREATE TABLE IF NOT EXISTS sharded_performance_events ON CLUSTER 'posthog'
  (
      uuid UUID,
  session_id String,
  window_id String,
  pageview_id String,
  distinct_id String,
  timestamp DateTime64,
  time_origin DateTime64(3, 'UTC'),
  entry_type LowCardinality(String),
  name String,
  team_id Int64,
  current_url String,
  start_time Float64,
  duration Float64,
  redirect_start Float64,
  redirect_end Float64,
  worker_start Float64,
  fetch_start Float64,
  domain_lookup_start Float64,
  domain_lookup_end Float64,
  connect_start Float64,
  secure_connection_start Float64,
  connect_end Float64,
  request_start Float64,
  response_start Float64,
  response_end Float64,
  decoded_body_size Int64,
  encoded_body_size Int64,
  initiator_type LowCardinality(String),
  next_hop_protocol LowCardinality(String),
  render_blocking_status LowCardinality(String),
  response_status Int64,
  transfer_size Int64,
  largest_contentful_paint_element String,
  largest_contentful_paint_render_time Float64,
  largest_contentful_paint_load_time Float64,
  largest_contentful_paint_size Float64,
  largest_contentful_paint_id String,
  largest_contentful_paint_url String,
  dom_complete Float64,
  dom_content_loaded_event Float64,
  dom_interactive Float64,
  load_event_end Float64,
  load_event_start Float64,
  redirect_count Int64,
  navigation_type LowCardinality(String),
  unload_event_end Float64,
  unload_event_start Float64
      
  , _timestamp DateTime
  , _offset UInt64
  , _partition UInt64
  
  ) ENGINE = MergeTree()
  PARTITION BY toYYYYMM(timestamp)
  ORDER BY (team_id, toDate(timestamp), session_id, pageview_id, timestamp)
  
  
  '
---
# name: test_create_table_query[sharded_session_recording_events]
  '
  
  CREATE TABLE IF NOT EXISTS session_recording_events ON CLUSTER 'posthog'
  (
      uuid UUID,
      timestamp DateTime64(6, 'UTC'),
      team_id Int64,
      distinct_id VARCHAR,
      session_id VARCHAR,
      window_id VARCHAR,
      snapshot_data VARCHAR,
      created_at DateTime64(6, 'UTC')
      , has_full_snapshot Int8 MATERIALIZED JSONExtractBool(snapshot_data, 'has_full_snapshot'), events_summary Array(String) MATERIALIZED JSONExtract(JSON_QUERY(snapshot_data, '$.events_summary[*]'), 'Array(String)'), click_count Int8 MATERIALIZED length(arrayFilter((x) -> JSONExtractInt(x, 'type') = 3 AND JSONExtractInt(x, 'data', 'source') = 2 AND JSONExtractInt(x, 'data', 'source') = 2, events_summary)), keypress_count Int8 MATERIALIZED length(arrayFilter((x) -> JSONExtractInt(x, 'type') = 3 AND JSONExtractInt(x, 'data', 'source') = 5, events_summary)), timestamps_summary Array(DateTime64(6, 'UTC')) MATERIALIZED arraySort(arrayMap((x) -> toDateTime(JSONExtractInt(x, 'timestamp') / 1000), events_summary)), first_event_timestamp Nullable(DateTime64(6, 'UTC')) MATERIALIZED if(empty(timestamps_summary), NULL, arrayReduce('min', timestamps_summary)), last_event_timestamp Nullable(DateTime64(6, 'UTC')) MATERIALIZED if(empty(timestamps_summary), NULL, arrayReduce('max', timestamps_summary)), urls Array(String) MATERIALIZED arrayFilter(x -> x != '', arrayMap((x) -> JSONExtractString(x, 'data', 'href'), events_summary))
      
      
  , _timestamp DateTime
  , _offset UInt64
  
      , INDEX kafka_timestamp_minmax_session_recording_events _timestamp TYPE minmax GRANULARITY 3
      
  ) ENGINE = ReplacingMergeTree(_timestamp)
  PARTITION BY toYYYYMMDD(timestamp)
  ORDER BY (team_id, toHour(timestamp), session_id, timestamp, uuid)
  
  SETTINGS index_granularity=512
  
  '
---
# name: test_create_table_query[writable_events]
  '
  
  CREATE TABLE IF NOT EXISTS writable_events ON CLUSTER 'posthog'
  (
      uuid UUID,
      event VARCHAR,
      properties VARCHAR CODEC(ZSTD(3)),
      timestamp DateTime64(6, 'UTC'),
      team_id Int64,
      distinct_id VARCHAR,
      elements_chain VARCHAR,
      created_at DateTime64(6, 'UTC'),
      person_id UUID,
      person_created_at DateTime64,
      person_properties VARCHAR Codec(ZSTD(3)),
      group0_properties VARCHAR Codec(ZSTD(3)),
      group1_properties VARCHAR Codec(ZSTD(3)),
      group2_properties VARCHAR Codec(ZSTD(3)),
      group3_properties VARCHAR Codec(ZSTD(3)),
      group4_properties VARCHAR Codec(ZSTD(3)),
      group0_created_at DateTime64,
      group1_created_at DateTime64,
      group2_created_at DateTime64,
      group3_created_at DateTime64,
      group4_created_at DateTime64
      
      
  , _timestamp DateTime
  , _offset UInt64
  
      
  ) ENGINE = Distributed('posthog', 'posthog_test', 'events', sipHash64(distinct_id))
  
  '
---
# name: test_create_table_query[writable_session_recording_events]
  '
  
  CREATE TABLE IF NOT EXISTS writable_session_recording_events ON CLUSTER 'posthog'
  (
      uuid UUID,
      timestamp DateTime64(6, 'UTC'),
      team_id Int64,
      distinct_id VARCHAR,
      session_id VARCHAR,
      window_id VARCHAR,
      snapshot_data VARCHAR,
      created_at DateTime64(6, 'UTC')
      
      
  , _timestamp DateTime
  , _offset UInt64
  
  ) ENGINE = Distributed('posthog', 'posthog_test', 'session_recording_events', sipHash64(distinct_id))
  
  '
---
# name: test_create_table_query[writeable_performance_events]
  '
  
  CREATE TABLE IF NOT EXISTS writeable_performance_events ON CLUSTER 'posthog'
  (
      uuid UUID,
  session_id String,
  window_id String,
  pageview_id String,
  distinct_id String,
  timestamp DateTime64,
  time_origin DateTime64(3, 'UTC'),
  entry_type LowCardinality(String),
  name String,
  team_id Int64,
  current_url String,
  start_time Float64,
  duration Float64,
  redirect_start Float64,
  redirect_end Float64,
  worker_start Float64,
  fetch_start Float64,
  domain_lookup_start Float64,
  domain_lookup_end Float64,
  connect_start Float64,
  secure_connection_start Float64,
  connect_end Float64,
  request_start Float64,
  response_start Float64,
  response_end Float64,
  decoded_body_size Int64,
  encoded_body_size Int64,
  initiator_type LowCardinality(String),
  next_hop_protocol LowCardinality(String),
  render_blocking_status LowCardinality(String),
  response_status Int64,
  transfer_size Int64,
  largest_contentful_paint_element String,
  largest_contentful_paint_render_time Float64,
  largest_contentful_paint_load_time Float64,
  largest_contentful_paint_size Float64,
  largest_contentful_paint_id String,
  largest_contentful_paint_url String,
  dom_complete Float64,
  dom_content_loaded_event Float64,
  dom_interactive Float64,
  load_event_end Float64,
  load_event_start Float64,
  redirect_count Int64,
  navigation_type LowCardinality(String),
  unload_event_end Float64,
  unload_event_start Float64
      
  , _timestamp DateTime
  , _offset UInt64
  , _partition UInt64
  
  ) ENGINE = Distributed('posthog', 'posthog_test', 'sharded_performance_events', sipHash64(session_id))
  
  '
---
# name: test_create_table_query_replicated_and_storage[cohortpeople]
  '
  
  CREATE TABLE IF NOT EXISTS cohortpeople ON CLUSTER 'posthog'
  (
      person_id UUID,
      cohort_id Int64,
      team_id Int64,
      sign Int8,
      version UInt64
  ) ENGINE = ReplicatedCollapsingMergeTree('/clickhouse/tables/77f1df52-4b43-11e9-910f-b8ca3a9b9f3e_noshard/posthog.cohortpeople', '{replica}-{shard}', sign)
  Order By (team_id, cohort_id, person_id, version)
  
  
  '
---
# name: test_create_table_query_replicated_and_storage[events_dead_letter_queue]
  '
  
  CREATE TABLE IF NOT EXISTS events_dead_letter_queue ON CLUSTER 'posthog'
  (
      id UUID,
      event_uuid UUID,
      event VARCHAR,
      properties VARCHAR,
      distinct_id VARCHAR,
      team_id Int64,
      elements_chain VARCHAR,
      created_at DateTime64(6, 'UTC'),
      ip VARCHAR,
      site_url VARCHAR,
      now DateTime64(6, 'UTC'),
      raw_payload VARCHAR,
      error_timestamp DateTime64(6, 'UTC'),
      error_location VARCHAR,
      error VARCHAR,
      tags Array(VARCHAR)
      
      
  , _timestamp DateTime
  , _offset UInt64
  
      , INDEX kafka_timestamp_minmax_events_dead_letter_queue _timestamp TYPE minmax GRANULARITY 3
      
  ) ENGINE = ReplicatedReplacingMergeTree('/clickhouse/tables/77f1df52-4b43-11e9-910f-b8ca3a9b9f3e_noshard/posthog.events_dead_letter_queue', '{replica}-{shard}', _timestamp)
  ORDER BY (id, event_uuid, distinct_id, team_id)
  
  SETTINGS index_granularity=512
  
  '
---
# name: test_create_table_query_replicated_and_storage[groups]
  '
  
  CREATE TABLE IF NOT EXISTS groups ON CLUSTER 'posthog'
  (
      group_type_index UInt8,
      group_key VARCHAR,
      created_at DateTime64,
      team_id Int64,
      group_properties VARCHAR
      
  , _timestamp DateTime
  , _offset UInt64
  
  ) ENGINE = ReplicatedReplacingMergeTree('/clickhouse/tables/77f1df52-4b43-11e9-910f-b8ca3a9b9f3e_noshard/posthog.groups', '{replica}-{shard}', _timestamp)
  Order By (team_id, group_type_index, group_key)
  SETTINGS storage_policy = 'hot_to_cold'
  
  '
---
# name: test_create_table_query_replicated_and_storage[person]
  '
  
  CREATE TABLE IF NOT EXISTS person ON CLUSTER 'posthog'
  (
      id UUID,
      created_at DateTime64,
      team_id Int64,
      properties VARCHAR,
      is_identified Int8,
      is_deleted Int8 DEFAULT 0,
      version UInt64
      
      
  , _timestamp DateTime
  , _offset UInt64
  
      , INDEX kafka_timestamp_minmax_person _timestamp TYPE minmax GRANULARITY 3
      
  ) ENGINE = ReplicatedReplacingMergeTree('/clickhouse/tables/77f1df52-4b43-11e9-910f-b8ca3a9b9f3e_noshard/posthog.person', '{replica}-{shard}', version)
  Order By (team_id, id)
  SETTINGS storage_policy = 'hot_to_cold'
  
  '
---
# name: test_create_table_query_replicated_and_storage[person_distinct_id2]
  '
  
  CREATE TABLE IF NOT EXISTS person_distinct_id2 ON CLUSTER 'posthog'
  (
      team_id Int64,
      distinct_id VARCHAR,
      person_id UUID,
      is_deleted Int8,
      version Int64 DEFAULT 1
      
      
  , _timestamp DateTime
  , _offset UInt64
  
      , _partition UInt64
      , INDEX kafka_timestamp_minmax_person_distinct_id2 _timestamp TYPE minmax GRANULARITY 3
      
  ) ENGINE = ReplicatedReplacingMergeTree('/clickhouse/tables/77f1df52-4b43-11e9-910f-b8ca3a9b9f3e_noshard/posthog.person_distinct_id2', '{replica}-{shard}', version)
  
      ORDER BY (team_id, distinct_id)
      SETTINGS index_granularity = 512
      
  '
---
# name: test_create_table_query_replicated_and_storage[person_distinct_id]
  '
  
  CREATE TABLE IF NOT EXISTS person_distinct_id ON CLUSTER 'posthog'
  (
      distinct_id VARCHAR,
      person_id UUID,
      team_id Int64,
      _sign Int8 DEFAULT 1,
      is_deleted Int8 ALIAS if(_sign==-1, 1, 0)
      
  , _timestamp DateTime
  , _offset UInt64
  
  ) ENGINE = ReplicatedCollapsingMergeTree('/clickhouse/tables/77f1df52-4b43-11e9-910f-b8ca3a9b9f3e_noshard/posthog.person_distinct_id', '{replica}-{shard}', _sign)
  Order By (team_id, distinct_id, person_id)
  SETTINGS storage_policy = 'hot_to_cold'
  
  '
---
# name: test_create_table_query_replicated_and_storage[person_static_cohort]
  '
  
  CREATE TABLE IF NOT EXISTS person_static_cohort ON CLUSTER 'posthog'
  (
      id UUID,
      person_id UUID,
      cohort_id Int64,
      team_id Int64
      
  , _timestamp DateTime
  , _offset UInt64
  
  ) ENGINE = ReplicatedReplacingMergeTree('/clickhouse/tables/77f1df52-4b43-11e9-910f-b8ca3a9b9f3e_noshard/posthog.person_static_cohort', '{replica}-{shard}', _timestamp)
  Order By (team_id, cohort_id, person_id, id)
  SETTINGS storage_policy = 'hot_to_cold'
  
  '
---
# name: test_create_table_query_replicated_and_storage[plugin_log_entries]
  '
  
  CREATE TABLE IF NOT EXISTS plugin_log_entries ON CLUSTER 'posthog'
  (
      id UUID,
      team_id Int64,
      plugin_id Int64,
      plugin_config_id Int64,
      timestamp DateTime64(6, 'UTC'),
      source VARCHAR,
      type VARCHAR,
      message VARCHAR,
      instance_id UUID
      
  , _timestamp DateTime
  , _offset UInt64
  
  ) ENGINE = ReplicatedReplacingMergeTree('/clickhouse/tables/77f1df52-4b43-11e9-910f-b8ca3a9b9f3e_noshard/posthog.plugin_log_entries', '{replica}-{shard}', _timestamp)
  PARTITION BY plugin_id ORDER BY (team_id, id)
  
  SETTINGS index_granularity=512
  
  '
---
# name: test_create_table_query_replicated_and_storage[sharded_app_metrics]
  '
  
  CREATE TABLE sharded_app_metrics ON CLUSTER 'posthog'
  (
      team_id Int64,
      timestamp DateTime64(6, 'UTC'),
      plugin_config_id Int64,
      category LowCardinality(String),
      job_id String,
      successes SimpleAggregateFunction(sum, Int64),
      successes_on_retry SimpleAggregateFunction(sum, Int64),
      failures SimpleAggregateFunction(sum, Int64),
      error_uuid UUID,
      error_type String,
      error_details String CODEC(ZSTD(3))
      
  , _timestamp DateTime
  , _offset UInt64
  , _partition UInt64
  
  )
  ENGINE = ReplicatedAggregatingMergeTree('/clickhouse/tables/77f1df52-4b43-11e9-910f-b8ca3a9b9f3e_{shard}/posthog.sharded_app_metrics', '{replica}')
  PARTITION BY toYYYYMM(timestamp)
  ORDER BY (team_id, plugin_config_id, job_id, category, toStartOfHour(timestamp), error_type, error_uuid)
  
  '
---
# name: test_create_table_query_replicated_and_storage[sharded_events]
  '
  
  CREATE TABLE IF NOT EXISTS sharded_events ON CLUSTER 'posthog'
  (
      uuid UUID,
      event VARCHAR,
      properties VARCHAR CODEC(ZSTD(3)),
      timestamp DateTime64(6, 'UTC'),
      team_id Int64,
      distinct_id VARCHAR,
      elements_chain VARCHAR,
      created_at DateTime64(6, 'UTC'),
      person_id UUID,
      person_created_at DateTime64,
      person_properties VARCHAR Codec(ZSTD(3)),
      group0_properties VARCHAR Codec(ZSTD(3)),
      group1_properties VARCHAR Codec(ZSTD(3)),
      group2_properties VARCHAR Codec(ZSTD(3)),
      group3_properties VARCHAR Codec(ZSTD(3)),
      group4_properties VARCHAR Codec(ZSTD(3)),
      group0_created_at DateTime64,
      group1_created_at DateTime64,
      group2_created_at DateTime64,
      group3_created_at DateTime64,
      group4_created_at DateTime64
      
      , $group_0 VARCHAR MATERIALIZED replaceRegexpAll(JSONExtractRaw(properties, '$group_0'), '^"|"$', '') COMMENT 'column_materializer::$group_0'
      , $group_1 VARCHAR MATERIALIZED replaceRegexpAll(JSONExtractRaw(properties, '$group_1'), '^"|"$', '') COMMENT 'column_materializer::$group_1'
      , $group_2 VARCHAR MATERIALIZED replaceRegexpAll(JSONExtractRaw(properties, '$group_2'), '^"|"$', '') COMMENT 'column_materializer::$group_2'
      , $group_3 VARCHAR MATERIALIZED replaceRegexpAll(JSONExtractRaw(properties, '$group_3'), '^"|"$', '') COMMENT 'column_materializer::$group_3'
      , $group_4 VARCHAR MATERIALIZED replaceRegexpAll(JSONExtractRaw(properties, '$group_4'), '^"|"$', '') COMMENT 'column_materializer::$group_4'
      , $window_id VARCHAR MATERIALIZED replaceRegexpAll(JSONExtractRaw(properties, '$window_id'), '^"|"$', '') COMMENT 'column_materializer::$window_id'
      , $session_id VARCHAR MATERIALIZED replaceRegexpAll(JSONExtractRaw(properties, '$session_id'), '^"|"$', '') COMMENT 'column_materializer::$session_id'
  
      
  , _timestamp DateTime
  , _offset UInt64
  
      
      , PROJECTION fast_max_kafka_timestamp_sharded_events (SELECT max(_timestamp))
      , INDEX kafka_timestamp_minmax_sharded_events _timestamp TYPE minmax GRANULARITY 3
      
  ) ENGINE = ReplicatedReplacingMergeTree('/clickhouse/tables/77f1df52-4b43-11e9-910f-b8ca3a9b9f3e_{shard}/posthog.events', '{replica}', _timestamp)
  PARTITION BY toYYYYMM(timestamp)
  ORDER BY (team_id, toDate(timestamp), event, cityHash64(distinct_id), cityHash64(uuid))
  SAMPLE BY cityHash64(distinct_id)
  SETTINGS storage_policy = 'hot_to_cold'
  
  '
---
# name: test_create_table_query_replicated_and_storage[sharded_ingestion_warnings]
  '
  
  CREATE TABLE IF NOT EXISTS sharded_ingestion_warnings ON CLUSTER 'posthog'
  (
      team_id Int64,
      source LowCardinality(VARCHAR),
      type VARCHAR,
      details VARCHAR CODEC(ZSTD(3)),
      timestamp DateTime64(6, 'UTC')
      
  , _timestamp DateTime
  , _offset UInt64
  , _partition UInt64
  
  ) ENGINE = ReplicatedMergeTree('/clickhouse/tables/77f1df52-4b43-11e9-910f-b8ca3a9b9f3e_{shard}/posthog.sharded_ingestion_warnings', '{replica}')
  PARTITION BY toYYYYMMDD(timestamp)
  ORDER BY (team_id, toHour(timestamp), type, source, timestamp)
  
  '
---
# name: test_create_table_query_replicated_and_storage[sharded_performance_events]
  '
  
  CREATE TABLE IF NOT EXISTS sharded_performance_events ON CLUSTER 'posthog'
  (
      uuid UUID,
  session_id String,
  window_id String,
  pageview_id String,
  distinct_id String,
  timestamp DateTime64,
  time_origin DateTime64(3, 'UTC'),
  entry_type LowCardinality(String),
  name String,
  team_id Int64,
  current_url String,
  start_time Float64,
  duration Float64,
  redirect_start Float64,
  redirect_end Float64,
  worker_start Float64,
  fetch_start Float64,
  domain_lookup_start Float64,
  domain_lookup_end Float64,
  connect_start Float64,
  secure_connection_start Float64,
  connect_end Float64,
  request_start Float64,
  response_start Float64,
  response_end Float64,
  decoded_body_size Int64,
  encoded_body_size Int64,
  initiator_type LowCardinality(String),
  next_hop_protocol LowCardinality(String),
  render_blocking_status LowCardinality(String),
  response_status Int64,
  transfer_size Int64,
  largest_contentful_paint_element String,
  largest_contentful_paint_render_time Float64,
  largest_contentful_paint_load_time Float64,
  largest_contentful_paint_size Float64,
  largest_contentful_paint_id String,
  largest_contentful_paint_url String,
  dom_complete Float64,
  dom_content_loaded_event Float64,
  dom_interactive Float64,
  load_event_end Float64,
  load_event_start Float64,
  redirect_count Int64,
  navigation_type LowCardinality(String),
  unload_event_end Float64,
  unload_event_start Float64
      
  , _timestamp DateTime
  , _offset UInt64
  , _partition UInt64
  
  ) ENGINE = ReplicatedMergeTree('/clickhouse/tables/77f1df52-4b43-11e9-910f-b8ca3a9b9f3e_{shard}/posthog.performance_events', '{replica}')
  PARTITION BY toYYYYMM(timestamp)
  ORDER BY (team_id, toDate(timestamp), session_id, pageview_id, timestamp)
  SETTINGS storage_policy = 'hot_to_cold'
  
  '
---
# name: test_create_table_query_replicated_and_storage[sharded_session_recording_events]
  '
  
  CREATE TABLE IF NOT EXISTS sharded_session_recording_events ON CLUSTER 'posthog'
  (
      uuid UUID,
      timestamp DateTime64(6, 'UTC'),
      team_id Int64,
      distinct_id VARCHAR,
      session_id VARCHAR,
      window_id VARCHAR,
      snapshot_data VARCHAR,
      created_at DateTime64(6, 'UTC')
      , has_full_snapshot Int8 MATERIALIZED JSONExtractBool(snapshot_data, 'has_full_snapshot'), events_summary Array(String) MATERIALIZED JSONExtract(JSON_QUERY(snapshot_data, '$.events_summary[*]'), 'Array(String)'), click_count Int8 MATERIALIZED length(arrayFilter((x) -> JSONExtractInt(x, 'type') = 3 AND JSONExtractInt(x, 'data', 'source') = 2 AND JSONExtractInt(x, 'data', 'source') = 2, events_summary)), keypress_count Int8 MATERIALIZED length(arrayFilter((x) -> JSONExtractInt(x, 'type') = 3 AND JSONExtractInt(x, 'data', 'source') = 5, events_summary)), timestamps_summary Array(DateTime64(6, 'UTC')) MATERIALIZED arraySort(arrayMap((x) -> toDateTime(JSONExtractInt(x, 'timestamp') / 1000), events_summary)), first_event_timestamp Nullable(DateTime64(6, 'UTC')) MATERIALIZED if(empty(timestamps_summary), NULL, arrayReduce('min', timestamps_summary)), last_event_timestamp Nullable(DateTime64(6, 'UTC')) MATERIALIZED if(empty(timestamps_summary), NULL, arrayReduce('max', timestamps_summary)), urls Array(String) MATERIALIZED arrayFilter(x -> x != '', arrayMap((x) -> JSONExtractString(x, 'data', 'href'), events_summary))
      
      
  , _timestamp DateTime
  , _offset UInt64
  
      , INDEX kafka_timestamp_minmax_sharded_session_recording_events _timestamp TYPE minmax GRANULARITY 3
      
  ) ENGINE = ReplicatedReplacingMergeTree('/clickhouse/tables/77f1df52-4b43-11e9-910f-b8ca3a9b9f3e_{shard}/posthog.session_recording_events', '{replica}', _timestamp)
  PARTITION BY toYYYYMMDD(timestamp)
  ORDER BY (team_id, toHour(timestamp), session_id, timestamp, uuid)
  
  SETTINGS index_granularity=512
  
  '
---
