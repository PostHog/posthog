import os

from posthog.settings.utils import get_from_env, get_list
from posthog.utils import str_to_bool

BATCH_EXPORT_S3_UPLOAD_CHUNK_SIZE_BYTES: int = 1024 * 1024 * 50  # 50MB
BATCH_EXPORT_S3_RECORD_BATCH_QUEUE_MAX_SIZE_BYTES: int = get_from_env(
    "BATCH_EXPORT_S3_RECORD_BATCH_QUEUE_MAX_SIZE_BYTES", 0, type_cast=int
)
BATCH_EXPORT_S3_MAX_CONCURRENT_UPLOADS: int = get_from_env("BATCH_EXPORT_S3_MAX_CONCURRENT_UPLOADS", 5, type_cast=int)

BATCH_EXPORT_SNOWFLAKE_UPLOAD_CHUNK_SIZE_BYTES: int = 1024 * 1024 * 100  # 100MB
BATCH_EXPORT_SNOWFLAKE_RECORD_BATCH_QUEUE_MAX_SIZE_BYTES: int = get_from_env(
    "BATCH_EXPORT_SNOWFLAKE_RECORD_BATCH_QUEUE_MAX_SIZE_BYTES", 1024 * 1024 * 300, type_cast=int
)

BATCH_EXPORT_POSTGRES_UPLOAD_CHUNK_SIZE_BYTES: int = 1024 * 1024 * 50  # 50MB
BATCH_EXPORT_POSTGRES_RECORD_BATCH_QUEUE_MAX_SIZE_BYTES: int = get_from_env(
    "BATCH_EXPORT_POSTGRES_RECORD_BATCH_QUEUE_MAX_SIZE_BYTES", 1024 * 1024 * 300, type_cast=int
)

BATCH_EXPORT_BIGQUERY_UPLOAD_CHUNK_SIZE_BYTES: int = 1024 * 1024 * 100  # 100MB
BATCH_EXPORT_BIGQUERY_RECORD_BATCH_QUEUE_MAX_SIZE_BYTES: int = get_from_env(
    "BATCH_EXPORT_BIGQUERY_RECORD_BATCH_QUEUE_MAX_SIZE_BYTES", 0, type_cast=int
)
BATCH_EXPORT_BIGQUERY_USE_STAGE_TEAM_IDS: list[str] = get_list(
    os.getenv("BATCH_EXPORT_BIGQUERY_USE_STAGE_TEAM_IDS", "")
)
BATCH_EXPORT_BIGQUERY_USE_INTERNAL_STAGE_ROLLOUT_PERCENTAGE: int = get_from_env(
    "BATCH_EXPORT_BIGQUERY_USE_INTERNAL_STAGE_ROLLOUT_PERCENTAGE", 0, type_cast=int
)

BATCH_EXPORT_REDSHIFT_UPLOAD_CHUNK_SIZE_BYTES: int = 1024 * 1024 * 8  # 8MB
BATCH_EXPORT_REDSHIFT_RECORD_BATCH_QUEUE_MAX_SIZE_BYTES: int = get_from_env(
    "BATCH_EXPORT_REDSHIFT_RECORD_BATCH_QUEUE_MAX_SIZE_BYTES", 1024 * 1024 * 300, type_cast=int
)
BATCH_EXPORT_REDSHIFT_USE_STAGE_TEAM_IDS: list[str] = get_list(
    os.getenv("BATCH_EXPORT_REDSHIFT_USE_STAGE_TEAM_IDS", "")
)
BATCH_EXPORT_REDSHIFT_USE_INTERNAL_STAGE_ROLLOUT_PERCENTAGE: int = get_from_env(
    "BATCH_EXPORT_REDSHIFT_USE_INTERNAL_STAGE_ROLLOUT_PERCENTAGE", 0, type_cast=int
)

BATCH_EXPORT_DATABRICKS_UPLOAD_CHUNK_SIZE_BYTES: int = 1024 * 1024 * 100  # 100MB
BATCH_EXPORT_DATABRICKS_RECORD_BATCH_QUEUE_MAX_SIZE_BYTES: int = get_from_env(
    "BATCH_EXPORT_DATABRICKS_RECORD_BATCH_QUEUE_MAX_SIZE_BYTES", 1024 * 1024 * 300, type_cast=int
)

BATCH_EXPORT_HTTP_UPLOAD_CHUNK_SIZE_BYTES: int = get_from_env(
    "BATCH_EXPORT_HTTP_UPLOAD_CHUNK_SIZE_BYTES", 1024 * 1024 * 50, type_cast=int
)
BATCH_EXPORT_HTTP_BATCH_SIZE: int = get_from_env("BATCH_EXPORT_HTTP_BATCH_SIZE", 5000, type_cast=int)


BATCH_EXPORT_HEARTBEAT_TIMEOUT_SECONDS: int = get_from_env("BATCH_EXPORT_HEARTBEAT_TIMEOUT_SECONDS", 30, type_cast=int)

BATCH_EXPORT_ORDERLESS_TEAM_IDS: list[str] = get_list(os.getenv("BATCH_EXPORT_ORDERLESS_TEAM_IDS", ""))
UNCONSTRAINED_TIMESTAMP_TEAM_IDS: list[str] = get_list(os.getenv("UNCONSTRAINED_TIMESTAMP_TEAM_IDS", ""))
DEFAULT_TIMESTAMP_LOOKBACK_DAYS = 7
# Comma separated list of overrides in the format "team_id:lookback_days"
OVERRIDE_TIMESTAMP_TEAM_IDS: dict[int, int] = dict(
    [map(int, o.split(":")) for o in os.getenv("OVERRIDE_TIMESTAMP_TEAM_IDS", "").split(",") if o]  # type: ignore
)

CLICKHOUSE_OFFLINE_5MIN_CLUSTER_HOST: str | None = os.getenv("CLICKHOUSE_OFFLINE_5MIN_CLUSTER_HOST", None)

BATCH_EXPORT_OBJECT_STORAGE_ENDPOINT: str = os.getenv(
    "BATCH_EXPORT_OBJECT_STORAGE_ENDPOINT", "http://objectstorage:19000"
)
BATCH_EXPORT_OBJECT_STORAGE_REGION: str = os.getenv("BATCH_EXPORT_OBJECT_STORAGE_REGION", "us-east-1")
BATCH_EXPORT_INTERNAL_STAGING_BUCKET: str = os.getenv("BATCH_EXPORT_INTERNAL_STAGING_BUCKET", "posthog")
# The number of partitions controls how many files ClickHouse writes to concurrently
BATCH_EXPORT_CLICKHOUSE_S3_PARTITIONS: int = get_from_env("BATCH_EXPORT_CLICKHOUSE_S3_PARTITIONS", 10, type_cast=int)
BATCH_EXPORT_TRANSFORMER_MAX_WORKERS: int = get_from_env("BATCH_EXPORT_TRANSFORMER_MAX_WORKERS", 2, type_cast=int)

BATCH_EXPORTS_ENABLE_BILLING_CHECK: bool = get_from_env(
    "BATCH_EXPORTS_ENABLE_BILLING_CHECK", False, type_cast=str_to_bool
)
