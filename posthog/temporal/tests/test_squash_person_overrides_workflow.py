import operator
import random
from collections import defaultdict, namedtuple
from datetime import datetime, timedelta
from typing import TypedDict
from uuid import UUID, uuid4

import psycopg2
import pytest
from django.conf import settings
from freezegun.api import freeze_time
from temporalio.client import Client
from temporalio.testing import ActivityEnvironment
from temporalio.worker import UnsandboxedWorkflowRunner, Worker

from posthog.clickhouse.client import sync_execute
from posthog.clickhouse.test.test_person_overrides import PersonOverrideValues
from posthog.models.person_overrides.sql import (
    DROP_KAFKA_PERSON_OVERRIDES_TABLE_SQL,
    DROP_PERSON_OVERRIDES_CREATE_MATERIALIZED_VIEW_SQL,
    KAFKA_PERSON_OVERRIDES_TABLE_SQL,
    PERSON_OVERRIDES_CREATE_MATERIALIZED_VIEW_SQL,
    PERSON_OVERRIDES_CREATE_TABLE_SQL,
)
from posthog.temporal.workflows.squash_person_overrides import (
    QueryInputs,
    SerializablePersonOverrideToDelete,
    SquashPersonOverridesInputs,
    SquashPersonOverridesWorkflow,
    delete_squashed_person_overrides_from_clickhouse,
    delete_squashed_person_overrides_from_postgres,
    drop_dictionary,
    prepare_dictionary,
    prepare_person_overrides,
    select_persons_to_delete,
    squash_events_partition,
)


@freeze_time("2023-03-14")
@pytest.mark.parametrize(
    "inputs,expected",
    [
        (
            {"partition_ids": None, "last_n_months": 5},
            ["202303", "202302", "202301", "202212", "202211"],
        ),
        ({"last_n_months": 1}, ["202303"]),
        (
            {"partition_ids": ["202303", "202302"], "last_n_months": 3},
            ["202303", "202302"],
        ),
        (
            {"partition_ids": ["202303", "202302"], "last_n_months": None},
            ["202303", "202302"],
        ),
    ],
)
def test_workflow_inputs_yields_partition_ids(inputs, expected):
    """Assert partition keys generated by iter_partition_ids."""
    workflow_inputs = SquashPersonOverridesInputs(**inputs)
    assert list(workflow_inputs.iter_partition_ids()) == expected


@pytest.fixture
def activity_environment():
    """Return a testing temporal ActivityEnvironment."""
    return ActivityEnvironment()


@pytest.fixture
def person_overrides_table(query_inputs):
    """Manage person_overrides tables for testing."""
    sync_execute(PERSON_OVERRIDES_CREATE_TABLE_SQL)
    sync_execute(KAFKA_PERSON_OVERRIDES_TABLE_SQL)
    sync_execute(PERSON_OVERRIDES_CREATE_MATERIALIZED_VIEW_SQL)
    sync_execute("TRUNCATE TABLE person_overrides")

    yield

    sync_execute(DROP_KAFKA_PERSON_OVERRIDES_TABLE_SQL)
    sync_execute(DROP_PERSON_OVERRIDES_CREATE_MATERIALIZED_VIEW_SQL)
    sync_execute("DROP TABLE person_overrides")


PersonOverrideTuple = namedtuple("PersonOverrideTuple", ("old_person_id", "override_person_id"))


OVERRIDES_CREATED_AT = datetime.fromisoformat("2020-01-02T00:00:00.123123+00:00")
OLDEST_EVENT_AT = OVERRIDES_CREATED_AT - timedelta(days=1)


@pytest.fixture
def person_overrides_data(person_overrides_table):
    """Produce some fake person_overrides data for testing.

    We yield a dictionary of team_id to sets of PersonOverrideTuple. These dict can be
    used to make assertions on which should be the right person id of an event.
    """
    person_overrides = {
        # These numbers are all arbitrary.
        100: {PersonOverrideTuple(uuid4(), uuid4()) for _ in range(5)},
        200: {PersonOverrideTuple(uuid4(), uuid4()) for _ in range(4)},
        300: {PersonOverrideTuple(uuid4(), uuid4()) for _ in range(3)},
    }

    all_test_values = []

    for team_id, person_ids in person_overrides.items():
        for old_person_id, override_person_id in person_ids:
            values: PersonOverrideValues = {
                "team_id": team_id,
                "old_person_id": old_person_id,
                "override_person_id": override_person_id,
                "merged_at": OVERRIDES_CREATED_AT,
                "oldest_event": OLDEST_EVENT_AT,
                "created_at": OVERRIDES_CREATED_AT,
                "version": 1,
            }
            all_test_values.append(values)

    sync_execute("INSERT INTO person_overrides (*) VALUES", all_test_values)

    yield person_overrides

    sync_execute("TRUNCATE TABLE person_overrides")


@pytest.fixture
def query_inputs():
    """A default set of QueryInputs to use in all tests."""
    query_inputs = QueryInputs()

    return query_inputs


@pytest.mark.django_db
@pytest.mark.asyncio
async def test_prepare_dictionary(query_inputs, activity_environment, person_overrides_data):
    """Test a DICTIONARY is created by the prepare_dictionary activity."""
    query_inputs.dictionary_name = "fancy_dictionary"
    query_inputs.dry_run = False

    latest_merge_at = await activity_environment.run(prepare_dictionary, query_inputs)

    assert latest_merge_at == OVERRIDES_CREATED_AT.isoformat()

    for team_id, person_overrides in person_overrides_data.items():
        for person_override in person_overrides:
            result = sync_execute(
                f"""
                SELECT
                    old_person_id,
                    dictGet(
                        '{settings.CLICKHOUSE_DATABASE}.fancy_dictionary',
                        'override_person_id',
                        (toInt32(team_id), old_person_id)
                    ) AS override_person_id
                FROM (
                    SELECT
                        {team_id} AS team_id,
                        '{person_override.old_person_id}'::UUID AS old_person_id
                )
                """
            )

            assert result[0][0] == person_override.old_person_id
            assert result[0][1] == person_override.override_person_id

    await activity_environment.run(drop_dictionary, query_inputs)


@pytest.fixture
def older_overrides(person_overrides_data):
    """Generate extra test data that is in an older partition."""
    older_overrides = defaultdict(set)

    older_values_to_insert = []
    for team_id, person_override in person_overrides_data.items():
        for old_person_id, override_person_id in person_override:
            override_person_id = uuid4()
            values: PersonOverrideValues = {
                "team_id": team_id,
                "old_person_id": old_person_id,
                "override_person_id": override_person_id,
                "merged_at": datetime.fromisoformat("2019-12-01T00:00:00+00:00"),
                "oldest_event": datetime.fromisoformat("2019-12-01T00:00:00+00:00"),
                "created_at": datetime.fromisoformat("2019-12-01T00:00:00+00:00"),
                "version": 1,
            }

            older_overrides[team_id].add(PersonOverrideTuple(old_person_id, override_person_id))
            older_values_to_insert.append(values)

    sync_execute("INSERT INTO person_overrides (*) VALUES", older_values_to_insert)

    yield older_overrides


@pytest.fixture
def newer_overrides(person_overrides_data):
    """Generate extra test data that is in a newer partition."""
    newer_overrides = defaultdict(set)

    newer_values_to_insert = []
    for team_id, person_override in person_overrides_data.items():
        for old_person_id, override_person_id in person_override:
            override_person_id = uuid4()
            values: PersonOverrideValues = {
                "team_id": team_id,
                "old_person_id": old_person_id,
                "override_person_id": override_person_id,
                "merged_at": datetime.fromisoformat("2020-02-02T00:00:00+00:00"),
                "oldest_event": datetime.fromisoformat("2020-02-01T00:00:00+00:00"),
                "created_at": datetime.fromisoformat("2020-02-01T00:00:00+00:00"),
                "version": 1,
            }

            newer_overrides[team_id].add(PersonOverrideTuple(old_person_id, override_person_id))
            newer_values_to_insert.append(values)

    sync_execute("INSERT INTO person_overrides (*) VALUES", newer_values_to_insert)

    yield newer_overrides


@pytest.mark.django_db
@pytest.mark.asyncio
async def test_prepare_dictionary_with_older_overrides_present(
    query_inputs, activity_environment, person_overrides_data, older_overrides
):
    """Test a DICTIONARY contains latest available mappings."""
    query_inputs.dictionary_name = "fancy_dictionary"
    query_inputs.dry_run = False

    latest_merge_at = await activity_environment.run(prepare_dictionary, query_inputs)

    assert latest_merge_at == OVERRIDES_CREATED_AT.isoformat()

    for team_id, person_overrides in person_overrides_data.items():
        for person_override in person_overrides:
            result = sync_execute(
                f"""
                SELECT
                    old_person_id,
                    dictGet(
                        {settings.CLICKHOUSE_DATABASE}.fancy_dictionary,
                        'override_person_id',
                        (toInt32(team_id), old_person_id)
                    ) AS override_person_id
                FROM (
                    SELECT
                        {team_id} AS team_id,
                        '{person_override.old_person_id}'::UUID AS old_person_id
                )
                """
            )

            assert len(result) == 1
            assert result[0][0] == person_override.old_person_id
            assert result[0][1] == person_override.override_person_id

    await activity_environment.run(drop_dictionary, query_inputs)


@pytest.mark.django_db
@pytest.mark.asyncio
async def test_drop_dictionary(query_inputs, activity_environment, person_overrides_data):
    """Test a DICTIONARY is dropped by drop_join_table activity."""
    query_inputs.dictionary_name = "distinguished_dictionary"
    query_inputs.dry_run = False

    # Ensure we are starting from scratch
    sync_execute(f"DROP DICTIONARY IF EXISTS {settings.CLICKHOUSE_DATABASE}.{query_inputs.dictionary_name}")
    before = sync_execute(f"EXISTS DICTIONARY {settings.CLICKHOUSE_DATABASE}.{query_inputs.dictionary_name}")[0][0]
    assert before == 0

    await activity_environment.run(prepare_dictionary, query_inputs)

    during = sync_execute(f"EXISTS DICTIONARY {settings.CLICKHOUSE_DATABASE}.{query_inputs.dictionary_name}")[0][0]
    assert during == 1

    await activity_environment.run(drop_dictionary, query_inputs)

    after = sync_execute(f"EXISTS DICTIONARY {settings.CLICKHOUSE_DATABASE}.{query_inputs.dictionary_name}")[0][0]
    assert after == 0


get_team_id_old_person_id = operator.attrgetter("team_id", "old_person_id")


def is_equal_sorted(list_left, list_right, key=get_team_id_old_person_id) -> bool:
    """Compare two lists sorted by key are equal.

    Useful when we don't care about order.
    """
    return sorted(list_left, key=key) == sorted(list_right, key=key)


@pytest.mark.django_db
@pytest.mark.asyncio
async def test_select_persons_to_delete(query_inputs, activity_environment, person_overrides_data):
    """Test selecting the correct dictionary of persons to delete.

    The select_persons_to_delete activity produces a dictionary of team_id to sets of
    old_person_id that will be overriden during the workflow and can be safely deleted
    afterwards.
    """
    query_inputs.dry_run = False
    query_inputs.partition_ids = ["202001"]
    query_inputs.latest_created_at = OVERRIDES_CREATED_AT

    to_delete = await activity_environment.run(select_persons_to_delete, query_inputs)

    expected = [
        SerializablePersonOverrideToDelete(
            team_id,
            person_override.old_person_id,
            person_override.override_person_id,
            OVERRIDES_CREATED_AT.isoformat(),
            1,
            OLDEST_EVENT_AT.isoformat(),
        )
        for team_id, person_overrides in person_overrides_data.items()
        for person_override in person_overrides
    ]

    assert is_equal_sorted(to_delete, expected)


@pytest.mark.django_db
@pytest.mark.asyncio
async def test_select_persons_to_delete_selects_persons_in_older_partitions(
    query_inputs, activity_environment, person_overrides_data, older_overrides
):
    """Test all (new and old)  persons to delete are selected when older overrides exist.

    This is because if we are processing a newer override in the current workflow, there is
    no reason to keep an older mapping for the same old_person_id around. The squash query
    will prefer newer mappings, so the older ones do nothing and can cause troubles if left
    uncleaned.
    """
    query_inputs.dry_run = False
    query_inputs.partition_ids = ["202001"]
    query_inputs.latest_created_at = OVERRIDES_CREATED_AT

    to_delete = await activity_environment.run(select_persons_to_delete, query_inputs)

    expected = [
        SerializablePersonOverrideToDelete(
            team_id,
            person_override.old_person_id,
            person_override.override_person_id,
            OVERRIDES_CREATED_AT.isoformat(),
            1,
            OLDEST_EVENT_AT.isoformat(),
        )
        for team_id, person_overrides in person_overrides_data.items()
        for person_override in person_overrides
    ]

    expected.extend(
        [
            SerializablePersonOverrideToDelete(
                team_id,
                person_override.old_person_id,
                person_override.override_person_id,
                "2019-12-01T00:00:00+00:00",
                1,
                "2019-12-01T00:00:00+00:00",
            )
            for team_id, person_overrides in older_overrides.items()
            for person_override in person_overrides
        ]
    )

    assert is_equal_sorted(to_delete, expected)


@pytest.mark.django_db
@pytest.mark.asyncio
async def test_select_persons_to_squash_with_empty_table(query_inputs, activity_environment, person_overrides_table):
    """Test nothing is selected to override when there are no person_overrides.

    If there are no person_overrides rows, then there is no work for us to do.
    """
    sync_execute(PERSON_OVERRIDES_CREATE_TABLE_SQL)

    query_inputs.dry_run = False
    query_inputs.partition_ids = ["202001"]
    query_inputs.latest_created_at = OVERRIDES_CREATED_AT

    to_delete = await activity_environment.run(select_persons_to_delete, query_inputs)

    assert to_delete == []


@pytest.mark.django_db
@pytest.mark.asyncio
async def test_select_persons_to_squash_with_different_partition(
    query_inputs, activity_environment, person_overrides_table
):
    """Test nothing is selected to override when there is no data in a partition.

    If there are no person_overrides rows, then there is no work for us to do.
    """
    sync_execute(PERSON_OVERRIDES_CREATE_TABLE_SQL)

    query_inputs.dry_run = False
    query_inputs.partition_ids = ["202001"]
    query_inputs.latest_created_at = OVERRIDES_CREATED_AT

    to_delete = await activity_environment.run(select_persons_to_delete, query_inputs)

    assert to_delete == []


@pytest.mark.django_db
@pytest.mark.asyncio
async def test_select_persons_to_delete_with_newer_merges(query_inputs, activity_environment, person_overrides_data):
    """Test selecting the correct persons to delete when we got new merges after starting.

    The select_persons_to_delete activity produces a dictionary of team_id to sets of
    old_person_id that will be overriden during the workflow and can be safely deleted
    afterwards.
    """
    newer_overrides = defaultdict(set)

    newer_values_to_insert = []
    for team_id, person_override in person_overrides_data.items():
        for old_person_id, override_person_id in person_override:
            override_person_id = uuid4()
            values: PersonOverrideValues = {
                "team_id": team_id,
                # Overrides for the same people
                "old_person_id": old_person_id,
                "override_person_id": override_person_id,
                # But happened an hour after
                "merged_at": OVERRIDES_CREATED_AT + timedelta(hours=1),
                "oldest_event": OLDEST_EVENT_AT + timedelta(hours=1),
                "created_at": OVERRIDES_CREATED_AT + timedelta(hours=1),
                "version": 1,
            }

            newer_overrides[team_id].add(PersonOverrideTuple(old_person_id, override_person_id))
            newer_values_to_insert.append(values)

    sync_execute("INSERT INTO person_overrides (*) VALUES", newer_values_to_insert)

    query_inputs.dry_run = False
    query_inputs.partition_ids = ["202001"]
    # Our latest_created_at is before the newer values happened
    query_inputs.latest_created_at = OVERRIDES_CREATED_AT

    to_delete = await activity_environment.run(select_persons_to_delete, query_inputs)

    expected = [
        SerializablePersonOverrideToDelete(
            team_id,
            person_override.old_person_id,
            person_override.override_person_id,
            OVERRIDES_CREATED_AT.isoformat(),
            1,
            OLDEST_EVENT_AT.isoformat(),
        )
        for team_id, person_overrides in person_overrides_data.items()
        for person_override in person_overrides
    ]

    assert is_equal_sorted(to_delete, expected)


class EventValues(TypedDict):
    """Events to be inserted for testing."""

    uuid: UUID
    event: str
    timestamp: datetime
    person_id: str
    team_id: int


@pytest.fixture
def events_to_override(person_overrides_data):
    """Produce some test events for testing.

    These events will be yielded so that we can re-fetch them and assert their
    person_ids have been overriden.
    """
    all_test_events = []
    for team_id, person_ids in person_overrides_data.items():
        for old_person_id, _ in person_ids:
            values: EventValues = {
                "uuid": uuid4(),
                "event": "test-event",
                "timestamp": OLDEST_EVENT_AT,
                "team_id": team_id,
                "person_id": old_person_id,
            }
            all_test_events.append(values)

    sync_execute(
        "INSERT INTO sharded_events (uuid, event, timestamp, team_id, person_id) VALUES",
        all_test_events,
    )

    yield all_test_events

    sync_execute("TRUNCATE TABLE sharded_events")


def assert_events_have_been_overriden(overriden_events, person_overrides):
    """Assert each event in overriden_events has actually been overriden.

    We use person_overrides to assert the person_id of each event now matches the
    overriden_person_id.
    """
    for event in overriden_events:
        rows = sync_execute(
            "SELECT uuid, event, team_id, person_id FROM events WHERE uuid = %(uuid)s",
            {"uuid": event["uuid"]},
        )
        new_event = {
            "uuid": rows[0][0],
            "event": rows[0][1],
            "team_id": rows[0][2],
            "person_id": rows[0][3],
        }

        assert event["uuid"] == new_event["uuid"]  # Sanity check
        assert event["team_id"] == new_event["team_id"]  # Sanity check
        assert event["person_id"] != new_event["person_id"]

        # If all is well, we should have overriden old_person_id with an override_person_id.
        # Let's find it first:
        new_person_id = [
            person_override.override_person_id
            for person_override in person_overrides[new_event["team_id"]]
            if person_override.old_person_id == event["person_id"]
        ][0]
        assert new_event["person_id"] == new_person_id


@pytest.mark.django_db
@pytest.mark.asyncio
async def test_squash_events_partition(query_inputs, activity_environment, person_overrides_data, events_to_override):
    """Test events are properly squashed by running squash_events_partition.

    After running squash_events_partition, we iterate over the test events created by
    events_to_override and check the person_id associated with each of them. It should
    match the override_person_id associated with the old_person_id they used to be set to.
    """
    query_inputs.dictionary_name = "exciting_dictionary"
    query_inputs.partition_ids = ["202001"]
    query_inputs.latest_created_at = OVERRIDES_CREATED_AT
    query_inputs.dry_run = False

    await activity_environment.run(prepare_dictionary, query_inputs)

    await activity_environment.run(squash_events_partition, query_inputs)

    await activity_environment.run(drop_dictionary, query_inputs)

    assert_events_have_been_overriden(events_to_override, person_overrides_data)


@pytest.mark.django_db
@pytest.mark.asyncio
async def test_squash_events_partition_dry_run(
    query_inputs, activity_environment, person_overrides_data, events_to_override
):
    """Test events are not squashed by running squash_events_partition with dry_run=True."""
    query_inputs.dictionary_name = "exciting_dictionary"
    query_inputs.partition_ids = ["202001"]
    query_inputs.latest_created_at = OVERRIDES_CREATED_AT

    await activity_environment.run(prepare_dictionary, query_inputs)

    await activity_environment.run(squash_events_partition, query_inputs)

    await activity_environment.run(drop_dictionary, query_inputs)

    for event in events_to_override:
        rows = sync_execute(
            "SELECT uuid, event, team_id, person_id FROM events WHERE uuid = %(uuid)s",
            {"uuid": event["uuid"]},
        )
        new_event = {
            "uuid": rows[0][0],
            "event": rows[0][1],
            "team_id": rows[0][2],
            "person_id": rows[0][3],
        }

        assert event["uuid"] == new_event["uuid"]  # Sanity check
        assert event["team_id"] == new_event["team_id"]  # Sanity check
        assert event["person_id"] == new_event["person_id"]


@pytest.mark.django_db
@pytest.mark.asyncio
async def test_squash_events_partition_with_older_overrides(
    query_inputs,
    activity_environment,
    person_overrides_data,
    events_to_override,
    older_overrides,
):
    """Test events are properly squashed even in the prescence of older overrides.

    If we get an override from Postgres we can be sure it's the only one for a given
    old_person_id as PG constraints enforce uniqueness on the mapping. However, ClickHouse
    doesn't enforce any kind of uniqueness constraints, so our queries need to be aware there
    could be duplicate overrides present, either in the partition we are currently working
    with as well as older ones.
    """
    query_inputs.dictionary_name = "exciting_dictionary"
    query_inputs.partition_ids = ["202001"]
    query_inputs.latest_created_at = OVERRIDES_CREATED_AT
    query_inputs.dry_run = False

    await activity_environment.run(prepare_dictionary, query_inputs)

    await activity_environment.run(squash_events_partition, query_inputs)

    await activity_environment.run(drop_dictionary, query_inputs)

    assert_events_have_been_overriden(events_to_override, person_overrides_data)


@pytest.mark.django_db
@pytest.mark.asyncio
async def test_squash_events_partition_with_newer_overrides(
    query_inputs,
    activity_environment,
    person_overrides_data,
    events_to_override,
    newer_overrides,
):
    """Test events are properly squashed even in the prescence of newer overrides.

    If we get an override from Postgres we can get be sure it's the only one for a given
    old_person_id as PG constraints enforce uniqueness on the mapping. However, ClickHouse
    doesn't enforce any kind of uniqueness constraints, so our queries need to be aware there
    could be duplicate overrides present, either in the partition we are currently working
    with as well as newer ones.
    """
    query_inputs.dictionary_name = "exciting_dictionary"
    query_inputs.partition_ids = ["202001"]
    query_inputs.latest_created_at = OVERRIDES_CREATED_AT
    query_inputs.dry_run = False

    await activity_environment.run(prepare_dictionary, query_inputs)

    await activity_environment.run(squash_events_partition, query_inputs)

    await activity_environment.run(drop_dictionary, query_inputs)

    assert_events_have_been_overriden(events_to_override, newer_overrides)


@pytest.mark.django_db
@pytest.mark.asyncio
async def test_squash_events_partition_with_limited_team_ids(
    query_inputs, activity_environment, person_overrides_data, events_to_override
):
    """Test events are properly squashed when we specify team_ids."""
    dictionary_name = "exciting_limited_dictionary"
    random_team = random.choice(list(person_overrides_data.keys()))
    query_inputs.dictionary_name = dictionary_name
    query_inputs.partition_ids = ["202001"]
    query_inputs.latest_created_at = OVERRIDES_CREATED_AT
    query_inputs.dry_run = False
    query_inputs.team_ids = [random_team]

    latest_created_at = await activity_environment.run(prepare_dictionary, query_inputs)
    query_inputs._latest_created_at = latest_created_at

    await activity_environment.run(squash_events_partition, query_inputs)

    await activity_environment.run(drop_dictionary, query_inputs)

    with pytest.raises(AssertionError):
        # Some checks will fail as we have limited the teams overriden.
        assert_events_have_been_overriden(events_to_override, person_overrides_data)

    # But if we only check the limited teams, there shouldn't be any issues.
    limited_events = [event for event in events_to_override if event["team_id"] == random_team]
    assert_events_have_been_overriden(limited_events, person_overrides_data)


@pytest.mark.django_db
@pytest.mark.asyncio
async def test_delete_squashed_person_overrides_from_clickhouse(
    query_inputs, activity_environment, person_overrides_data
):
    """Test we can delete person overrides that have already been squashed.

    For the purposes of this unit test, we take the person overrides as given. A
    comprehensive test will cover the entire worflow end-to-end.

    We insert an extra person to ensure we are not deleting persons we shouldn't
    delete.
    """
    persons_to_delete = [
        SerializablePersonOverrideToDelete(
            team_id,
            person_override.old_person_id,
            person_override.override_person_id,
            OVERRIDES_CREATED_AT.isoformat(),
            1,
            OLDEST_EVENT_AT.isoformat(),
        )
        for team_id, person_overrides in person_overrides_data.items()
        for person_override in person_overrides
    ]
    query_inputs.partition_ids = ["202001"]
    query_inputs.latest_created_at = OVERRIDES_CREATED_AT
    query_inputs.dry_run = False
    query_inputs.person_overrides_to_delete = persons_to_delete

    not_overriden_id = uuid4()
    not_overriden_person: PersonOverrideValues = {
        "team_id": 1,
        "old_person_id": not_overriden_id,
        "override_person_id": uuid4(),
        "merged_at": OVERRIDES_CREATED_AT,
        "oldest_event": OLDEST_EVENT_AT,
        "created_at": OVERRIDES_CREATED_AT,
        "version": 1,
    }

    sync_execute("INSERT INTO person_overrides (*) VALUES", [not_overriden_person])

    await activity_environment.run(delete_squashed_person_overrides_from_clickhouse, query_inputs)

    rows = sync_execute("SELECT team_id, old_person_id FROM person_overrides")

    assert len(rows) == 1
    assert rows[0][0] == 1
    assert rows[0][1] == not_overriden_id


@pytest.mark.django_db
@pytest.mark.asyncio
async def test_delete_squashed_person_overrides_from_clickhouse_dry_run(
    query_inputs, activity_environment, person_overrides_data
):
    """Test we do not delete person overrides when dry_run=True."""
    persons_to_delete = [
        SerializablePersonOverrideToDelete(
            team_id,
            person_override.old_person_id,
            person_override.override_person_id,
            OVERRIDES_CREATED_AT.isoformat(),
            1,
            OLDEST_EVENT_AT.isoformat(),
        )
        for team_id, person_overrides in person_overrides_data.items()
        for person_override in person_overrides
    ]
    query_inputs.partition_ids = ["202001"]
    query_inputs.latest_created_at = OVERRIDES_CREATED_AT
    query_inputs.dry_run = True
    query_inputs.person_overrides_to_delete = persons_to_delete

    not_overriden_id = uuid4()
    not_overriden_person: PersonOverrideValues = {
        "team_id": 1,
        "old_person_id": not_overriden_id,
        "override_person_id": uuid4(),
        "merged_at": OVERRIDES_CREATED_AT,
        "oldest_event": OLDEST_EVENT_AT,
        "created_at": OVERRIDES_CREATED_AT,
        "version": 1,
    }

    sync_execute("INSERT INTO person_overrides (*) VALUES", [not_overriden_person])

    await activity_environment.run(delete_squashed_person_overrides_from_clickhouse, query_inputs)

    rows = sync_execute("SELECT team_id, old_person_id FROM person_overrides")

    assert len(rows) == len(persons_to_delete) + 1


@pytest.fixture(scope="session")
def django_db_setup_fixture():
    """Re-use pytest_django's django_db_setup."""
    from pytest_django.fixtures import django_db_setup

    yield django_db_setup


@pytest.fixture
def pg_connection():
    """Manage a Postgres connection with psycopg2."""
    conn = psycopg2.connect(
        dbname=settings.DATABASES["default"]["NAME"],
        user=settings.DATABASES["default"]["USER"],
        password=settings.DATABASES["default"]["PASSWORD"],
        host=settings.DATABASES["default"]["HOST"],
        port=settings.DATABASES["default"]["PORT"],
    )

    try:
        yield conn
    finally:
        conn.close()


@pytest.fixture
def organization_uuid(pg_connection, query_inputs, django_db_setup_fixture):
    """Create an Organization and return its UUID.

    We cannot use the Django ORM safely in an async context, so we INSERT INTO directly
    on the database. This means we need to clean up after ourselves, which we do after
    yielding.
    """
    organization_uuid = uuid4()

    with pg_connection:
        with pg_connection.cursor() as cursor:
            cursor.execute(
                """
                INSERT INTO posthog_organization (
                    id,
                    name,
                    created_at,
                    updated_at,
                    personalization,
                    setup_section_2_completed,
                    plugins_access_level,
                    for_internal_metrics,
                    available_features,
                    domain_whitelist,
                    is_member_join_email_enabled,
                    slug
                )
                VALUES (
                    %(uuid)s,
                    'test-workflows-org',
                    NOW(),
                    NOW(),
                    '{}',
                    TRUE,
                    1,
                    FALSE,
                    '{}',
                    '{}',
                    TRUE,
                    'test-worflows-org'
                )
                """,
                {"uuid": organization_uuid},
            )

    yield organization_uuid

    with pg_connection:
        with pg_connection.cursor() as cursor:
            cursor.execute("DELETE FROM posthog_organization WHERE id = %s", [organization_uuid])


@pytest.fixture
def team_id(query_inputs, organization_uuid, pg_connection):
    """Create a Team and return its ID.

    We cannot use the Django ORM safely in an async context, so we INSERT INTO directly
    on the database. This means we need to clean up after ourselves, which we do after
    yielding.
    """
    with pg_connection:
        with pg_connection.cursor() as cursor:
            cursor.execute(
                """
                INSERT INTO posthog_team (
                    api_token,
                    name,
                    opt_out_capture,
                    app_urls,
                    event_names,
                    event_properties,
                    anonymize_ips,
                    completed_snippet_onboarding,
                    created_at,
                    updated_at,
                    event_properties_numerical,
                    ingested_event,
                    uuid,
                    organization_id,
                    session_recording_opt_in,
                    plugins_opt_in,
                    event_names_with_usage,
                    event_properties_with_usage,
                    is_demo,
                    test_account_filters,
                    timezone,
                    data_attributes,
                    access_control
                )
                VALUES (
                     'the_token',
                     'test_workflow_team',
                     TRUE,
                     '{}',
                     '{}',
                     '{}',
                     TRUE,
                     TRUE,
                     NOW(),
                     NOW(),
                     '{}',
                     TRUE,
                     '00000000-0000-0000-0000-000000000000'::UUID,
                     %(organization_uuid)s,
                     TRUE,
                     TRUE,
                     '{}',
                     '{}',
                     FALSE,
                     '{}',
                     'UTC',
                     '{}',
                     TRUE
                )
                RETURNING id
                """,
                {"organization_uuid": organization_uuid},
            )
            team_id = cursor.fetchone()

    yield team_id

    with pg_connection:
        with pg_connection.cursor() as cursor:
            cursor.execute("DELETE FROM posthog_team WHERE id = %s", [team_id])


@pytest.fixture
def person_overrides(query_inputs, team_id, pg_connection):
    """Create a PersonOverrideMapping and a PersonOverride.

    We cannot use the Django ORM safely in an async context, so we INSERT INTO directly
    on the database. This means we need to clean up after ourselves, which we do after
    yielding.
    """
    old_person_id = uuid4()
    override_person_id = uuid4()
    person_override = PersonOverrideTuple(old_person_id, override_person_id)

    with pg_connection:
        with pg_connection.cursor() as cursor:
            person_ids = []
            for person_uuid in (override_person_id, old_person_id):
                cursor.execute(
                    """
                    INSERT INTO posthog_personoverridemapping(
                        team_id,
                        uuid
                    )
                    VALUES (
                        %(team_id)s,
                        %(uuid)s
                    )
                    ON CONFLICT("team_id", "uuid") DO NOTHING
                    RETURNING id
                    """,
                    {"team_id": team_id, "uuid": person_uuid},
                )
                person_ids.append(cursor.fetchone())

            cursor.execute(
                """
                INSERT INTO posthog_personoverride(
                    team_id,
                    old_person_id,
                    override_person_id,
                    oldest_event,
                    version
                )
                VALUES (
                    %(team_id)s,
                    %(old_person_id)s,
                    %(override_person_id)s,
                    NOW(),
                    1
                );
                """,
                {
                    "team_id": team_id,
                    "old_person_id": person_ids[1],
                    "override_person_id": person_ids[0],
                },
            )

    yield person_override

    with pg_connection:
        with pg_connection.cursor() as cursor:
            cursor.execute(
                "DELETE FROM posthog_personoverride WHERE team_id = %s AND old_person_id = %s",
                [team_id, person_ids[1]],
            )
            cursor.execute(
                "DELETE FROM posthog_personoverridemapping WHERE team_id = %s AND (uuid = %s OR uuid = %s)",
                [team_id, old_person_id, override_person_id],
            )


@pytest.mark.django_db
@pytest.mark.asyncio
async def test_delete_squashed_person_overrides_from_postgres(
    query_inputs, activity_environment, team_id, person_overrides, pg_connection
):
    """Test we can delete person overrides that have already been squashed.

    For the purposes of this unit test, we take the person overrides as given. A
    comprehensive test will cover the entire worflow end-to-end.
    """
    # These are sanity checks to ensure the fixtures are working properly.
    # If any assertions fail here, its likely a test setup issue.
    with pg_connection:
        with pg_connection.cursor() as cursor:
            cursor.execute("SELECT id, team_id, uuid FROM posthog_personoverridemapping")
            mappings = cursor.fetchall()
            assert len(mappings) == 2

            cursor.execute("SELECT * FROM posthog_personoverride")
            overrides = cursor.fetchall()
            assert len(overrides) == 1

    person_overrides_to_delete = [
        SerializablePersonOverrideToDelete(
            team_id,
            person_overrides.old_person_id,
            person_overrides.override_person_id,
            OVERRIDES_CREATED_AT.isoformat(),
            1,
            OLDEST_EVENT_AT.isoformat(),
        )
    ]
    query_inputs.person_overrides_to_delete = person_overrides_to_delete
    query_inputs.dry_run = False

    await activity_environment.run(delete_squashed_person_overrides_from_postgres, query_inputs)

    with pg_connection:
        with pg_connection.cursor() as cursor:
            cursor.execute("SELECT team_id, uuid FROM posthog_personoverridemapping")
            mappings = cursor.fetchall()
            assert len(mappings) == 1
            assert mappings[0][1] == person_overrides.override_person_id

            cursor.execute("SELECT * FROM posthog_personoverride")
            overrides = cursor.fetchall()
            assert len(overrides) == 0


@pytest.mark.django_db
@pytest.mark.asyncio
async def test_delete_squashed_person_overrides_from_postgres_dry_run(
    query_inputs, activity_environment, team_id, person_overrides, pg_connection
):
    """Test we do not delete person overrides when dry_run=True."""
    # These are sanity checks to ensure the fixtures are working properly.
    # If any assertions fail here, its likely a test setup issue.
    with pg_connection:
        with pg_connection.cursor() as cursor:
            cursor.execute("SELECT id, team_id, uuid FROM posthog_personoverridemapping")
            mappings = cursor.fetchall()
            assert len(mappings) == 2

            cursor.execute("SELECT * FROM posthog_personoverride")
            overrides = cursor.fetchall()
            assert len(overrides) == 1

    person_overrides_to_delete = [
        SerializablePersonOverrideToDelete(
            team_id,
            person_overrides.old_person_id,
            person_overrides.override_person_id,
            OVERRIDES_CREATED_AT.isoformat(),
            1,
            OLDEST_EVENT_AT.isoformat(),
        )
    ]
    query_inputs.person_overrides_to_delete = person_overrides_to_delete
    query_inputs.dry_run = True

    await activity_environment.run(delete_squashed_person_overrides_from_postgres, query_inputs)

    with pg_connection:
        with pg_connection.cursor() as cursor:
            cursor.execute("SELECT team_id, uuid FROM posthog_personoverridemapping")
            mappings = cursor.fetchall()
            assert len(mappings) == 2
            assert mappings[0][1] == person_overrides.override_person_id

            cursor.execute("SELECT * FROM posthog_personoverride")
            overrides = cursor.fetchall()
            assert len(overrides) == 1


@pytest.mark.django_db
@pytest.mark.asyncio
async def test_delete_squashed_person_overrides_from_postgres_with_newer_override(
    query_inputs, activity_environment, team_id, person_overrides, pg_connection
):
    """Test we do not delete a newer mapping from Postgres.

    For the purposes of this unit test, we take the person overrides as given. A
    comprehensive test will cover the entire worflow end-to-end.
    """
    # These are sanity checks to ensure the fixtures are working properly.
    # If any assertions fail here, its likely a test setup issue.
    with pg_connection:
        with pg_connection.cursor() as cursor:
            cursor.execute("SELECT id, team_id, uuid FROM posthog_personoverridemapping")
            mappings = cursor.fetchall()
            assert len(mappings) == 2

            cursor.execute("SELECT team_id, old_person_id, override_person_id FROM posthog_personoverride")
            overrides = cursor.fetchall()
            assert len(overrides) == 1

    with pg_connection:
        with pg_connection.cursor() as cursor:
            # Let's insert a newer mapping that arrives while we are running the squash job.
            # Since only one mapping can exist per old_person_id, we'll bump the version number.
            cursor.execute(
                """
                UPDATE posthog_personoverride
                SET version = version + 1
                WHERE
                    team_id = %(team_id)s
                    AND old_person_id = %(old_person_id)s
                """,
                {
                    "team_id": team_id,
                    "old_person_id": [
                        mapping[0] for mapping in mappings if mapping[2] == person_overrides.old_person_id
                    ][0],
                },
            )

    person_overrides_to_delete = [
        # We are schedulling for deletion an override with lower version number, so nothing should happen.
        SerializablePersonOverrideToDelete(
            team_id,
            person_overrides.old_person_id,
            person_overrides.override_person_id,
            OVERRIDES_CREATED_AT.isoformat(),
            1,
            OLDEST_EVENT_AT.isoformat(),
        )
    ]
    query_inputs.person_overrides_to_delete = person_overrides_to_delete
    query_inputs.dry_run = False

    await activity_environment.run(delete_squashed_person_overrides_from_postgres, query_inputs)

    with pg_connection:
        with pg_connection.cursor() as cursor:
            cursor.execute("SELECT id, team_id, uuid FROM posthog_personoverridemapping")
            mappings = cursor.fetchall()

            # Nothing was deleted from mappings table
            assert len(mappings) == 2
            assert person_overrides.override_person_id in [mapping[2] for mapping in mappings]
            assert person_overrides.old_person_id in [mapping[2] for mapping in mappings]

            cursor.execute("SELECT team_id, old_person_id, override_person_id, version FROM posthog_personoverride")
            overrides = cursor.fetchall()

            # And nothing was deleted from overrides table
            assert len(overrides) == 1

            team_id, old_person_id, override_person_id, version = overrides[0]
            assert team_id == team_id
            assert (
                old_person_id == [mapping[0] for mapping in mappings if mapping[2] == person_overrides.old_person_id][0]
            )
            assert (
                override_person_id
                == [mapping[0] for mapping in mappings if mapping[2] == person_overrides.override_person_id][0]
            )
            assert version == 2


@pytest.mark.django_db
@pytest.mark.asyncio
async def test_squash_person_overrides_workflow(
    query_inputs,
    events_to_override,
    person_overrides_data,
    person_overrides,
    person_overrides_table,
):
    """Test the squash_person_overrides workflow end-to-end."""
    client = await Client.connect(
        f"{settings.TEMPORAL_HOST}:{settings.TEMPORAL_PORT}",
        namespace=settings.TEMPORAL_NAMESPACE,
    )

    workflow_id = str(uuid4())
    inputs = SquashPersonOverridesInputs(
        partition_ids=["202001"],
        dry_run=False,
    )

    async with Worker(
        client,
        task_queue=settings.TEMPORAL_TASK_QUEUE,
        workflows=[SquashPersonOverridesWorkflow],
        activities=[
            prepare_person_overrides,
            prepare_dictionary,
            select_persons_to_delete,
            squash_events_partition,
            drop_dictionary,
            delete_squashed_person_overrides_from_clickhouse,
            delete_squashed_person_overrides_from_postgres,
        ],
        workflow_runner=UnsandboxedWorkflowRunner(),
    ):
        await client.execute_workflow(
            SquashPersonOverridesWorkflow.run,
            inputs,
            id=workflow_id,
            task_queue=settings.TEMPORAL_TASK_QUEUE,
        )

    assert_events_have_been_overriden(events_to_override, person_overrides_data)

    rows = sync_execute("SELECT team_id, old_person_id FROM person_overrides")
    assert len(rows) == 0


@pytest.mark.django_db
@pytest.mark.asyncio
async def test_squash_person_overrides_workflow_with_newer_overrides(
    query_inputs,
    events_to_override,
    person_overrides_data,
    person_overrides,
    newer_overrides,
):
    """Test the squash_person_overrides workflow end-to-end with newer overrides."""
    client = await Client.connect(
        f"{settings.TEMPORAL_HOST}:{settings.TEMPORAL_PORT}",
        namespace=settings.TEMPORAL_NAMESPACE,
    )

    workflow_id = str(uuid4())
    inputs = SquashPersonOverridesInputs(
        partition_ids=["202001"],
        dry_run=False,
    )

    async with Worker(
        client,
        task_queue=settings.TEMPORAL_TASK_QUEUE,
        workflows=[SquashPersonOverridesWorkflow],
        activities=[
            prepare_person_overrides,
            prepare_dictionary,
            select_persons_to_delete,
            squash_events_partition,
            drop_dictionary,
            delete_squashed_person_overrides_from_clickhouse,
            delete_squashed_person_overrides_from_postgres,
        ],
        workflow_runner=UnsandboxedWorkflowRunner(),
    ):
        await client.execute_workflow(
            SquashPersonOverridesWorkflow.run,
            inputs,
            id=workflow_id,
            task_queue=settings.TEMPORAL_TASK_QUEUE,
        )

    assert_events_have_been_overriden(events_to_override, newer_overrides)


@pytest.mark.django_db
@pytest.mark.asyncio
async def test_squash_person_overrides_workflow_with_limited_team_ids(
    query_inputs, events_to_override, person_overrides_data, person_overrides
):
    """Test the squash_person_overrides workflow end-to-end."""
    client = await Client.connect(
        f"{settings.TEMPORAL_HOST}:{settings.TEMPORAL_PORT}",
        namespace=settings.TEMPORAL_NAMESPACE,
    )

    workflow_id = str(uuid4())
    random_team = random.choice(list(person_overrides_data.keys()))
    inputs = SquashPersonOverridesInputs(
        partition_ids=["202001"],
        team_ids=[random_team],
        dry_run=False,
    )

    async with Worker(
        client,
        task_queue=settings.TEMPORAL_TASK_QUEUE,
        workflows=[SquashPersonOverridesWorkflow],
        activities=[
            prepare_person_overrides,
            prepare_dictionary,
            select_persons_to_delete,
            squash_events_partition,
            drop_dictionary,
            delete_squashed_person_overrides_from_clickhouse,
            delete_squashed_person_overrides_from_postgres,
        ],
        workflow_runner=UnsandboxedWorkflowRunner(),
    ):
        await client.execute_workflow(
            SquashPersonOverridesWorkflow.run,
            inputs,
            id=workflow_id,
            task_queue=settings.TEMPORAL_TASK_QUEUE,
        )

    with pytest.raises(AssertionError):
        # Some checks will fail as we have limited the teams overriden.
        assert_events_have_been_overriden(events_to_override, person_overrides_data)

    # But if we only check the limited teams, there shouldn't be any issues.
    limited_events = [event for event in events_to_override if event["team_id"] == random_team]
    assert_events_have_been_overriden(limited_events, person_overrides_data)
