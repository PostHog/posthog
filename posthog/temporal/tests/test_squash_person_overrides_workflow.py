import operator
import random
from collections import defaultdict
from datetime import datetime, timedelta
from typing import Iterator, TypedDict
from uuid import UUID, uuid4

import psycopg2
import pytest
import pytest_asyncio
from django.conf import settings
from freezegun.api import freeze_time
from temporalio.client import Client
from temporalio.testing import ActivityEnvironment
from temporalio.worker import UnsandboxedWorkflowRunner, Worker

from posthog.clickhouse.test.test_person_overrides import PersonOverrideValues
from posthog.models.person_overrides.sql import (
    DROP_KAFKA_PERSON_OVERRIDES_TABLE_SQL,
    DROP_PERSON_OVERRIDES_CREATE_MATERIALIZED_VIEW_SQL,
    KAFKA_PERSON_OVERRIDES_TABLE_SQL,
    PERSON_OVERRIDES_CREATE_MATERIALIZED_VIEW_SQL,
    PERSON_OVERRIDES_CREATE_TABLE_SQL,
)
from posthog.temporal.batch_exports.squash_person_overrides import (
    FlatPostgresPersonOverridesManager,
    PersonOverrideTuple,
    QueryInputs,
    SerializablePersonOverrideToDelete,
    SquashPersonOverridesInputs,
    SquashPersonOverridesWorkflow,
    delete_squashed_person_overrides_from_clickhouse,
    delete_squashed_person_overrides_from_postgres,
    drop_dictionary,
    prepare_dictionary,
    prepare_person_overrides,
    select_persons_to_delete,
    squash_events_partition,
)
from posthog.temporal.common.clickhouse import get_client

pytestmark = [pytest.mark.asyncio(scope="session")]


@freeze_time("2023-03-14")
@pytest.mark.parametrize(
    "inputs,expected",
    [
        (
            {"partition_ids": None, "last_n_months": 5},
            ["202303", "202302", "202301", "202212", "202211"],
        ),
        ({"last_n_months": 1}, ["202303"]),
        (
            {"partition_ids": ["202303", "202302"], "last_n_months": 3},
            ["202303", "202302"],
        ),
        (
            {"partition_ids": ["202303", "202302"], "last_n_months": None},
            ["202303", "202302"],
        ),
    ],
)
def test_workflow_inputs_yields_partition_ids(inputs, expected):
    """Assert partition keys generated by iter_partition_ids."""
    workflow_inputs = SquashPersonOverridesInputs(**inputs)
    assert list(workflow_inputs.iter_partition_ids()) == expected


@pytest.fixture
def activity_environment():
    """Return a testing temporal ActivityEnvironment."""
    return ActivityEnvironment()


@pytest_asyncio.fixture
async def person_overrides_table(clickhouse_client):
    """Manage person_overrides tables for testing."""
    await clickhouse_client.execute_query(PERSON_OVERRIDES_CREATE_TABLE_SQL)
    await clickhouse_client.execute_query(KAFKA_PERSON_OVERRIDES_TABLE_SQL)
    await clickhouse_client.execute_query(PERSON_OVERRIDES_CREATE_MATERIALIZED_VIEW_SQL)
    await clickhouse_client.execute_query("TRUNCATE TABLE person_overrides")

    yield

    await clickhouse_client.execute_query(DROP_KAFKA_PERSON_OVERRIDES_TABLE_SQL)
    await clickhouse_client.execute_query(DROP_PERSON_OVERRIDES_CREATE_MATERIALIZED_VIEW_SQL)
    await clickhouse_client.execute_query("DROP TABLE person_overrides")


OVERRIDES_CREATED_AT = datetime.fromisoformat("2020-01-02T00:00:00.123123+00:00")
OLDEST_EVENT_AT = OVERRIDES_CREATED_AT - timedelta(days=1)


@pytest_asyncio.fixture
async def person_overrides_data(person_overrides_table, clickhouse_client):
    """Produce some fake person_overrides data for testing.

    We yield a dictionary of team_id to sets of PersonOverrideTuple. These dict can be
    used to make assertions on which should be the right person id of an event.
    """
    person_overrides = {
        # These numbers are all arbitrary.
        100: {PersonOverrideTuple(uuid4(), uuid4()) for _ in range(5)},
        200: {PersonOverrideTuple(uuid4(), uuid4()) for _ in range(4)},
        300: {PersonOverrideTuple(uuid4(), uuid4()) for _ in range(3)},
    }

    all_test_values = []

    for team_id, person_ids in person_overrides.items():
        for old_person_id, override_person_id in person_ids:
            values: PersonOverrideValues = {
                "team_id": team_id,
                "old_person_id": old_person_id,
                "override_person_id": override_person_id,
                "merged_at": OVERRIDES_CREATED_AT,
                "oldest_event": OLDEST_EVENT_AT,
                "created_at": OVERRIDES_CREATED_AT,
                "version": 1,
            }
            all_test_values.append(values)

    await clickhouse_client.execute_query("INSERT INTO person_overrides FORMAT JSONEachRow", *all_test_values)

    yield person_overrides

    await clickhouse_client.execute_query("TRUNCATE TABLE person_overrides")


@pytest.fixture
def query_inputs():
    """A default set of QueryInputs to use in all tests."""
    return QueryInputs()


@pytest.mark.django_db
async def test_prepare_dictionary(query_inputs, activity_environment, person_overrides_data, clickhouse_client):
    """Test a DICTIONARY is created by the prepare_dictionary activity."""
    query_inputs.dictionary_name = "fancy_dictionary"
    query_inputs.dry_run = False

    latest_merge_at = await activity_environment.run(prepare_dictionary, query_inputs)

    assert latest_merge_at == OVERRIDES_CREATED_AT.isoformat()

    for team_id, person_overrides in person_overrides_data.items():
        for person_override in person_overrides:
            response = await clickhouse_client.read_query(
                f"""
                SELECT
                    old_person_id,
                    dictGet(
                        '{settings.CLICKHOUSE_DATABASE}.fancy_dictionary',
                        'override_person_id',
                        (toInt32(team_id), old_person_id)
                    ) AS override_person_id
                FROM (
                    SELECT
                        {team_id} AS team_id,
                        '{person_override.old_person_id}'::UUID AS old_person_id
                )
                """
            )
            ids = response.decode("utf-8").strip().split("\t")

            assert UUID(ids[0]) == person_override.old_person_id
            assert UUID(ids[1]) == person_override.override_person_id

    await activity_environment.run(drop_dictionary, query_inputs)


@pytest_asyncio.fixture
async def older_overrides(person_overrides_data, clickhouse_client):
    """Generate extra test data that is in an older partition."""
    older_overrides = defaultdict(set)

    older_values_to_insert = []
    for team_id, person_override in person_overrides_data.items():
        for old_person_id, override_person_id in person_override:
            override_person_id = uuid4()
            values: PersonOverrideValues = {
                "team_id": team_id,
                "old_person_id": old_person_id,
                "override_person_id": override_person_id,
                "merged_at": datetime.fromisoformat("2019-12-01T00:00:00+00:00"),
                "oldest_event": datetime.fromisoformat("2019-12-01T00:00:00+00:00"),
                "created_at": datetime.fromisoformat("2019-12-01T00:00:00+00:00"),
                "version": 1,
            }

            older_overrides[team_id].add(PersonOverrideTuple(old_person_id, override_person_id))
            older_values_to_insert.append(values)

    await clickhouse_client.execute_query("INSERT INTO person_overrides FORMAT JSONEachRow", *older_values_to_insert)

    yield older_overrides


@pytest_asyncio.fixture
async def newer_overrides(person_overrides_data, clickhouse_client):
    """Generate extra test data that is in a newer partition."""
    newer_overrides = defaultdict(set)

    newer_values_to_insert = []
    for team_id, person_override in person_overrides_data.items():
        for old_person_id, override_person_id in person_override:
            override_person_id = uuid4()
            values: PersonOverrideValues = {
                "team_id": team_id,
                "old_person_id": old_person_id,
                "override_person_id": override_person_id,
                "merged_at": datetime.fromisoformat("2020-02-02T00:00:00+00:00"),
                "oldest_event": datetime.fromisoformat("2020-02-01T00:00:00+00:00"),
                "created_at": datetime.fromisoformat("2020-02-01T00:00:00+00:00"),
                "version": 1,
            }

            newer_overrides[team_id].add(PersonOverrideTuple(old_person_id, override_person_id))
            newer_values_to_insert.append(values)

    await clickhouse_client.execute_query("INSERT INTO person_overrides FORMAT JSONEachRow", *newer_values_to_insert)

    yield newer_overrides


@pytest.mark.django_db
async def test_prepare_dictionary_with_older_overrides_present(
    query_inputs, activity_environment, person_overrides_data, older_overrides, clickhouse_client
):
    """Test a DICTIONARY contains latest available mappings."""
    query_inputs.dictionary_name = "fancy_dictionary"
    query_inputs.dry_run = False

    latest_merge_at = await activity_environment.run(prepare_dictionary, query_inputs)

    assert latest_merge_at == OVERRIDES_CREATED_AT.isoformat()

    for team_id, person_overrides in person_overrides_data.items():
        for person_override in person_overrides:
            response = await clickhouse_client.read_query(
                f"""
                SELECT
                    old_person_id,
                    dictGet(
                        {settings.CLICKHOUSE_DATABASE}.fancy_dictionary,
                        'override_person_id',
                        (toInt32(team_id), old_person_id)
                    ) AS override_person_id
                FROM (
                    SELECT
                        {team_id} AS team_id,
                        '{person_override.old_person_id}'::UUID AS old_person_id
                )
                """
            )
            lines = response.decode("utf-8").splitlines()
            assert len(lines) == 1
            ids = lines[0].split("\t")

            assert UUID(ids[0]) == person_override.old_person_id
            assert UUID(ids[1]) == person_override.override_person_id

    await activity_environment.run(drop_dictionary, query_inputs)


@pytest.mark.django_db
async def test_drop_dictionary(query_inputs, activity_environment, person_overrides_data, clickhouse_client):
    """Test a DICTIONARY is dropped by drop_join_table activity."""
    query_inputs.dictionary_name = "distinguished_dictionary"
    query_inputs.dry_run = False
    query_inputs.wait_for_mutations = True

    # Ensure we are starting from scratch
    await clickhouse_client.execute_query(
        f"DROP DICTIONARY IF EXISTS {settings.CLICKHOUSE_DATABASE}.{query_inputs.dictionary_name}"
    )
    response = await clickhouse_client.read_query(
        f"EXISTS DICTIONARY {settings.CLICKHOUSE_DATABASE}.{query_inputs.dictionary_name}"
    )
    before = int(response.splitlines()[0])
    assert before == 0

    await activity_environment.run(prepare_dictionary, query_inputs)

    response = await clickhouse_client.read_query(
        f"EXISTS DICTIONARY {settings.CLICKHOUSE_DATABASE}.{query_inputs.dictionary_name}"
    )
    during = int(response.splitlines()[0])
    assert during == 1

    await activity_environment.run(drop_dictionary, query_inputs)

    response = await clickhouse_client.read_query(
        f"EXISTS DICTIONARY {settings.CLICKHOUSE_DATABASE}.{query_inputs.dictionary_name}"
    )
    after = int(response.splitlines()[0])
    assert after == 0


get_team_id_old_person_id = operator.attrgetter("team_id", "old_person_id")


def is_equal_sorted(list_left, list_right, key=get_team_id_old_person_id) -> bool:
    """Compare two lists sorted by key are equal.

    Useful when we don't care about order.
    """
    return sorted(list_left, key=key) == sorted(list_right, key=key)


@pytest.mark.django_db
async def test_select_persons_to_delete(query_inputs, activity_environment, person_overrides_data):
    """Test selecting the correct dictionary of persons to delete.

    The select_persons_to_delete activity produces a dictionary of team_id to sets of
    old_person_id that will be overriden during the workflow and can be safely deleted
    afterwards.
    """
    query_inputs.dry_run = False
    query_inputs.partition_ids = ["202001"]
    query_inputs.latest_created_at = OVERRIDES_CREATED_AT

    to_delete = await activity_environment.run(select_persons_to_delete, query_inputs)

    expected = [
        SerializablePersonOverrideToDelete(
            team_id,
            person_override.old_person_id,
            person_override.override_person_id,
            OVERRIDES_CREATED_AT.isoformat(),
            1,
            OLDEST_EVENT_AT.isoformat(),
        )
        for team_id, person_overrides in person_overrides_data.items()
        for person_override in person_overrides
    ]

    assert is_equal_sorted(to_delete, expected)


@pytest.mark.django_db
async def test_select_persons_to_delete_selects_persons_in_older_partitions(
    query_inputs, activity_environment, person_overrides_data, older_overrides
):
    """Test all (new and old)  persons to delete are selected when older overrides exist.

    This is because if we are processing a newer override in the current workflow, there is
    no reason to keep an older mapping for the same old_person_id around. The squash query
    will prefer newer mappings, so the older ones do nothing and can cause troubles if left
    uncleaned.
    """
    query_inputs.dry_run = False
    query_inputs.partition_ids = ["202001"]
    query_inputs.latest_created_at = OVERRIDES_CREATED_AT

    to_delete = await activity_environment.run(select_persons_to_delete, query_inputs)

    expected = [
        SerializablePersonOverrideToDelete(
            team_id,
            person_override.old_person_id,
            person_override.override_person_id,
            OVERRIDES_CREATED_AT.isoformat(),
            1,
            OLDEST_EVENT_AT.isoformat(),
        )
        for team_id, person_overrides in person_overrides_data.items()
        for person_override in person_overrides
    ]

    expected.extend(
        [
            SerializablePersonOverrideToDelete(
                team_id,
                person_override.old_person_id,
                person_override.override_person_id,
                "2019-12-01T00:00:00+00:00",
                1,
                "2019-12-01T00:00:00+00:00",
            )
            for team_id, person_overrides in older_overrides.items()
            for person_override in person_overrides
        ]
    )

    assert is_equal_sorted(to_delete, expected)


@pytest.mark.django_db
async def test_select_persons_to_squash_with_empty_table(
    query_inputs, activity_environment, person_overrides_table, clickhouse_client
):
    """Test nothing is selected to override when there are no person_overrides.

    If there are no person_overrides rows, then there is no work for us to do.
    """
    await clickhouse_client.execute_query(PERSON_OVERRIDES_CREATE_TABLE_SQL)

    query_inputs.dry_run = False
    query_inputs.partition_ids = ["202001"]
    query_inputs.latest_created_at = OVERRIDES_CREATED_AT

    to_delete = await activity_environment.run(select_persons_to_delete, query_inputs)

    assert to_delete == []


@pytest.mark.django_db
async def test_select_persons_to_squash_with_different_partition(
    query_inputs, activity_environment, person_overrides_table, clickhouse_client
):
    """Test nothing is selected to override when there is no data in a partition.

    If there are no person_overrides rows, then there is no work for us to do.
    """
    await clickhouse_client.execute_query(PERSON_OVERRIDES_CREATE_TABLE_SQL)

    query_inputs.dry_run = False
    query_inputs.partition_ids = ["202001"]
    query_inputs.latest_created_at = OVERRIDES_CREATED_AT

    to_delete = await activity_environment.run(select_persons_to_delete, query_inputs)

    assert to_delete == []


@pytest.mark.django_db
async def test_select_persons_to_delete_with_newer_merges(
    query_inputs, activity_environment, person_overrides_data, clickhouse_client
):
    """Test selecting the correct persons to delete when we got new merges after starting.

    The select_persons_to_delete activity produces a dictionary of team_id to sets of
    old_person_id that will be overriden during the workflow and can be safely deleted
    afterwards.
    """
    newer_overrides = defaultdict(set)

    newer_values_to_insert = []
    for team_id, person_override in person_overrides_data.items():
        for old_person_id, override_person_id in person_override:
            override_person_id = uuid4()
            values: PersonOverrideValues = {
                "team_id": team_id,
                # Overrides for the same people
                "old_person_id": old_person_id,
                "override_person_id": override_person_id,
                # But happened an hour after
                "merged_at": OVERRIDES_CREATED_AT + timedelta(hours=1),
                "oldest_event": OLDEST_EVENT_AT + timedelta(hours=1),
                "created_at": OVERRIDES_CREATED_AT + timedelta(hours=1),
                "version": 1,
            }

            newer_overrides[team_id].add(PersonOverrideTuple(old_person_id, override_person_id))
            newer_values_to_insert.append(values)

    await clickhouse_client.execute_query("INSERT INTO person_overrides FORMAT JSONEachRow", *newer_values_to_insert)

    query_inputs.dry_run = False
    query_inputs.partition_ids = ["202001"]
    # Our latest_created_at is before the newer values happened
    query_inputs.latest_created_at = OVERRIDES_CREATED_AT

    to_delete = await activity_environment.run(select_persons_to_delete, query_inputs)

    expected = [
        SerializablePersonOverrideToDelete(
            team_id,
            person_override.old_person_id,
            person_override.override_person_id,
            OVERRIDES_CREATED_AT.isoformat(),
            1,
            OLDEST_EVENT_AT.isoformat(),
        )
        for team_id, person_overrides in person_overrides_data.items()
        for person_override in person_overrides
    ]

    assert is_equal_sorted(to_delete, expected)


class EventValues(TypedDict):
    """Events to be inserted for testing."""

    uuid: UUID
    event: str
    timestamp: datetime
    person_id: str
    team_id: int


@pytest_asyncio.fixture
async def events_to_override(person_overrides_data, clickhouse_client):
    """Produce some test events for testing.

    These events will be yielded so that we can re-fetch them and assert their
    person_ids have been overriden.
    """
    all_test_events = []
    for team_id, person_ids in person_overrides_data.items():
        for old_person_id, _ in person_ids:
            values: EventValues = {
                "uuid": uuid4(),
                "event": "test-event",
                "timestamp": OLDEST_EVENT_AT,
                "team_id": team_id,
                "person_id": old_person_id,
            }
            all_test_events.append(values)

    await clickhouse_client.execute_query(
        "INSERT INTO sharded_events FORMAT JSONEachRow",
        *all_test_events,
    )

    yield all_test_events

    await clickhouse_client.execute_query("TRUNCATE TABLE sharded_events")


async def assert_events_have_been_overriden(overriden_events, person_overrides):
    """Assert each event in overriden_events has actually been overriden.

    We use person_overrides to assert the person_id of each event now matches the
    overriden_person_id.
    """
    async with get_client() as clickhouse_client:
        for event in overriden_events:
            response = await clickhouse_client.read_query(
                "SELECT uuid, event, team_id, person_id FROM events WHERE uuid = %(uuid)s",
                query_parameters={"uuid": event["uuid"]},
            )
            row = response.decode("utf-8").splitlines()[0]
            values = [value for value in row.split("\t")]
            new_event = {
                "uuid": UUID(values[0]),
                "event": values[1],
                "team_id": int(values[2]),
                "person_id": UUID(values[3]),
            }

            assert event["uuid"] == new_event["uuid"]  # Sanity check
            assert event["team_id"] == new_event["team_id"]  # Sanity check
            assert event["person_id"] != new_event["person_id"]

            # If all is well, we should have overriden old_person_id with an override_person_id.
            # Let's find it first:
            new_person_id = [
                person_override.override_person_id
                for person_override in person_overrides[new_event["team_id"]]
                if person_override.old_person_id == event["person_id"]
            ][0]
            assert new_event["person_id"] == new_person_id


@pytest.mark.django_db
async def test_squash_events_partition(query_inputs, activity_environment, person_overrides_data, events_to_override):
    """Test events are properly squashed by running squash_events_partition.

    After running squash_events_partition, we iterate over the test events created by
    events_to_override and check the person_id associated with each of them. It should
    match the override_person_id associated with the old_person_id they used to be set to.
    """
    query_inputs.dictionary_name = "exciting_dictionary"
    query_inputs.partition_ids = ["202001"]
    query_inputs.latest_created_at = OVERRIDES_CREATED_AT
    query_inputs.dry_run = False
    query_inputs.wait_for_mutations = True

    await activity_environment.run(prepare_dictionary, query_inputs)

    await activity_environment.run(squash_events_partition, query_inputs)

    await assert_events_have_been_overriden(events_to_override, person_overrides_data)

    await activity_environment.run(drop_dictionary, query_inputs)


@pytest.mark.django_db
async def test_squash_events_partition_dry_run(
    query_inputs, activity_environment, person_overrides_data, events_to_override, clickhouse_client
):
    """Test events are not squashed by running squash_events_partition with dry_run=True."""
    query_inputs.dictionary_name = "exciting_dictionary"
    query_inputs.partition_ids = ["202001"]
    query_inputs.latest_created_at = OVERRIDES_CREATED_AT
    query_inputs.wait_for_mutations = True

    await activity_environment.run(prepare_dictionary, query_inputs)

    await activity_environment.run(squash_events_partition, query_inputs)

    await activity_environment.run(drop_dictionary, query_inputs)

    for event in events_to_override:
        response = await clickhouse_client.read_query(
            "SELECT uuid, event, team_id, person_id FROM events WHERE uuid = %(uuid)s",
            query_parameters={"uuid": event["uuid"]},
        )
        row = response.decode("utf-8").splitlines()[0]
        values = [value for value in row.split("\t")]
        new_event = {
            "uuid": UUID(values[0]),
            "event": values[1],
            "team_id": int(values[2]),
            "person_id": UUID(values[3]),
        }

        assert event["uuid"] == new_event["uuid"]  # Sanity check
        assert event["team_id"] == new_event["team_id"]  # Sanity check
        assert event["person_id"] == new_event["person_id"]


@pytest.mark.django_db
async def test_squash_events_partition_with_older_overrides(
    query_inputs,
    activity_environment,
    person_overrides_data,
    events_to_override,
    older_overrides,
):
    """Test events are properly squashed even in the prescence of older overrides.

    If we get an override from Postgres we can be sure it's the only one for a given
    old_person_id as PG constraints enforce uniqueness on the mapping. However, ClickHouse
    doesn't enforce any kind of uniqueness constraints, so our queries need to be aware there
    could be duplicate overrides present, either in the partition we are currently working
    with as well as older ones.
    """
    query_inputs.dictionary_name = "exciting_dictionary"
    query_inputs.partition_ids = ["202001"]
    query_inputs.latest_created_at = OVERRIDES_CREATED_AT
    query_inputs.dry_run = False
    query_inputs.wait_for_mutations = True

    await activity_environment.run(prepare_dictionary, query_inputs)

    await activity_environment.run(squash_events_partition, query_inputs)

    await activity_environment.run(drop_dictionary, query_inputs)

    await assert_events_have_been_overriden(events_to_override, person_overrides_data)


@pytest.mark.django_db
@pytest.mark.asyncio
async def test_squash_events_partition_with_newer_overrides(
    query_inputs,
    activity_environment,
    person_overrides_data,
    events_to_override,
    newer_overrides,
):
    """Test events are properly squashed even in the prescence of newer overrides.

    If we get an override from Postgres we can get be sure it's the only one for a given
    old_person_id as PG constraints enforce uniqueness on the mapping. However, ClickHouse
    doesn't enforce any kind of uniqueness constraints, so our queries need to be aware there
    could be duplicate overrides present, either in the partition we are currently working
    with as well as newer ones.
    """
    query_inputs.dictionary_name = "exciting_dictionary"
    query_inputs.partition_ids = ["202001"]
    query_inputs.latest_created_at = OVERRIDES_CREATED_AT
    query_inputs.dry_run = False
    query_inputs.wait_for_mutations = True

    await activity_environment.run(prepare_dictionary, query_inputs)

    await activity_environment.run(squash_events_partition, query_inputs)

    await activity_environment.run(drop_dictionary, query_inputs)

    await assert_events_have_been_overriden(events_to_override, newer_overrides)


@pytest.mark.django_db
@pytest.mark.asyncio
async def test_squash_events_partition_with_limited_team_ids(
    query_inputs, activity_environment, person_overrides_data, events_to_override
):
    """Test events are properly squashed when we specify team_ids."""
    dictionary_name = "exciting_limited_dictionary"
    random_team = random.choice(list(person_overrides_data.keys()))
    query_inputs.dictionary_name = dictionary_name
    query_inputs.partition_ids = ["202001"]
    query_inputs.latest_created_at = OVERRIDES_CREATED_AT
    query_inputs.dry_run = False
    query_inputs.team_ids = [random_team]
    query_inputs.wait_for_mutations = True

    latest_created_at = await activity_environment.run(prepare_dictionary, query_inputs)
    query_inputs._latest_created_at = latest_created_at

    await activity_environment.run(squash_events_partition, query_inputs)

    await activity_environment.run(drop_dictionary, query_inputs)

    with pytest.raises(AssertionError):
        # Some checks will fail as we have limited the teams overriden.
        await assert_events_have_been_overriden(events_to_override, person_overrides_data)

    # But if we only check the limited teams, there shouldn't be any issues.
    limited_events = [event for event in events_to_override if event["team_id"] == random_team]
    await assert_events_have_been_overriden(limited_events, person_overrides_data)


@pytest.mark.django_db
@pytest.mark.asyncio
async def test_delete_squashed_person_overrides_from_clickhouse(
    query_inputs, activity_environment, person_overrides_data, clickhouse_client
):
    """Test we can delete person overrides that have already been squashed.

    For the purposes of this unit test, we take the person overrides as given. A
    comprehensive test will cover the entire worflow end-to-end.

    We insert an extra person to ensure we are not deleting persons we shouldn't
    delete.
    """
    persons_to_delete = [
        SerializablePersonOverrideToDelete(
            team_id,
            person_override.old_person_id,
            person_override.override_person_id,
            OVERRIDES_CREATED_AT.isoformat(),
            1,
            OLDEST_EVENT_AT.isoformat(),
        )
        for team_id, person_overrides in person_overrides_data.items()
        for person_override in person_overrides
    ]
    query_inputs.partition_ids = ["202001"]
    query_inputs.latest_created_at = OVERRIDES_CREATED_AT
    query_inputs.dry_run = False
    query_inputs.person_overrides_to_delete = persons_to_delete
    query_inputs.wait_for_mutations = True

    not_overriden_id = uuid4()
    not_overriden_person: PersonOverrideValues = {
        "team_id": 1,
        "old_person_id": not_overriden_id,
        "override_person_id": uuid4(),
        "merged_at": OVERRIDES_CREATED_AT,
        "oldest_event": OLDEST_EVENT_AT,
        "created_at": OVERRIDES_CREATED_AT,
        "version": 1,
    }

    await clickhouse_client.execute_query("INSERT INTO person_overrides FORMAT JSONEachRow", not_overriden_person)

    await activity_environment.run(delete_squashed_person_overrides_from_clickhouse, query_inputs)

    response = await clickhouse_client.read_query("SELECT team_id, old_person_id FROM person_overrides")
    rows = response.decode("utf-8").splitlines()

    assert len(rows) == 1

    row = rows[0].split("\t")
    assert int(row[0]) == 1
    assert UUID(row[1]) == not_overriden_id


@pytest.mark.django_db
@pytest.mark.asyncio
async def test_delete_squashed_person_overrides_from_clickhouse_dry_run(
    query_inputs, activity_environment, person_overrides_data, clickhouse_client
):
    """Test we do not delete person overrides when dry_run=True."""
    persons_to_delete = [
        SerializablePersonOverrideToDelete(
            team_id,
            person_override.old_person_id,
            person_override.override_person_id,
            OVERRIDES_CREATED_AT.isoformat(),
            1,
            OLDEST_EVENT_AT.isoformat(),
        )
        for team_id, person_overrides in person_overrides_data.items()
        for person_override in person_overrides
    ]
    query_inputs.partition_ids = ["202001"]
    query_inputs.latest_created_at = OVERRIDES_CREATED_AT
    query_inputs.dry_run = True
    query_inputs.person_overrides_to_delete = persons_to_delete
    query_inputs.wait_for_mutations = True

    not_overriden_id = uuid4()
    not_overriden_person: PersonOverrideValues = {
        "team_id": 1,
        "old_person_id": not_overriden_id,
        "override_person_id": uuid4(),
        "merged_at": OVERRIDES_CREATED_AT,
        "oldest_event": OLDEST_EVENT_AT,
        "created_at": OVERRIDES_CREATED_AT,
        "version": 1,
    }

    await clickhouse_client.execute_query("INSERT INTO person_overrides FORMAT JSONEachRow", not_overriden_person)

    await activity_environment.run(delete_squashed_person_overrides_from_clickhouse, query_inputs)

    response = await clickhouse_client.read_query("SELECT team_id, old_person_id FROM person_overrides")
    rows = response.decode("utf-8").splitlines()

    assert len(rows) == len(persons_to_delete) + 1


@pytest.fixture(scope="session")
def django_db_setup_fixture():
    """Re-use pytest_django's django_db_setup."""
    from pytest_django.fixtures import django_db_setup

    yield django_db_setup


@pytest.fixture
def pg_connection():
    """Manage a Postgres connection with psycopg2."""
    conn = psycopg2.connect(
        dbname=settings.DATABASES["default"]["NAME"],
        user=settings.DATABASES["default"]["USER"],
        password=settings.DATABASES["default"]["PASSWORD"],
        host=settings.DATABASES["default"]["HOST"],
        port=settings.DATABASES["default"]["PORT"],
    )

    try:
        yield conn
    finally:
        conn.close()


@pytest.fixture
def organization_uuid(pg_connection, query_inputs, django_db_setup_fixture):
    """Create an Organization and return its UUID.

    We cannot use the Django ORM safely in an async context, so we INSERT INTO directly
    on the database. This means we need to clean up after ourselves, which we do after
    yielding.
    """
    organization_uuid = uuid4()

    with pg_connection:
        with pg_connection.cursor() as cursor:
            cursor.execute(
                """
                INSERT INTO posthog_organization (
                    id,
                    name,
                    created_at,
                    updated_at,
                    personalization,
                    setup_section_2_completed,
                    plugins_access_level,
                    for_internal_metrics,
                    available_features,
                    domain_whitelist,
                    is_member_join_email_enabled,
                    slug
                )
                VALUES (
                    %(uuid)s,
                    'test-workflows-org',
                    NOW(),
                    NOW(),
                    '{}',
                    TRUE,
                    1,
                    FALSE,
                    '{}',
                    '{}',
                    TRUE,
                    'test-worflows-org'
                )
                """,
                {"uuid": organization_uuid},
            )

    yield organization_uuid

    with pg_connection:
        with pg_connection.cursor() as cursor:
            cursor.execute("DELETE FROM posthog_organization WHERE id = %s", [organization_uuid])


@pytest.fixture
def team_id(query_inputs, organization_uuid, pg_connection):
    """Create a Team and return its ID.

    We cannot use the Django ORM safely in an async context, so we INSERT INTO directly
    on the database. This means we need to clean up after ourselves, which we do after
    yielding.
    """
    with pg_connection:
        with pg_connection.cursor() as cursor:
            cursor.execute(
                """
                INSERT INTO posthog_team (
                    api_token,
                    name,
                    opt_out_capture,
                    app_urls,
                    event_names,
                    event_properties,
                    anonymize_ips,
                    completed_snippet_onboarding,
                    created_at,
                    updated_at,
                    event_properties_numerical,
                    ingested_event,
                    uuid,
                    organization_id,
                    session_recording_opt_in,
                    plugins_opt_in,
                    event_names_with_usage,
                    event_properties_with_usage,
                    is_demo,
                    test_account_filters,
                    timezone,
                    data_attributes,
                    access_control
                )
                VALUES (
                     'the_token',
                     'test_workflow_team',
                     TRUE,
                     '{}',
                     '{}',
                     '{}',
                     TRUE,
                     TRUE,
                     NOW(),
                     NOW(),
                     '{}',
                     TRUE,
                     '00000000-0000-0000-0000-000000000000'::UUID,
                     %(organization_uuid)s,
                     TRUE,
                     TRUE,
                     '{}',
                     '{}',
                     FALSE,
                     '{}',
                     'UTC',
                     '{}',
                     TRUE
                )
                RETURNING id
                """,
                {"organization_uuid": organization_uuid},
            )
            [team_id] = cursor.fetchone()

    yield team_id

    with pg_connection:
        with pg_connection.cursor() as cursor:
            cursor.execute("DELETE FROM posthog_team WHERE id = %s", [team_id])


@pytest.fixture
def postgres_person_override(team_id, pg_connection) -> Iterator[PersonOverrideTuple]:
    """Create a PersonOverrideMapping and a PersonOverride.

    We cannot use the Django ORM safely in an async context, so we INSERT INTO directly
    on the database. This means we need to clean up after ourselves, which we do after
    yielding.
    """
    override = PersonOverrideTuple(uuid4(), uuid4())

    with pg_connection:
        FlatPostgresPersonOverridesManager(pg_connection).insert(team_id, override)

    yield override

    with pg_connection:
        FlatPostgresPersonOverridesManager(pg_connection).clear(team_id)


@pytest.mark.django_db
@pytest.mark.asyncio
async def test_delete_squashed_person_overrides_from_postgres(
    query_inputs,
    activity_environment,
    team_id,
    postgres_person_override: PersonOverrideTuple,
    pg_connection,
):
    """Test we can delete person overrides that have already been squashed.

    For the purposes of this unit test, we take the person overrides as given. A
    comprehensive test will cover the entire worflow end-to-end.
    """
    # These are sanity checks to ensure the fixtures are working properly.
    # If any assertions fail here, its likely a test setup issue.
    with pg_connection:
        assert FlatPostgresPersonOverridesManager(pg_connection).fetchall(team_id) == [postgres_person_override]

    person_overrides_to_delete = [
        SerializablePersonOverrideToDelete(
            team_id,
            postgres_person_override.old_person_id,
            postgres_person_override.override_person_id,
            OVERRIDES_CREATED_AT.isoformat(),
            1,
            OLDEST_EVENT_AT.isoformat(),
        )
    ]
    query_inputs.person_overrides_to_delete = person_overrides_to_delete
    query_inputs.dry_run = False

    await activity_environment.run(delete_squashed_person_overrides_from_postgres, query_inputs)

    with pg_connection:
        assert FlatPostgresPersonOverridesManager(pg_connection).fetchall(team_id) == []


@pytest.mark.django_db
@pytest.mark.asyncio
async def test_delete_squashed_person_overrides_from_postgres_dry_run(
    query_inputs,
    activity_environment,
    team_id,
    postgres_person_override: PersonOverrideTuple,
    pg_connection,
):
    """Test we do not delete person overrides when dry_run=True."""
    # These are sanity checks to ensure the fixtures are working properly.
    # If any assertions fail here, its likely a test setup issue.
    with pg_connection:
        assert FlatPostgresPersonOverridesManager(pg_connection).fetchall(team_id) == [postgres_person_override]

    person_overrides_to_delete = [
        SerializablePersonOverrideToDelete(
            team_id,
            postgres_person_override.old_person_id,
            postgres_person_override.override_person_id,
            OVERRIDES_CREATED_AT.isoformat(),
            1,
            OLDEST_EVENT_AT.isoformat(),
        )
    ]
    query_inputs.person_overrides_to_delete = person_overrides_to_delete
    query_inputs.dry_run = True

    await activity_environment.run(delete_squashed_person_overrides_from_postgres, query_inputs)

    with pg_connection:
        assert FlatPostgresPersonOverridesManager(pg_connection).fetchall(team_id) == [postgres_person_override]


@pytest.mark.django_db
async def test_squash_person_overrides_workflow(
    events_to_override,
    person_overrides_data,
    postgres_person_override: PersonOverrideTuple,
    person_overrides_table,
    clickhouse_client,
):
    """Test the squash_person_overrides workflow end-to-end."""
    client = await Client.connect(
        f"{settings.TEMPORAL_HOST}:{settings.TEMPORAL_PORT}",
        namespace=settings.TEMPORAL_NAMESPACE,
    )

    workflow_id = str(uuid4())
    inputs = SquashPersonOverridesInputs(
        partition_ids=["202001"],
        dry_run=False,
    )

    async with Worker(
        client,
        task_queue=settings.TEMPORAL_TASK_QUEUE,
        workflows=[SquashPersonOverridesWorkflow],
        activities=[
            prepare_person_overrides,
            prepare_dictionary,
            select_persons_to_delete,
            squash_events_partition,
            drop_dictionary,
            delete_squashed_person_overrides_from_clickhouse,
            delete_squashed_person_overrides_from_postgres,
        ],
        workflow_runner=UnsandboxedWorkflowRunner(),
    ):
        await client.execute_workflow(
            SquashPersonOverridesWorkflow.run,
            inputs,
            id=workflow_id,
            task_queue=settings.TEMPORAL_TASK_QUEUE,
        )

    await assert_events_have_been_overriden(events_to_override, person_overrides_data)

    response = await clickhouse_client.read_query("SELECT team_id, old_person_id FROM person_overrides")
    rows = response.splitlines()
    assert len(rows) == 0


@pytest.mark.django_db
async def test_squash_person_overrides_workflow_with_newer_overrides(
    events_to_override,
    person_overrides_data,
    postgres_person_override: PersonOverrideTuple,
    newer_overrides,
):
    """Test the squash_person_overrides workflow end-to-end with newer overrides."""
    client = await Client.connect(
        f"{settings.TEMPORAL_HOST}:{settings.TEMPORAL_PORT}",
        namespace=settings.TEMPORAL_NAMESPACE,
    )

    workflow_id = str(uuid4())
    inputs = SquashPersonOverridesInputs(
        partition_ids=["202001"],
        dry_run=False,
        wait_for_mutations=True,
    )

    async with Worker(
        client,
        task_queue=settings.TEMPORAL_TASK_QUEUE,
        workflows=[SquashPersonOverridesWorkflow],
        activities=[
            prepare_person_overrides,
            prepare_dictionary,
            select_persons_to_delete,
            squash_events_partition,
            drop_dictionary,
            delete_squashed_person_overrides_from_clickhouse,
            delete_squashed_person_overrides_from_postgres,
        ],
        workflow_runner=UnsandboxedWorkflowRunner(),
    ):
        await client.execute_workflow(
            SquashPersonOverridesWorkflow.run,
            inputs,
            id=workflow_id,
            task_queue=settings.TEMPORAL_TASK_QUEUE,
        )

    await assert_events_have_been_overriden(events_to_override, newer_overrides)


@pytest.mark.django_db
async def test_squash_person_overrides_workflow_with_limited_team_ids(
    events_to_override,
    person_overrides_data,
    postgres_person_override: PersonOverrideTuple,
):
    """Test the squash_person_overrides workflow end-to-end."""
    client = await Client.connect(
        f"{settings.TEMPORAL_HOST}:{settings.TEMPORAL_PORT}",
        namespace=settings.TEMPORAL_NAMESPACE,
    )

    workflow_id = str(uuid4())
    random_team = random.choice(list(person_overrides_data.keys()))
    inputs = SquashPersonOverridesInputs(
        partition_ids=["202001"],
        team_ids=[random_team],
        dry_run=False,
        wait_for_mutations=True,
    )

    async with Worker(
        client,
        task_queue=settings.TEMPORAL_TASK_QUEUE,
        workflows=[SquashPersonOverridesWorkflow],
        activities=[
            prepare_person_overrides,
            prepare_dictionary,
            select_persons_to_delete,
            squash_events_partition,
            drop_dictionary,
            delete_squashed_person_overrides_from_clickhouse,
            delete_squashed_person_overrides_from_postgres,
        ],
        workflow_runner=UnsandboxedWorkflowRunner(),
    ):
        await client.execute_workflow(
            SquashPersonOverridesWorkflow.run,
            inputs,
            id=workflow_id,
            task_queue=settings.TEMPORAL_TASK_QUEUE,
        )

    with pytest.raises(AssertionError):
        # Some checks will fail as we have limited the teams overriden.
        await assert_events_have_been_overriden(events_to_override, person_overrides_data)

    # But if we only check the limited teams, there shouldn't be any issues.
    limited_events = [event for event in events_to_override if event["team_id"] == random_team]
    await assert_events_have_been_overriden(limited_events, person_overrides_data)
