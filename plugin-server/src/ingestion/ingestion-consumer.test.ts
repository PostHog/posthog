import { DateTime } from 'luxon'
import { Message } from 'node-rdkafka'

import { UUIDT } from '~/src/utils/utils'
import {
    getProducedKafkaMessages,
    getProducedKakfaMessagesForTopic,
    mockProducer,
} from '~/tests/helpers/mocks/producer.mock'
import { forSnapshot } from '~/tests/helpers/snapshots'

import { Hub, PipelineEvent, Team } from '../../src/types'
import { closeHub, createHub } from '../../src/utils/db/hub'
import { createTeam, getFirstTeam, resetTestDatabase } from '../../tests/helpers/sql'
import { status } from '../utils/status'
import { IngestionConsumer } from './ingestion-consumer'

const mockConsumer = {
    on: jest.fn(),
    commitSync: jest.fn(),
    commit: jest.fn(),
    queryWatermarkOffsets: jest.fn(),
    committed: jest.fn(),
    assignments: jest.fn(),
    isConnected: jest.fn(() => true),
    getMetadata: jest.fn(),
}

jest.mock('../../src/kafka/batch-consumer', () => {
    return {
        startBatchConsumer: jest.fn(() =>
            Promise.resolve({
                join: () => ({
                    finally: jest.fn(),
                }),
                stop: jest.fn(),
                consumer: mockConsumer,
            })
        ),
    }
})

jest.setTimeout(1000)

let offsetIncrementer = 0

const createKafkaMessages: (events: PipelineEvent[]) => Message[] = (events) => {
    return events.map((event) => {
        // TRICKY: This is the slightly different format that capture sends
        const captureEvent = {
            uuid: event.uuid,
            distinct_id: event.distinct_id,
            ip: event.ip,
            now: event.now,
            token: event.token,
            data: JSON.stringify(event),
        }
        return {
            value: Buffer.from(JSON.stringify(captureEvent)),
            size: 1,
            topic: 'test',
            offset: offsetIncrementer++,
            partition: 1,
        }
    })
}

describe('IngestionConsumer', () => {
    let ingester: IngestionConsumer
    let hub: Hub
    let team: Team
    let team2: Team

    const createEvent = (event?: Partial<PipelineEvent>): PipelineEvent => ({
        distinct_id: 'user-1',
        uuid: new UUIDT().toString(),
        token: team.api_token,
        ip: '127.0.0.1',
        site_url: 'us.posthog.com',
        now: DateTime.now().toISO(),
        event: '$pageview',
        properties: {
            $current_url: 'http://localhost:8000',
        },
        ...event,
    })

    beforeEach(async () => {
        const now = new Date(2025, 1, 1).getTime()
        jest.spyOn(Date, 'now').mockReturnValue(now)
        offsetIncrementer = 0
        await resetTestDatabase()
        hub = await createHub()

        hub.kafkaProducer = mockProducer
        team = await getFirstTeam(hub)
        const team2Id = await createTeam(hub.db.postgres, team.organization_id)
        team2 = (await hub.db.fetchTeam(team2Id)) as Team
    })

    afterEach(async () => {
        jest.restoreAllMocks()
        jest.setTimeout(10000)
        if (ingester) {
            await ingester.stop()
        }
        await closeHub(hub)
    })

    afterAll(() => {
        jest.useRealTimers()
    })

    describe('general', () => {
        beforeEach(async () => {
            ingester = new IngestionConsumer(hub)
            await ingester.start()
        })

        it('should have the correct config', () => {
            expect(ingester['name']).toMatchInlineSnapshot(`"ingestion-consumer-events_plugin_ingestion_test"`)
            expect(ingester['groupId']).toMatchInlineSnapshot(`"events-ingestion-consumer"`)
            expect(ingester['topic']).toMatchInlineSnapshot(`"events_plugin_ingestion_test"`)
            expect(ingester['dlqTopic']).toMatchInlineSnapshot(`"events_plugin_ingestion_dlq_test"`)
            expect(ingester['overflowTopic']).toMatchInlineSnapshot(`"events_plugin_ingestion_overflow_test"`)
        })

        it('should process a standard event', async () => {
            await ingester.handleKafkaBatch(createKafkaMessages([createEvent()]))

            expect(forSnapshot(getProducedKafkaMessages())).toMatchSnapshot()
        })
    })

    describe('dropping events', () => {
        describe.each(['headers', 'payload'] as const)('via %s', (kind) => {
            const addMessageHeaders = (message: Message, token?: string, distinctId?: string) => {
                if (kind !== 'headers') {
                    return
                }
                message.headers = []

                if (distinctId) {
                    message.headers.push({
                        key: 'distinct_id',
                        value: Buffer.from(distinctId),
                    })
                }
                if (token) {
                    message.headers.push({
                        key: 'token',
                        value: Buffer.from(token),
                    })
                }
            }

            beforeEach(() => {
                jest.spyOn(status, 'debug')
            })

            const expectDropLogs = (pairs: [string, string | undefined][]) => {
                expect(jest.mocked(status.debug)).toHaveBeenCalledTimes(pairs.length)
                for (const [token, distinctId] of pairs) {
                    expect(jest.mocked(status.debug)).toHaveBeenCalledWith('ðŸ”', 'Dropped event', {
                        distinctId,
                        token,
                    })
                }
            }

            describe('with DROP_EVENTS_BY_TOKEN', () => {
                beforeEach(async () => {
                    hub.DROP_EVENTS_BY_TOKEN = `${team.api_token},phc_other`
                    ingester = new IngestionConsumer(hub)
                    await ingester.start()
                })

                it('should drop events with matching token', async () => {
                    const messages = createKafkaMessages([createEvent({}), createEvent({})])
                    addMessageHeaders(messages[0], team.api_token)
                    await ingester.handleKafkaBatch(messages)
                    expect(getProducedKakfaMessagesForTopic('clickhouse_events_json_test')).toHaveLength(0)
                    expectDropLogs([
                        [team.api_token, 'user-1'],
                        [team.api_token, 'user-1'],
                    ])
                })

                it('should not drop events for a different team token', async () => {
                    const messages = createKafkaMessages([createEvent({ token: team2.api_token })])
                    addMessageHeaders(messages[0], team2.api_token)
                    await ingester.handleKafkaBatch(messages)
                    expect(getProducedKakfaMessagesForTopic('clickhouse_events_json_test')).not.toHaveLength(0)
                    expectDropLogs([])
                })

                it('should only drop events in batch matching', async () => {
                    const messages = createKafkaMessages([
                        createEvent({ token: team.api_token }),
                        createEvent({ token: team2.api_token, distinct_id: 'team2-distinct-id' }),
                        createEvent({ token: team.api_token }),
                    ])
                    addMessageHeaders(messages[0], team.api_token)
                    addMessageHeaders(messages[1], team2.api_token)
                    addMessageHeaders(messages[2], team.api_token)
                    await ingester.handleKafkaBatch(messages)
                    const eventsMessages = getProducedKakfaMessagesForTopic('clickhouse_events_json_test')
                    expect(eventsMessages).toHaveLength(1)
                    expect(eventsMessages[0].value).toMatchObject({
                        team_id: team2.id,
                        distinct_id: 'team2-distinct-id',
                    })
                    expectDropLogs([
                        [team.api_token, kind === 'headers' ? undefined : 'user-1'],
                        [team.api_token, kind === 'headers' ? undefined : 'user-1'],
                    ])
                })
            })

            describe('with DROP_EVENTS_BY_TOKEN_DISTINCT_ID', () => {
                beforeEach(async () => {
                    hub.DROP_EVENTS_BY_TOKEN_DISTINCT_ID = `${team.api_token}:distinct-id-to-ignore,phc_other:distinct-id-to-ignore`
                    ingester = new IngestionConsumer(hub)
                    await ingester.start()
                })
                it('should drop events with matching token and distinct_id', async () => {
                    const messages = createKafkaMessages([
                        createEvent({
                            distinct_id: 'distinct-id-to-ignore',
                        }),
                    ])
                    addMessageHeaders(messages[0], team.api_token, 'distinct-id-to-ignore')
                    await ingester.handleKafkaBatch(messages)
                    expect(getProducedKakfaMessagesForTopic('clickhouse_events_json_test')).toHaveLength(0)
                    expectDropLogs([[team.api_token, 'distinct-id-to-ignore']])
                })

                it('should not drop events for a different team token', async () => {
                    const messages = createKafkaMessages([
                        createEvent({
                            token: team2.api_token,
                            distinct_id: 'distinct-id-to-ignore',
                        }),
                    ])
                    addMessageHeaders(messages[0], team2.api_token, 'distinct-id-to-ignore')
                    await ingester.handleKafkaBatch(messages)
                    expect(getProducedKakfaMessagesForTopic('clickhouse_events_json_test')).not.toHaveLength(0)
                    expectDropLogs([])
                })

                it('should not drop events for a different distinct_id', async () => {
                    const messages = createKafkaMessages([
                        createEvent({
                            distinct_id: 'other-id',
                        }),
                    ])
                    addMessageHeaders(messages[0], team.api_token, 'not-ignored')
                    await ingester.handleKafkaBatch(messages)
                    expect(getProducedKakfaMessagesForTopic('clickhouse_events_json_test')).not.toHaveLength(0)
                    expectDropLogs([])
                })
            })
        })
    })
})
