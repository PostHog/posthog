# Front matter
title = "Google LLM analytics installation"
description = "Step-by-step guide to install PostHog SDK for Google Gen AI LLM analytics"
product = "llm_analytics"
sdk = ["python", "node"]

# Step 1: Install PostHog SDK
[[steps]]
title = "Install the PostHog SDK"
badge = "required"

[[steps.content]]
type = "markdown"
content = """
Setting up analytics starts with installing the PostHog SDK for your language. LLM analytics works best with our Python and Node SDKs.
"""

[[steps.content]]
type = "multi_code"
languages = ["python", "node"]

[[steps.content.code_blocks]]
language = "bash"
label = "Python"
tab = "python"
content = """
pip install posthog
"""

[[steps.content.code_blocks]]
language = "bash"
label = "Node"
tab = "node"
content = """
npm install @posthog/ai posthog-node
"""

# Step 2: Install Google Gen AI SDK
[[steps]]
title = "Install the Google Gen AI SDK"
badge = "required"

[[steps.content]]
type = "markdown"
content = """
Install the Google Gen AI SDK:
"""

[[steps.content]]
type = "multi_code"
languages = ["python", "node"]

[[steps.content.code_blocks]]
language = "bash"
label = "Python"
tab = "python"
content = """
pip install google-genai
"""

[[steps.content.code_blocks]]
language = "bash"
label = "Node"
tab = "node"
content = """
npm install @google/genai
"""

[[steps.content]]
type = "callout"
icon = "IconInfo"
title = "Proxy note"
callout_type = "fyi"
content = """
These SDKs **do not** proxy your calls. They only fire off an async call to PostHog in the background to send the data.

You can also use LLM analytics with other SDKs or our API, but you will need to capture the data in the right format. See the schema in the [manual capture section](/docs/llm-analytics/installation/manual-capture) for more details.
"""

# Step 3: Initialize PostHog and Google Gen AI client
[[steps]]
title = "Initialize PostHog and Google Gen AI client"
badge = "required"

[[steps.content]]
type = "markdown"
content = """
Initialize PostHog with your project API key and host from [your project settings](https://app.posthog.com/settings/project), then pass it to our Google Gen AI wrapper.
"""

[[steps.content]]
type = "multi_code"
languages = ["python", "node"]

[[steps.content.code_blocks]]
language = "python"
label = "Python"
tab = "python"
content = """
from posthog.ai.gemini import Client
from posthog import Posthog

posthog = Posthog(
    "<ph_project_api_key>",
    host="<ph_client_api_host>"
)

client = Client(
    api_key="...", # Replace with your Gemini API key
    posthog_client=posthog # This is an optional parameter. If it is not provided, a default client will be used.
)
"""

[[steps.content.code_blocks]]
language = "typescript"
label = "Node"
tab = "node"
content = """
import { GoogleGenAI } from '@posthog/ai'
import { PostHog } from 'posthog-node'

const phClient = new PostHog(
    '<ph_project_api_key>',
    { host: '<ph_client_api_host>' }
)

const client = new GoogleGenAI({
    apiKey: '...', // Replace with your Gemini API key
    posthog: phClient
})
"""

[[steps.content]]
type = "note"
content = """
**Note:** This integration also works with Vertex AI via Google Cloud Platform. You can use the Google Gen AI SDK's Vertex AI client with PostHog analytics.
"""

[[steps.content]]
type = "collapsible"
title = "Click to see Vertex AI code example"

[[steps.content.collapsible_content]]
type = "multi_code"
languages = ["python", "node"]

[[steps.content.collapsible_content.code_blocks]]
language = "python"
label = "Python"
tab = "python"
content = """
from posthog import Posthog
from posthog.ai.gemini import Client

# Initialize PostHog
posthog = Posthog(
    project_api_key="<ph_project_api_key>",
    host="<ph_client_api_host>"
)

# Initialize Gemini client with Vertex AI
client = Client(
    vertexai=True,
    project="your-gcp-project-id",
    location="us-central1",
    posthog_client=posthog,
    posthog_distinct_id="user-123"
)

# Use it
response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents=["Hello, world!"]
)

print(response.text)
"""

[[steps.content.collapsible_content.code_blocks]]
language = "typescript"
label = "Node"
tab = "node"
content = """
import { PostHog } from 'posthog-node'
import { PostHogGoogleGenAI } from '@posthog/ai'

// Initialize PostHog
const posthog = new PostHog(
  '<ph_project_api_key>',
  { host: '<ph_client_api_host>' }
)

// Initialize Gemini client with Vertex AI
const client = new PostHogGoogleGenAI({
  vertexai: true,
  project: 'your-gcp-project-id',
  location: 'us-central1',
  posthog: posthog
})

// Use it
const response = await client.models.generateContent({
  model: 'gemini-2.0-flash',
  contents: 'Hello, world!',
  posthogDistinctId: 'user-123'
})

console.log(response.text)
"""

# Step 4: Call Google Gen AI LLMs
[[steps]]
title = "Call Google Gen AI LLMs"
badge = "required"

[[steps.content]]
type = "markdown"
content = """
Now, when you use the Google Gen AI SDK to call LLMs, PostHog automatically captures an `$ai_generation` event.

You can enrich the event with additional data such as the trace ID, distinct ID, custom properties, groups, and privacy mode options.
"""

[[steps.content]]
type = "multi_code"
languages = ["python", "node"]

[[steps.content.code_blocks]]
language = "python"
label = "Python"
tab = "python"
content = """
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=["Tell me a fun fact about hedgehogs"],
    posthog_distinct_id="user_123", # optional
    posthog_trace_id="trace_123", # optional
    posthog_properties={"conversation_id": "abc123", "paid": True}, # optional
    posthog_groups={"company": "company_id_in_your_db"},  # optional 
    posthog_privacy_mode=False # optional
)

print(response.text)
"""

[[steps.content.code_blocks]]
language = "typescript"
label = "Node"
tab = "node"
content = """
const response = await client.models.generateContent({
  model: "gemini-2.5-flash",
  contents: ["Tell me a fun fact about hedgehogs"],
  posthogDistinctId: "user_123", // optional
  posthogTraceId: "trace_123", // optional
  posthogProperties: { conversationId: "abc123", paid: true }, // optional
  posthogGroups: { company: "company_id_in_your_db" }, // optional
  posthogPrivacyMode: false // optional
})

console.log(response.text)
phClient.shutdown()
"""

[[steps.content]]
type = "note"
content = """
**Note:** If you want to capture LLM events anonymously, **don't** pass a distinct ID to the request. See our docs on [anonymous vs identified events](/docs/data/anonymous-vs-identified-events) to learn more.
"""

[[steps.content]]
type = "markdown"
content = """
You can expect captured `$ai_generation` events to have the following properties:
"""

[[steps.content]]
type = "table"
headers = ["Property", "Description"]
rows = [
    ["$ai_model", "The specific model, like `gpt-5-mini` or `claude-4-sonnet`"],
    ["$ai_latency", "The latency of the LLM call in seconds"],
    ["$ai_tools", "Tools and functions available to the LLM"],
    ["$ai_input", "List of messages sent to the LLM"],
    ["$ai_input_tokens", "The number of tokens in the input (often found in response.usage)"],
    ["$ai_output_choices", "List of response choices from the LLM"],
    ["$ai_output_tokens", "The number of tokens in the output (often found in `response.usage`)"],
    ["$ai_total_cost_usd", "The total cost in USD (input + output)"],
    ["[...]", "See [full list](/docs/llm-analytics/generations#event-properties) of properties"]
]

# Step 5: Verify traces and generations (checkpoint)
[[steps]]
title = "Verify traces and generations"
subtitle = "Confirm LLM events are being sent to PostHog"
type = "checkpoint"
badge = "checkpoint"

# Docs-specific content
[[steps.content.docs]]
type = "markdown"
content = """
Let's make sure LLM events are being captured and sent to PostHog. Under **LLM analytics**, you should see rows of data appear in the **Traces** and **Generations** tabs.
"""

[[steps.content.docs]]
type = "screenshot"
imageLight = "https://res.cloudinary.com/dmukukwp6/image/upload/SCR_20250807_syne_ecd0801880.png"
imageDark = "https://res.cloudinary.com/dmukukwp6/image/upload/SCR_20250807_syjm_5baab36590.png"
alt = "LLM generations in PostHog"
caption = "LLM generations in PostHog"

[[steps.content.docs]]
type = "button"
variant = "secondary"
size = "sm"
url = "https://app.posthog.com/llm-analytics/generations"
external = true
text = "Check for LLM events in PostHog"

# App-specific content
[[steps.content.app]]
type = "markdown"
content = """
Make some LLM calls, PostHog should now receive your LLM traces. Once your captures are ingested, you'll see the `Waiting for events` widget below update in realtime.

If PostHog receives LLM traces, the widget below will say `Installation Complete`.
"""
