# Front matter
title = "LangChain LLM analytics installation"
description = "Step-by-step guide to install PostHog SDK for LangChain LLM analytics"
product = "llm_analytics"
sdk = ["python", "node"]

# Step 1: Install PostHog SDK
[[steps]]
title = "Install the PostHog SDK"
badge = "required"

[[steps.content]]
type = "markdown"
content = """
Setting up analytics starts with installing the PostHog SDK for your language. LLM analytics works best with our Python and Node SDKs.
"""

[[steps.content]]
type = "multi_code"
languages = ["python", "node"]

[[steps.content.code_blocks]]
language = "bash"
label = "Python"
tab = "python"
content = """
pip install posthog
"""

[[steps.content.code_blocks]]
language = "bash"
label = "Node"
tab = "node"
content = """
npm install @posthog/ai posthog-node
"""

# Step 2: Install LangChain and OpenAI SDKs
[[steps]]
title = "Install LangChain and OpenAI SDKs"
badge = "required"

[[steps.content]]
type = "markdown"
content = """
Install the LangChain and OpenAI Python SDKs:
"""

[[steps.content]]
type = "multi_code"
languages = ["python", "node"]

[[steps.content.code_blocks]]
language = "bash"
label = "Python"
tab = "python"
content = """
pip install langchain openai langchain-openai
"""

[[steps.content.code_blocks]]
language = "bash"
label = "Node"
tab = "node"
content = """
npm install langchain @langchain/core @posthog/ai
"""

[[steps.content]]
type = "callout"
icon = "IconInfo"
title = "Proxy note"
callout_type = "fyi"
content = """
These SDKs **do not** proxy your calls. They only fire off an async call to PostHog in the background to send the data.

You can also use LLM analytics with other SDKs or our API, but you will need to capture the data in the right format. See the schema in the [manual capture section](/docs/llm-analytics/installation/manual-capture) for more details.
"""

# Step 3: Initialize PostHog and LangChain
[[steps]]
title = "Initialize PostHog and LangChain"
badge = "required"

[[steps.content]]
type = "markdown"
content = """
Initialize PostHog with your project API key and host from [your project settings](https://app.posthog.com/settings/project), then pass it to the LangChain `CallbackHandler` wrapper.

Optionally, you can provide a user distinct ID, trace ID, PostHog properties, [groups](/docs/product-analytics/group-analytics), and privacy mode.
"""

[[steps.content]]
type = "multi_code"
languages = ["python", "node"]

[[steps.content.code_blocks]]
language = "python"
label = "Python"
tab = "python"
content = """
from posthog.ai.langchain import CallbackHandler
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from posthog import Posthog

posthog = Posthog(
    "<ph_project_api_key>",
    host="<ph_client_api_host>"
)

callback_handler = CallbackHandler(
    client=posthog, # This is an optional parameter. If it is not provided, a default client will be used.
    distinct_id="user_123", # optional
    trace_id="trace_456", # optional
    properties={"conversation_id": "abc123"} # optional
    groups={"company": "company_id_in_your_db"} # optional
    privacy_mode=False # optional
)
"""

[[steps.content.code_blocks]]
language = "typescript"
label = "Node"
tab = "node"
content = """
import { PostHog } from 'posthog-node';
import { LangChainCallbackHandler } from '@posthog/ai';
import { ChatOpenAI } from '@langchain/openai';
import { ChatPromptTemplate } from '@langchain/core/prompts';

const phClient = new PostHog(
  '<ph_project_api_key>',
  { host: '<ph_client_api_host>' }
);

const callbackHandler = new LangChainCallbackHandler({
  client: phClient,
  distinctId: 'user_123', // optional
  traceId: 'trace_456', // optional
  properties: { conversationId: 'abc123' }, // optional
  groups: { company: 'company_id_in_your_db' }, // optional
  privacyMode: false, // optional
  debug: false // optional - when true, logs all events to console
});
"""

[[steps.content]]
type = "note"
content = """
**Note:** If you want to capture LLM events anonymously, **don't** pass a distinct ID to the `CallbackHandler`. See our docs on [anonymous vs identified events](/docs/data/anonymous-vs-identified-events) to learn more.
"""

# Step 4: Call LangChain
[[steps]]
title = "Call LangChain"
badge = "required"

[[steps.content]]
type = "markdown"
content = """
When you invoke your chain, pass the `callback_handler` in the `config` as part of your `callbacks`:
"""

[[steps.content]]
type = "multi_code"
languages = ["python", "node"]

[[steps.content.code_blocks]]
language = "python"
label = "Python"
tab = "python"
content = """
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    ("user", "{input}")
])

model = ChatOpenAI(openai_api_key="your_openai_api_key")
chain = prompt | model

# Execute the chain with the callback handler
response = chain.invoke(
    {"input": "Tell me a joke about programming"},
    config={"callbacks": [callback_handler]}
)

print(response.content)
"""

[[steps.content.code_blocks]]
language = "typescript"
label = "Node"
tab = "node"
content = """
const prompt = ChatPromptTemplate.fromMessages([
  ["system", "You are a helpful assistant."],
  ["user", "{input}"]
]);

const model = new ChatOpenAI({
  apiKey: "your_openai_api_key"
});

const chain = prompt.pipe(model);

// Execute the chain with the callback handler
const response = await chain.invoke(
  { input: "Tell me a joke about programming" },
  { callbacks: [callbackHandler] }
);

console.log(response.content);
phClient.shutdown();
"""

[[steps.content]]
type = "markdown"
content = """
PostHog automatically captures an `$ai_generation` event along with these properties:
"""

[[steps.content]]
type = "table"
headers = ["Property", "Description"]
rows = [
    ["$ai_model", "The specific model, like `gpt-5-mini` or `claude-4-sonnet`"],
    ["$ai_latency", "The latency of the LLM call in seconds"],
    ["$ai_tools", "Tools and functions available to the LLM"],
    ["$ai_input", "List of messages sent to the LLM"],
    ["$ai_input_tokens", "The number of tokens in the input (often found in response.usage)"],
    ["$ai_output_choices", "List of response choices from the LLM"],
    ["$ai_output_tokens", "The number of tokens in the output (often found in `response.usage`)"],
    ["$ai_total_cost_usd", "The total cost in USD (input + output)"],
    ["[...]", "See [full list](/docs/llm-analytics/generations#event-properties) of properties"]
]

[[steps.content]]
type = "markdown"
content = """
It also automatically creates a trace hierarchy based on how LangChain components are nested.
"""

# Step 5: Verify traces and generations (checkpoint)
[[steps]]
title = "Verify traces and generations"
subtitle = "Confirm LLM events are being sent to PostHog"
type = "checkpoint"
badge = "checkpoint"

# Docs-specific content
[[steps.content.docs]]
type = "markdown"
content = """
Let's make sure LLM events are being captured and sent to PostHog. Under **LLM analytics**, you should see rows of data appear in the **Traces** and **Generations** tabs.
"""

[[steps.content.docs]]
type = "screenshot"
imageLight = "https://res.cloudinary.com/dmukukwp6/image/upload/SCR_20250807_syne_ecd0801880.png"
imageDark = "https://res.cloudinary.com/dmukukwp6/image/upload/SCR_20250807_syjm_5baab36590.png"
alt = "LLM generations in PostHog"
caption = "LLM generations in PostHog"

[[steps.content.docs]]
type = "button"
variant = "secondary"
size = "sm"
url = "https://app.posthog.com/llm-analytics/generations"
external = true
text = "Check for LLM events in PostHog"

# App-specific content
[[steps.content.app]]
type = "markdown"
content = """
Make some LLM calls, PostHog should now receive your LLM traces. Once your captures are ingested, you'll see the `Waiting for events` widget below update in realtime.

If PostHog receives LLM traces, the widget below will say `Installation Complete`.
"""
