# Front matter
title = "LiteLLM analytics installation"
description = "Step-by-step guide to install PostHog SDK for LiteLLM analytics"
product = "llm_analytics"
sdk = ["python", "proxy"]

# Step 1: Install LiteLLM
[[steps]]
title = "Install LiteLLM"
badge = "required"

[[steps.content]]
type = "note"
content = """
**Note:** LiteLLM can be used as a Python SDK or as a proxy server. PostHog observability requires LiteLLM version 1.77.3 or higher.
"""

[[steps.content]]
type = "markdown"
content = """
Choose your installation method based on how you want to use LiteLLM:
"""

[[steps.content]]
type = "multi_code"
languages = ["sdk", "proxy"]

[[steps.content.code_blocks]]
language = "bash"
label = "SDK"
tab = "sdk"
content = """
pip install litellm
"""

[[steps.content.code_blocks]]
language = "bash"
label = "Proxy"
tab = "proxy"
content = """
# Install via pip
pip install 'litellm[proxy]'

# Or run via Docker
docker run --rm -p 4000:4000 ghcr.io/berriai/litellm:latest
"""

# Step 2: Configure PostHog observability
[[steps]]
title = "Configure PostHog observability"
badge = "required"

[[steps.content]]
type = "markdown"
content = """
Configure PostHog by setting your project API key and host as well as adding `posthog` to your LiteLLM callback handlers. You can find your API key in [your project settings](https://app.posthog.com/settings/project).
"""

[[steps.content]]
type = "multi_code"
languages = ["sdk", "proxy"]

[[steps.content.code_blocks]]
language = "python"
label = "SDK"
tab = "sdk"
content = """
import os
import litellm

# Set environment variables
os.environ["POSTHOG_API_KEY"] = "<ph_project_api_key>"
os.environ["POSTHOG_API_URL"] = "<ph_client_api_host>"  # Optional, defaults to https://app.posthog.com

# Enable PostHog callbacks
litellm.success_callback = ["posthog"]
litellm.failure_callback = ["posthog"]  # Optional: also log failures
"""

[[steps.content.code_blocks]]
language = "yaml"
label = "Proxy"
tab = "proxy"
content = """
# config.yaml
model_list:
- model_name: gpt-4o-mini
  litellm_params:
    model: gpt-4o-mini

litellm_settings:
  success_callback: ["posthog"]
  failure_callback: ["posthog"]  # Optional: also log failures

environment_variables:
  POSTHOG_API_KEY: "<ph_project_api_key>"
  POSTHOG_API_URL: "<ph_client_api_host>"  # Optional
"""

# Step 3: Call LLMs through LiteLLM
[[steps]]
title = "Call LLMs through LiteLLM"
badge = "required"

[[steps.content]]
type = "markdown"
content = """
Now, when you use LiteLLM to call various LLM providers, PostHog automatically captures an `$ai_generation` event.
"""

[[steps.content]]
type = "multi_code"
languages = ["sdk", "proxy"]

[[steps.content.code_blocks]]
language = "python"
label = "SDK"
tab = "sdk"
content = """
response = litellm.completion(
    model="gpt-4o-mini",
    messages=[
        {"role": "user", "content": "Tell me a fun fact about hedgehogs"}
    ],
    metadata={
        "user_id": "user_123",  # Maps to PostHog distinct_id
        "company": "company_id_in_your_db"  # Custom property
    }
)

print(response.choices[0].message.content)
"""

[[steps.content.code_blocks]]
language = "bash"
label = "Proxy"
tab = "proxy"
content = """
# Start the proxy (if not already running)
litellm --config config.yaml

# Make a request to the proxy
curl -X POST http://localhost:4000/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o-mini",
    "messages": [
      {"role": "user", "content": "Tell me a fun fact about hedgehogs"}
    ],
    "metadata": {
      "user_id": "user_123",
      "company": "company_id_in_your_db"
    }
  }'
"""

[[steps.content]]
type = "note"
content = """
**Notes:**
- This works with streaming responses by setting `stream=True`.
- To disable logging for specific requests, add `{"no-log": true}` to metadata.
- If you want to capture LLM events anonymously, **don't** pass a `user_id` in metadata. See our docs on [anonymous vs identified events](/docs/data/anonymous-vs-identified-events) to learn more.
"""

[[steps.content]]
type = "markdown"
content = """
You can expect captured `$ai_generation` events to have the following properties:
"""

[[steps.content]]
type = "table"
headers = ["Property", "Description"]
rows = [
    ["$ai_model", "The specific model, like `gpt-5-mini` or `claude-4-sonnet`"],
    ["$ai_latency", "The latency of the LLM call in seconds"],
    ["$ai_tools", "Tools and functions available to the LLM"],
    ["$ai_input", "List of messages sent to the LLM"],
    ["$ai_input_tokens", "The number of tokens in the input (often found in response.usage)"],
    ["$ai_output_choices", "List of response choices from the LLM"],
    ["$ai_output_tokens", "The number of tokens in the output (often found in `response.usage`)"],
    ["$ai_total_cost_usd", "The total cost in USD (input + output)"],
    ["[...]", "See [full list](/docs/llm-analytics/generations#event-properties) of properties"]
]

# Step 4: Verify traces and generations (checkpoint)
[[steps]]
title = "Verify traces and generations"
subtitle = "Confirm LLM events are being sent to PostHog"
type = "checkpoint"
badge = "checkpoint"

# Docs-specific content
[[steps.content.docs]]
type = "markdown"
content = """
Let's make sure LLM events are being captured and sent to PostHog. Under **LLM analytics**, you should see rows of data appear in the **Traces** and **Generations** tabs.
"""

[[steps.content.docs]]
type = "screenshot"
imageLight = "https://res.cloudinary.com/dmukukwp6/image/upload/SCR_20250807_syne_ecd0801880.png"
imageDark = "https://res.cloudinary.com/dmukukwp6/image/upload/SCR_20250807_syjm_5baab36590.png"
alt = "LLM generations in PostHog"
caption = "LLM generations in PostHog"

[[steps.content.docs]]
type = "button"
variant = "secondary"
size = "sm"
url = "https://app.posthog.com/llm-analytics/generations"
external = true
text = "Check for LLM events in PostHog"

# App-specific content
[[steps.content.app]]
type = "markdown"
content = """
Make some LLM calls, PostHog should now receive your LLM traces. Once your captures are ingested, you'll see the `Waiting for events` widget below update in realtime.

If PostHog receives LLM traces, the widget below will say `Installation Complete`.
"""

# Step 5: Capture embeddings (optional)
[[steps]]
title = "Capture embeddings"
badge = "optional"

[[steps.content]]
type = "markdown"
content = """
PostHog can also capture embedding generations as `$ai_embedding` events through LiteLLM:
"""

[[steps.content]]
type = "multi_code"
languages = ["sdk", "proxy"]

[[steps.content.code_blocks]]
language = "python"
label = "SDK"
tab = "sdk"
content = """
response = litellm.embedding(
    input="The quick brown fox",
    model="text-embedding-3-small",
    metadata={
        "user_id": "user_123",  # Maps to PostHog distinct_id
        "company": "company_id_in_your_db"  # Custom property
    }
)
"""

[[steps.content.code_blocks]]
language = "bash"
label = "Proxy"
tab = "proxy"
content = """
# Make an embeddings request to the proxy
curl -X POST http://localhost:4000/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "input": "The quick brown fox",
    "model": "text-embedding-3-small",
    "metadata": {
      "user_id": "user_123",
      "company": "company_id_in_your_db"
    }
  }'
"""