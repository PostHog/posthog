{
    "description": "Step-by-step guide to install PostHog SDK for LiteLLM analytics",
    "product": "llm_analytics",
    "sdk": ["python", "proxy"],
    "title": "LiteLLM analytics installation",
    "steps": [
        {
            "badge": "required",
            "title": "Install LiteLLM",
            "content": [
                {
                    "content": "**Note:** LiteLLM can be used as a Python SDK or as a proxy server. PostHog observability requires LiteLLM version 1.77.3 or higher.\n",
                    "type": "note"
                },
                {
                    "content": "Choose your installation method based on how you want to use LiteLLM:\n",
                    "type": "markdown"
                },
                {
                    "languages": ["sdk", "proxy"],
                    "type": "multi_code",
                    "code_blocks": [
                        {
                            "content": "pip install litellm\n",
                            "label": "SDK",
                            "language": "bash",
                            "tab": "sdk"
                        },
                        {
                            "content": "# Install via pip\npip install 'litellm[proxy]'\n\n# Or run via Docker\ndocker run --rm -p 4000:4000 ghcr.io/berriai/litellm:latest\n",
                            "label": "Proxy",
                            "language": "bash",
                            "tab": "proxy"
                        }
                    ]
                }
            ]
        },
        {
            "badge": "required",
            "title": "Configure PostHog observability",
            "content": [
                {
                    "content": "Configure PostHog by setting your project API key and host as well as adding `posthog` to your LiteLLM callback handlers. You can find your API key in [your project settings](https://app.posthog.com/settings/project).\n",
                    "type": "markdown"
                },
                {
                    "languages": ["sdk", "proxy"],
                    "type": "multi_code",
                    "code_blocks": [
                        {
                            "content": "import os\nimport litellm\n\n# Set environment variables\nos.environ[\"POSTHOG_API_KEY\"] = \"<ph_project_api_key>\"\nos.environ[\"POSTHOG_API_URL\"] = \"<ph_client_api_host>\"  # Optional, defaults to https://app.posthog.com\n\n# Enable PostHog callbacks\nlitellm.success_callback = [\"posthog\"]\nlitellm.failure_callback = [\"posthog\"]  # Optional: also log failures\n",
                            "label": "SDK",
                            "language": "python",
                            "tab": "sdk"
                        },
                        {
                            "content": "# config.yaml\nmodel_list:\n- model_name: gpt-4o-mini\n  litellm_params:\n    model: gpt-4o-mini\n\nlitellm_settings:\n  success_callback: [\"posthog\"]\n  failure_callback: [\"posthog\"]  # Optional: also log failures\n\nenvironment_variables:\n  POSTHOG_API_KEY: \"<ph_project_api_key>\"\n  POSTHOG_API_URL: \"<ph_client_api_host>\"  # Optional\n",
                            "label": "Proxy",
                            "language": "yaml",
                            "tab": "proxy"
                        }
                    ]
                }
            ]
        },
        {
            "badge": "required",
            "title": "Call LLMs through LiteLLM",
            "content": [
                {
                    "content": "Now, when you use LiteLLM to call various LLM providers, PostHog automatically captures an `$ai_generation` event.\n",
                    "type": "markdown"
                },
                {
                    "languages": ["sdk", "proxy"],
                    "type": "multi_code",
                    "code_blocks": [
                        {
                            "content": "response = litellm.completion(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Tell me a fun fact about hedgehogs\"}\n    ],\n    metadata={\n        \"user_id\": \"user_123\",  # Maps to PostHog distinct_id\n        \"company\": \"company_id_in_your_db\"  # Custom property\n    }\n)\n\nprint(response.choices[0].message.content)\n",
                            "label": "SDK",
                            "language": "python",
                            "tab": "sdk"
                        },
                        {
                            "content": "# Start the proxy (if not already running)\nlitellm --config config.yaml\n\n# Make a request to the proxy\ncurl -X POST http://localhost:4000/chat/completions -H \"Content-Type: application/json\" -d '{\n    \"model\": \"gpt-4o-mini\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"Tell me a fun fact about hedgehogs\"}\n    ],\n    \"metadata\": {\n      \"user_id\": \"user_123\",\n      \"company\": \"company_id_in_your_db\"\n    }\n  }'\n",
                            "label": "Proxy",
                            "language": "bash",
                            "tab": "proxy"
                        }
                    ]
                },
                {
                    "content": "**Notes:**\n- This works with streaming responses by setting `stream=True`.\n- To disable logging for specific requests, add `{\"no-log\": true}` to metadata.\n- If you want to capture LLM events anonymously, **don't** pass a `user_id` in metadata. See our docs on [anonymous vs identified events](/docs/data/anonymous-vs-identified-events) to learn more.\n",
                    "type": "note"
                },
                {
                    "content": "You can expect captured `$ai_generation` events to have the following properties:\n",
                    "type": "markdown"
                },
                {
                    "headers": ["Property", "Description"],
                    "rows": [
                        ["$ai_model", "The specific model, like `gpt-5-mini` or `claude-4-sonnet`"],
                        ["$ai_latency", "The latency of the LLM call in seconds"],
                        ["$ai_tools", "Tools and functions available to the LLM"],
                        ["$ai_input", "List of messages sent to the LLM"],
                        ["$ai_input_tokens", "The number of tokens in the input (often found in response.usage)"],
                        ["$ai_output_choices", "List of response choices from the LLM"],
                        ["$ai_output_tokens", "The number of tokens in the output (often found in `response.usage`)"],
                        ["$ai_total_cost_usd", "The total cost in USD (input + output)"],
                        ["[...]", "See [full list](/docs/llm-analytics/generations#event-properties) of properties"]
                    ],
                    "type": "table"
                }
            ]
        },
        {
            "badge": "checkpoint",
            "subtitle": "Confirm LLM events are being sent to PostHog",
            "title": "Verify traces and generations",
            "type": "checkpoint",
            "content": {
                "app": [
                    {
                        "content": "Make some LLM calls, PostHog should now receive your LLM traces. Once your captures are ingested, you'll see the `Waiting for events` widget below update in realtime.\n\nIf PostHog receives LLM traces, the widget below will say `Installation Complete`.\n",
                        "type": "markdown"
                    }
                ],
                "docs": [
                    {
                        "content": "Let's make sure LLM events are being captured and sent to PostHog. Under **LLM analytics**, you should see rows of data appear in the **Traces** and **Generations** tabs.\n",
                        "type": "markdown"
                    },
                    {
                        "alt": "LLM generations in PostHog",
                        "caption": "LLM generations in PostHog",
                        "imageDark": "https://res.cloudinary.com/dmukukwp6/image/upload/SCR_20250807_syjm_5baab36590.png",
                        "imageLight": "https://res.cloudinary.com/dmukukwp6/image/upload/SCR_20250807_syne_ecd0801880.png",
                        "type": "screenshot"
                    },
                    {
                        "external": true,
                        "size": "sm",
                        "text": "Check for LLM events in PostHog",
                        "type": "button",
                        "url": "https://app.posthog.com/llm-analytics/generations",
                        "variant": "secondary"
                    }
                ]
            }
        },
        {
            "badge": "optional",
            "title": "Capture embeddings",
            "content": [
                {
                    "content": "PostHog can also capture embedding generations as `$ai_embedding` events through LiteLLM:\n",
                    "type": "markdown"
                },
                {
                    "languages": ["sdk", "proxy"],
                    "type": "multi_code",
                    "code_blocks": [
                        {
                            "content": "response = litellm.embedding(\n    input=\"The quick brown fox\",\n    model=\"text-embedding-3-small\",\n    metadata={\n        \"user_id\": \"user_123\",  # Maps to PostHog distinct_id\n        \"company\": \"company_id_in_your_db\"  # Custom property\n    }\n)\n",
                            "label": "SDK",
                            "language": "python",
                            "tab": "sdk"
                        },
                        {
                            "content": "# Make an embeddings request to the proxy\ncurl -X POST http://localhost:4000/embeddings -H \"Content-Type: application/json\" -d '{\n    \"input\": \"The quick brown fox\",\n    \"model\": \"text-embedding-3-small\",\n    \"metadata\": {\n      \"user_id\": \"user_123\",\n      \"company\": \"company_id_in_your_db\"\n    }\n  }'\n",
                            "label": "Proxy",
                            "language": "bash",
                            "tab": "proxy"
                        }
                    ]
                }
            ]
        }
    ]
}
