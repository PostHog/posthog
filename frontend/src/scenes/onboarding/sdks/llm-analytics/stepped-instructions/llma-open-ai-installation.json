{
    "description": "Step-by-step guide to install PostHog SDK for OpenAI LLM analytics",
    "product": "llm_analytics",
    "sdk": ["python", "node"],
    "title": "OpenAI LLM analytics installation",
    "steps": [
        {
            "badge": "required",
            "title": "Install the PostHog SDK",
            "content": [
                {
                    "content": "Setting up analytics starts with installing the PostHog SDK for your language. LLM analytics works best with our Python and Node SDKs.\n",
                    "type": "markdown"
                },
                {
                    "languages": ["python", "node"],
                    "type": "multi_code",
                    "code_blocks": [
                        {
                            "content": "pip install posthog\n",
                            "label": "Python",
                            "language": "bash",
                            "tab": "python"
                        },
                        {
                            "content": "npm install @posthog/ai posthog-node\n",
                            "label": "Node",
                            "language": "bash",
                            "tab": "node"
                        }
                    ]
                }
            ]
        },
        {
            "badge": "required",
            "title": "Install the OpenAI SDK",
            "content": [
                {
                    "content": "Install the OpenAI SDK:\n",
                    "type": "markdown"
                },
                {
                    "languages": ["python", "node"],
                    "type": "multi_code",
                    "code_blocks": [
                        {
                            "content": "pip install openai\n",
                            "label": "Python",
                            "language": "bash",
                            "tab": "python"
                        },
                        {
                            "content": "npm install openai\n",
                            "label": "Node",
                            "language": "bash",
                            "tab": "node"
                        }
                    ]
                }
            ]
        },
        {
            "badge": "required",
            "title": "Initialize PostHog and OpenAI client",
            "content": [
                {
                    "content": "Initialize PostHog with your project API key and host from [your project settings](https://app.posthog.com/settings/project), then pass it to our OpenAI wrapper.\n",
                    "type": "markdown"
                },
                {
                    "languages": ["python", "node"],
                    "type": "multi_code",
                    "code_blocks": [
                        {
                            "content": "from posthog.ai.openai import OpenAI\nfrom posthog import Posthog\n\nposthog = Posthog(\n    \"<ph_project_api_key>\",\n    host=\"<ph_client_api_host>\"\n)\n\nclient = OpenAI(\n    api_key=\"your_openai_api_key\",\n    posthog_client=posthog # This is an optional parameter. If it is not provided, a default client will be used.\n)\n",
                            "label": "Python",
                            "language": "python",
                            "tab": "python"
                        },
                        {
                            "content": "import { OpenAI } from '@posthog/ai'\nimport { PostHog } from 'posthog-node'\n\nconst phClient = new PostHog(\n  '<ph_project_api_key>',\n  { host: '<ph_client_api_host>' }\n);\n\nconst openai = new OpenAI({\n  apiKey: 'your_openai_api_key',\n  posthog: phClient,\n});\n\n// ... your code here ...\n\n// IMPORTANT: Shutdown the client when you're done to ensure all events are sent\nphClient.shutdown()\n",
                            "label": "Node",
                            "language": "typescript",
                            "tab": "node"
                        }
                    ]
                },
                {
                    "content": "**Note:** This also works with the `AsyncOpenAI` client.\n",
                    "type": "note"
                },
                {
                    "callout_type": "fyi",
                    "content": "These SDKs **do not** proxy your calls. They only fire off an async call to PostHog in the background to send the data.\n\nYou can also use LLM analytics with other SDKs or our API, but you will need to capture the data in the right format. See the schema in the [manual capture section](/docs/llm-analytics/installation/manual-capture) for more details.\n",
                    "icon": "IconInfo",
                    "title": "Proxy note",
                    "type": "callout"
                }
            ]
        },
        {
            "badge": "required",
            "title": "Call OpenAI LLMs",
            "content": [
                {
                    "content": "Now, when you use the OpenAI SDK to call LLMs, PostHog automatically captures an `$ai_generation` event.\n\nYou can enrich the event with additional data such as the trace ID, distinct ID, custom properties, groups, and privacy mode options.\n",
                    "type": "markdown"
                },
                {
                    "languages": ["python", "node"],
                    "type": "multi_code",
                    "code_blocks": [
                        {
                            "content": "response = client.responses.create(\n    model=\"gpt-4o-mini\",\n    input=[\n        {\"role\": \"user\", \"content\": \"Tell me a fun fact about hedgehogs\"}\n    ],\n    posthog_distinct_id=\"user_123\", # optional\n    posthog_trace_id=\"trace_123\", # optional\n    posthog_properties={\"conversation_id\": \"abc123\", \"paid\": True}, # optional\n    posthog_groups={\"company\": \"company_id_in_your_db\"},  # optional \n    posthog_privacy_mode=False # optional\n)\n\nprint(response.choices[0].message.content)\n",
                            "label": "Python",
                            "language": "python",
                            "tab": "python"
                        },
                        {
                            "content": "const completion = await openai.responses.create({\n    model: \"gpt-4o-mini\",\n    input: [{ role: \"user\", content: \"Tell me a fun fact about hedgehogs\" }],\n    posthogDistinctId: \"user_123\", // optional\n    posthogTraceId: \"trace_123\", // optional\n    posthogProperties: { conversation_id: \"abc123\", paid: true }, // optional\n    posthogGroups: { company: \"company_id_in_your_db\" }, // optional \n    posthogPrivacyMode: false // optional\n});\n\nconsole.log(completion.choices[0].message.content)\n",
                            "label": "Node",
                            "language": "typescript",
                            "tab": "node"
                        }
                    ]
                },
                {
                    "content": "**Notes:**\n- We also support the old `chat.completions` API.\n- This works with responses where `stream=True`.\n- If you want to capture LLM events anonymously, **don't** pass a distinct ID to the request. See our docs on [anonymous vs identified events](/docs/data/anonymous-vs-identified-events) to learn more.\n",
                    "type": "notes"
                },
                {
                    "content": "You can expect captured `$ai_generation` events to have the following properties:\n",
                    "type": "markdown"
                },
                {
                    "headers": ["Property", "Description"],
                    "rows": [
                        ["$ai_model", "The specific model, like `gpt-5-mini` or `claude-4-sonnet`"],
                        ["$ai_latency", "The latency of the LLM call in seconds"],
                        ["$ai_tools", "Tools and functions available to the LLM"],
                        ["$ai_input", "List of messages sent to the LLM"],
                        ["$ai_input_tokens", "The number of tokens in the input (often found in response.usage)"],
                        ["$ai_output_choices", "List of response choices from the LLM"],
                        ["$ai_output_tokens", "The number of tokens in the output (often found in `response.usage`)"],
                        ["$ai_total_cost_usd", "The total cost in USD (input + output)"],
                        ["[...]", "See [full list](/docs/llm-analytics/generations#event-properties) of properties"]
                    ],
                    "type": "table"
                }
            ]
        },
        {
            "badge": "checkpoint",
            "subtitle": "Confirm LLM events are being sent to PostHog",
            "title": "Verify traces and generations",
            "type": "checkpoint",
            "content": {
                "app": [
                    {
                        "content": "Make some LLM calls, PostHog should now receive your LLM traces.  \n\nIf PostHog receives LLM traces, the widget below will say `Installation Complete`.\n",
                        "type": "markdown"
                    }
                ],
                "docs": [
                    {
                        "content": "Let's make sure LLM events are being captured and sent to PostHog. Under **LLM analytics**, you should see rows of data appear in the **Traces** and **Generations** tabs.\n",
                        "type": "markdown"
                    },
                    {
                        "alt": "LLM generations in PostHog",
                        "caption": "LLM generations in PostHog",
                        "imageDark": "https://res.cloudinary.com/dmukukwp6/image/upload/SCR_20250807_syjm_5baab36590.png",
                        "imageLight": "https://res.cloudinary.com/dmukukwp6/image/upload/SCR_20250807_syne_ecd0801880.png",
                        "type": "screenshot"
                    },
                    {
                        "external": true,
                        "size": "sm",
                        "text": "Check for LLM events in PostHog",
                        "type": "button",
                        "url": "https://app.posthog.com/llm-analytics/generations",
                        "variant": "secondary"
                    }
                ]
            }
        },
        {
            "badge": "optional",
            "title": "Capture embeddings",
            "content": {
                "docs": [
                    {
                        "content": "PostHog can also capture embedding generations as `$ai_embedding` events. Just make sure to use the same `posthog.ai.openai` client to do so:\n",
                        "type": "markdown"
                    },
                    {
                        "languages": ["python"],
                        "type": "multi_code",
                        "code_blocks": [
                            {
                                "content": "response = client.embeddings.create(\n    input=\"The quick brown fox\",\n    model=\"text-embedding-3-small\",\n    posthog_distinct_id=\"user_123\", # optional\n    posthog_trace_id=\"trace_123\",   # optional\n    posthog_properties={\"key\": \"value\"} # optional\n    posthog_groups={\"company\": \"company_id_in_your_db\"}  # optional \n    posthog_privacy_mode=False # optional\n)\n",
                                "label": "Python",
                                "language": "python",
                                "tab": "python"
                            }
                        ]
                    }
                ]
            }
        }
    ]
}
