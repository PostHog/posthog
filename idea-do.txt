How env works in GitHub Actions (important nuance)
Inside a single job:
	•	Within one step (one run: or python script.py): normal shell/Python env applies; child processes inherit env.
	•	Between two separate steps, normal export FOO=bar in step 1 does not carry over to step 2.
To move env between steps, you must either:
	1	Write to $GITHUB_ENV → becomes env for all later steps of that job.
	2	Or write to $GITHUB_OUTPUT → expose as “step outputs” (then reference via steps.<id>.outputs.*).
	3	Or write to a file and source/read it in later steps.
So your two-scripts idea is totally fine, just make sure anything needed in script 2 (droplet IP, SSH key, etc.) is either:
	•	passed via GITHUB_ENV / GITHUB_OUTPUT, or
	•	re-read from a file created by script 1.

2. Pattern: two Python scripts, env and shell commands
Let’s say you have:
	•	ci/setup_droplet.py → creates droplet, generates ephemeral SSH key, writes env.
	•	ci/run_tests_and_logs.py → SSH into droplet, run tests, collect logs.
Step 1: Setup (Python)
Workflow:
jobs:
  e2e:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Setup droplet (create, keygen, persist env)
        env:
          DO_API_TOKEN: ${{ secrets.DO_API_TOKEN }}
          GITHUB_ENV: ${{ env.GITHUB_ENV }}  # passed implicitly, but I like being explicit
        run: |
          python ci/setup_droplet.py

      - name: Run tests and collect logs
        run: |
          python ci/run_tests_and_logs.py

      - name: Upload logs artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: droplet-logs
          path: docker-logs.txt
ci/setup_droplet.py (sketch)
import os
import subprocess
from pydo import Client  # or your adapter

do_token = os.environ["DO_API_TOKEN"]
client = Client(token=do_token)

# 1) Generate ephemeral SSH keypair
ssh_dir = os.path.expanduser("~/.ssh")
os.makedirs(ssh_dir, exist_ok=True)
key_path = os.path.join(ssh_dir, "ci_key")

subprocess.run(
    ["ssh-keygen", "-t", "ed25519", "-f", key_path, "-N", ""],
    check=True,
)

with open(key_path) as f:
    private_key = f.read()
with open(key_path + ".pub") as f:
    public_key = f.read().strip()

# 2) Create droplet with cloud-init user_data containing the pubkey
user_data = f"""#cloud-config
users:
  - name: root
    ssh-authorized-keys:
      - {public_key}
"""

droplet = client.droplets.create(body={
    "name": "ci-test-droplet",
    "region": "fra1",
    "size": "s-1vcpu-1gb",
    "image": "ubuntu-22-04-x64",
    "user_data": user_data,
})
droplet_id = droplet["droplet"]["id"]

# TODO: poll until droplet has an IP
# ... get ip ...
droplet_ip = "1.2.3.4"  # replace with real lookup

# 3) Persist values for later steps using GITHUB_ENV
github_env = os.environ["GITHUB_ENV"]
with open(github_env, "a") as f:
    f.write(f"DROPLET_ID={droplet_id}\n")
    f.write(f"DROPLET_IP={droplet_ip}\n")
    # multiline env var for private key
    f.write("CI_SSH_PRIVATE_KEY<<EOF\n")
    f.write(private_key)
    f.write("\nEOF\n")
That’s the key bit: writing to GITHUB_ENV from Python so that step 2 sees DROPLET_IP, CI_SSH_PRIVATE_KEY, etc.
Step 2: Use env in the second script
ci/run_tests_and_logs.py:
import os
import subprocess

droplet_ip = os.environ["DROPLET_IP"]
private_key = os.environ["CI_SSH_PRIVATE_KEY"]

ssh_dir = os.path.expanduser("~/.ssh")
os.makedirs(ssh_dir, exist_ok=True)
key_path = os.path.join(ssh_dir, "ci_key")
with open(key_path, "w") as f:
    f.write(private_key)
os.chmod(key_path, 0o600)

# Create SSH config for convenience (optional)
config_path = os.path.join(ssh_dir, "config")
with open(config_path, "a") as f:
    f.write(f"Host do-ci\n  HostName {droplet_ip}\n  User root\n  IdentityFile {key_path}\n  StrictHostKeyChecking no\n")

# Run tests on droplet
subprocess.run(
    ["ssh", "do-ci", "cd /app && docker compose up --build --abort-on-container-exit"],
    check=False,  # let it fail but continue
)

# Collect logs
subprocess.run(
    ["ssh", "do-ci", "cd /app && docker compose logs --no-color > /tmp/docker-logs.txt"],
    check=True,
)
subprocess.run(
    ["scp", "do-ci:/tmp/docker-logs.txt", "./docker-logs.txt"],
    check=True,
)
You can equally do all of this via pure Python (e.g. paramiko) instead of shelling out to ssh / scp, but conceptually it’s the same: the env from step 1 is now just os.environ in step 2.

3. Security & env passing recap
	•	Yes, you can “just pass around the env”:
	◦	Within one script: normal env.
	◦	Between scripts in the same step: they inherit from the same shell.
	◦	Between steps in the job: use $GITHUB_ENV (as above) or job outputs.
	•	Doing the work “in Python” vs “in shell” is mostly a taste/maintainability choice:
	◦	From Python you can invoke shell commands (ssh-keygen, ssh, scp) with subprocess,
	◦	or use libraries (DO SDK, paramiko, etc.) if you want fewer external binaries.
The pattern I’d recommend for you:
	•	Keep the GH YAML thin.
	•	Let Python orchestrate everything (DO API, keygen, user_data, SSH, log collection).
	•	Use GITHUB_ENV only as a bridge if you really want two separate scripts/steps; otherwise you can even do it in a single Python script in one run step.
