# This workflow runs all of our backend django tests.
#
# If these tests get too slow, look at increasing concurrency and re-timing the tests by manually dispatching
# .github/workflows/ci-backend-update-test-timing.yml action
name: Backend CI
on:
    push:
        branches:
            - master
    workflow_dispatch:
        inputs:
            clickhouseServerVersion:
                description: ClickHouse server version. Leave blank for default
                type: string
    pull_request:
    merge_group:

concurrency:
    group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
    cancel-in-progress: ${{ github.event_name == 'pull_request' }}
env:
    SECRET_KEY: '6b01eee4f945ca25045b5aab440b953461faf08693a9abbf1166dc7c6b9772da' # unsafe - for testing only
    DATABASE_URL: 'postgres://posthog:posthog@localhost:5432/posthog'
    REDIS_URL: 'redis://localhost'
    CLICKHOUSE_HOST: 'localhost'
    CLICKHOUSE_SECURE: 'False'
    CLICKHOUSE_VERIFY: 'False'
    TEST: 1
    CLICKHOUSE_SERVER_IMAGE_VERSION: ${{ github.event.inputs.clickhouseServerVersion || '' }}
    OBJECT_STORAGE_ENABLED: 'True'
    OBJECT_STORAGE_ENDPOINT: 'http://localhost:19000'
    OBJECT_STORAGE_ACCESS_KEY_ID: 'object_storage_root_user'
    OBJECT_STORAGE_SECRET_ACCESS_KEY: 'object_storage_root_password'
    # tests would intermittently fail in GH actions
    # with exit code 134 _after passing_ all tests
    # this appears to fix it
    # absolute wild tbh https://stackoverflow.com/a/75503402
    DISPLAY: ':99.0'
    # this is a fake key so this workflow can run for external contributors as they do not have access to secrets (that we don't need here)
    OIDC_RSA_PRIVATE_KEY: ${{ vars.OIDC_RSA_FAKE_PRIVATE_KEY }}
    RUNS_ON_INTERNAL_PR: ${{ github.event_name != 'pull_request' || github.event.pull_request.head.repo.fork == false }}

permissions:
    contents: write
    pull-requests: write

jobs:
    # Job to decide if we should run backend ci
    # See https://github.com/dorny/paths-filter#conditional-execution for more details
    changes:
        runs-on: ubuntu-latest
        timeout-minutes: 5
        name: Determine need to run backend and migration checks
        # Set job outputs to values from filter step
        outputs:
            backend: ${{ steps.filter.outputs.backend || 'true' }}
            backend_files: ${{ steps.filter.outputs.backend_files }}
            migrations: ${{ steps.filter.outputs.migrations || 'true' }}
            migrations_files: ${{ steps.filter.outputs.migrations_files }}
            tasks_temporal: ${{ steps.filter.outputs.tasks_temporal || 'true' }}
            llm_gateway: ${{ steps.filter.outputs.llm_gateway || 'true' }}
            openapi_types: ${{ steps.filter.outputs.openapi_types || 'true' }}
        steps:
            # For pull requests it's not necessary to checkout the code, but we
            # also want this to run on master so we need to checkout
            - uses: actions/checkout@v6
              with:
                  clean: false
            - name: Clean up data directories with container permissions
              run: |
                  # Use docker to clean up files created by containers
                  [ -d "data" ] && docker run --rm -v "$(pwd)/data:/data" alpine sh -c "rm -rf /data/seaweedfs /data/minio" || true
              continue-on-error: true

            - uses: dorny/paths-filter@de90cc6fb38fc0963ad72b210f1f284cd68cea36 # v3.0.2
              id: filter
              if: github.event_name != 'push' # Run all tests on master push
              with:
                  list-files: 'escape'
                  filters: |
                      backend:
                        # Avoid running backend tests for irrelevant changes
                        # NOTE: we are at risk of missing a dependency here. We could make
                        # the dependencies more clear if we separated the backend/frontend
                        # code completely
                        # really we should ignore ee/frontend/** but dorny doesn't support that
                        # - '!ee/frontend/**'
                        # including the negated rule appears to work
                        # but makes it always match because the checked file always isn't `ee/frontend/**` ðŸ™ˆ
                        - 'ee/**/*'
                        - 'common/hogvm/**/*'
                        - 'posthog/**/*'
                        - 'products/**/backend/**/*'
                        - 'bin/*.py'
                        - pyproject.toml
                        - uv.lock
                        - requirements.txt
                        - requirements-dev.txt
                        - mypy.ini
                        - pytest.ini
                        - .test_durations # Used for pytest-split sharding
                        - frontend/src/queries/schema.json # Used for generating schema.py
                        - common/plugin_transpiler/src # Used for transpiling plugins
                        # Make sure we run if someone is explicitly changing the workflow
                        - .github/workflows/ci-backend.yml
                        # We use docker compose for tests, make sure we rerun on
                        # changes to docker-compose.dev.yml e.g. dependency
                        # version changes
                        - docker-compose.dev.yml
                        - docker-compose.base.yml
                        - frontend/public/email/*
                        - docker/clickhouse
                        # These scripts are used in the CI
                        - bin/check_temporal_up
                        - bin/check_kafka_clickhouse_up
                      migrations:
                        - docker/clickhouse
                        - 'posthog/migrations/*.py'
                        - 'products/*/backend/migrations/*.py'
                        - 'products/*/migrations/*.py'  # Legacy structure
                        - 'rust/persons_migrations/*.sql'
                        - 'rust/bin/migrate-persons'
                      tasks_temporal:
                        - 'products/tasks/backend/temporal/**/*'
                      llm_gateway:
                        - 'services/llm-gateway/**/*'
                      openapi_types:
                        # Generated OpenAPI types - validate they match schema
                        - 'frontend/src/generated/**/*'
                        - 'products/*/frontend/generated/**/*'
                        # Generation tooling - changes here could affect output
                        - 'frontend/bin/generate-openapi-types.mjs'
                        - 'frontend/src/lib/api-orval-mutator.ts'
                        - 'frontend/src/lib/api-stub-for-orval.ts'

    detect-snapshot-mode:
        name: Detect snapshot mode
        runs-on: ubuntu-latest
        needs: [changes]
        if: needs.changes.outputs.backend == 'true'
        outputs:
            mode: ${{ steps.detect.outputs.mode }}
        steps:
            - name: Detect mode
              id: detect
              run: |
                  if [ "${{ github.event.pull_request.head.repo.full_name }}" != "${{ github.repository }}" ]; then
                    echo "mode=check" >> $GITHUB_OUTPUT
                    echo "Fork detected - running in CHECK mode (no commits allowed)"
                  else
                    AUTHOR="${{ github.actor }}"
                    echo "Workflow triggered by: $AUTHOR"
                    if [[ "$AUTHOR" == *"github-actions"* ]] || [[ "$AUTHOR" == *"[bot]"* ]] || [[ "$AUTHOR" == "posthog-bot" ]]; then
                      echo "mode=check" >> $GITHUB_OUTPUT
                      echo "::notice::ðŸ” Running in CHECK mode - snapshots must match exactly"
                    else
                      echo "mode=update" >> $GITHUB_OUTPUT
                      echo "::notice::ðŸ”„ Running in UPDATE mode - snapshots can be updated"
                    fi
                  fi

    check-migrations:
        needs: [changes]
        if: needs.changes.outputs.backend == 'true' || needs.changes.outputs.openapi_types == 'true'
        timeout-minutes: 20

        name: Validate migrations and OpenAPI types
        runs-on: depot-ubuntu-latest

        steps:
            - uses: actions/checkout@v6
              with:
                  clean: false
            - name: Clean up data directories with container permissions
              run: |
                  # Use docker to clean up files created by containers
                  [ -d "data" ] && docker run --rm -v "$(pwd)/data:/data" alpine sh -c "rm -rf /data/seaweedfs /data/minio" || true
              continue-on-error: true

            - name: Stop/Start stack with Docker Compose
              run: |
                  docker compose -f docker-compose.dev.yml down
                  docker compose -f docker-compose.dev.yml up -d &

            - name: Set up Python
              uses: actions/setup-python@83679a892e2d95755f2dac6acb0bfd1e9ac5d548 # v6.1.0
              with:
                  python-version: 3.12.12

            - name: Install uv
              id: setup-uv
              uses: astral-sh/setup-uv@61cb8a9741eeb8a550a1b8544337180c0fc8476b # v7.2.0
              with:
                  enable-cache: true
                  version: 0.9.9

            - name: Install SAML (python3-saml) dependencies
              if: steps.setup-uv.outputs.cache-hit != 'true'
              run: |
                  sudo apt-get update
                  sudo apt-get install libxml2-dev libxmlsec1-dev libxmlsec1-openssl
            - name: Install Rust
              uses: dtolnay/rust-toolchain@0b1efabc08b657293548b77fb76cc02d26091c7e
              with:
                  toolchain: 1.91.1
                  components: cargo

            - name: Cache Rust dependencies
              uses: Swatinem/rust-cache@779680da715d629ac1d338a641029a2f4372abb5 # v2.8.2

            - name: Install sqlx-cli
              run: |
                  cargo install sqlx-cli --version 0.8.0 --features postgres --no-default-features --locked

            # First running migrations from master, to simulate the real-world scenario
            - name: Checkout master
              uses: actions/checkout@v6
              with:
                  ref: master
                  clean: false

            - name: Install python dependencies for master
              run: |
                  UV_PROJECT_ENVIRONMENT=.venv-master uv sync --frozen --dev

            - name: Wait for services to be available
              run: |
                  bin/check_postgres_up
                  bin/check_kafka_clickhouse_up

            - name: Run migrations up to master
              run: |
                  # Run Django migrations first (excluding managed=False models)
                  .venv-master/bin/python manage.py migrate
                  # Then run persons migrations using sqlx; comment out until we've merged
                  # DATABASE_URL="postgres://posthog:posthog@localhost:5432/posthog_persons" \
                  #   sqlx database create
                  # DATABASE_URL="postgres://posthog:posthog@localhost:5432/posthog_persons" \
                  #   sqlx migrate run --source rust/persons_migrations/

            # Now we can consider this PR's migrations
            - name: Checkout this PR
              uses: actions/checkout@v6
              with:
                  clean: false

            - name: Install python dependencies for this PR
              run: |
                  UV_PROJECT_ENVIRONMENT=$pythonLocation uv sync --frozen --dev

            - name: Check migrations and post SQL comment
              if: github.event_name == 'pull_request' && needs.changes.outputs.migrations == 'true'
              env:
                  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
              run: |
                  # Read the changed files from the output
                  CHANGED_FILES="${{ needs.changes.outputs.migrations_files }}"

                  # If no migration files changed, exit
                  if [ -z "$CHANGED_FILES" ]; then
                    echo "No migration files changed"
                    exit 0
                  fi

                  # Initialize comment body for SQL changes
                  COMMENT_BODY="## Migration SQL Changes\n\nHey ðŸ‘‹, we've detected some migrations on this PR. Here's the SQL output for each migration, make sure they make sense:\n\n"

                  # Process each changed migration file (excluding Rust migrations)
                  for file in $CHANGED_FILES; do
                    # Skip Rust migrations as they're handled separately by sqlx
                    if [[ $file =~ rust/persons_migrations ]]; then
                      continue
                    fi
                    if [[ $file =~ migrations/([0-9]+)_ ]]; then
                      migration_number="${BASH_REMATCH[1]}"
                      # Get app name by looking at the directory structure
                      # For new structure products/user_interviews/backend/migrations, we want user_interviews
                      # For old structure products/user_interviews/migrations, we want user_interviews
                      if [[ $file =~ products/([^/]+)/backend/migrations/ ]]; then
                        app_name="${BASH_REMATCH[1]}"
                      else
                        app_name=$(echo $file | sed -E 's|^([^/]+/)*([^/]+)/migrations/.*|\2|')
                      fi
                        echo "Checking migration $migration_number for app $app_name"

                      # Get SQL output
                      SQL_OUTPUT=$(python manage.py sqlmigrate $app_name $migration_number)

                      # Add to comment body
                      COMMENT_BODY+="#### [\`$file\`](https:\/\/github.com\/${{ github.repository }}\/blob\/${{ github.sha }}\/$file)\n\`\`\`sql\n$SQL_OUTPUT\n\`\`\`\n\n"
                    fi
                  done

                  # Get existing comments
                  COMMENTS=$(curl -s -H "Authorization: token $GITHUB_TOKEN" \
                    -H "Accept: application/vnd.github.v3+json" \
                    "https://api.github.com/repos/${{ github.repository }}/issues/${{ github.event.pull_request.number }}/comments")

                  # Extract comment ID if exists
                  SQL_COMMENT_ID=$(echo "$COMMENTS" | jq -r '.[] | select(.body | startswith("## Migration SQL Changes")) | .id' | head -1)

                  # Add timestamp and commit SHA to SQL changes
                  TIMESTAMP=$(date -u '+%Y-%m-%d %H:%M UTC')
                  COMMIT_SHA="${{ github.event.pull_request.head.sha }}"
                  COMMIT_SHORT="${COMMIT_SHA:0:7}"
                  COMMENT_BODY+="\n*Last updated: $TIMESTAMP ([${COMMIT_SHORT}](https://github.com/${{ github.repository }}/commit/${COMMIT_SHA}))*"

                  # Convert \n into actual newlines
                  COMMENT_BODY=$(printf '%b' "$COMMENT_BODY")
                  COMMENT_BODY_JSON=$(jq -n --arg body "$COMMENT_BODY" '{body: $body}')

                  if [ -n "$SQL_COMMENT_ID" ]; then
                    # Update existing comment
                    echo "Updating existing SQL comment $SQL_COMMENT_ID"
                    curl -X PATCH \
                      -H "Authorization: token $GITHUB_TOKEN" \
                      -H "Accept: application/vnd.github.v3+json" \
                      "https://api.github.com/repos/${{ github.repository }}/issues/comments/$SQL_COMMENT_ID" \
                      -d "$COMMENT_BODY_JSON"
                  else
                    # Post new SQL comment to PR
                    echo "Posting new SQL comment to PR"
                    curl -X POST \
                      -H "Authorization: token $GITHUB_TOKEN" \
                      -H "Accept: application/vnd.github.v3+json" \
                      "https://api.github.com/repos/${{ github.repository }}/issues/${{ github.event.pull_request.number }}/comments" \
                      -d "$COMMENT_BODY_JSON"
                  fi

            - name: Run migration risk analysis and post comment
              if: github.event_name == 'pull_request'
              env:
                  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
              run: |
                  # Get risk analysis for all unapplied migrations (including third-party)
                  set +e  # Don't exit immediately on error
                  RISK_ANALYSIS=$(python manage.py analyze_migration_risk --fail-on-blocked 2>/dev/null)
                  EXIT_CODE=$?
                  set -e  # Re-enable exit on error

                  # Save analysis to file for artifact upload
                  if [ -n "$RISK_ANALYSIS" ]; then
                    echo "$RISK_ANALYSIS" > migration_analysis.md
                  fi

                  # Get existing comments
                  COMMENTS=$(curl -s -H "Authorization: token $GITHUB_TOKEN" \
                    -H "Accept: application/vnd.github.v3+json" \
                    "https://api.github.com/repos/${{ github.repository }}/issues/${{ github.event.pull_request.number }}/comments")

                  # Extract comment ID if exists
                  COMMENT_ID=$(echo "$COMMENTS" | jq -r '.[] | select(.body | startswith("## ðŸ” Migration Risk Analysis")) | .id' | head -1)

                  if [ -n "$RISK_ANALYSIS" ] && echo "$RISK_ANALYSIS" | grep -q "Summary:"; then
                    # Add timestamp and commit SHA to analysis
                    TIMESTAMP=$(date -u '+%Y-%m-%d %H:%M UTC')
                    COMMIT_SHA="${{ github.event.pull_request.head.sha }}"
                    COMMIT_SHORT="${COMMIT_SHA:0:7}"
                    RISK_COMMENT="## ðŸ” Migration Risk Analysis\n\nWe've analyzed your migrations for potential risks.\n\n$RISK_ANALYSIS\n\n*Last updated: $TIMESTAMP ([${COMMIT_SHORT}](https://github.com/${{ github.repository }}/commit/${COMMIT_SHA}))*"
                    RISK_COMMENT=$(printf '%b' "$RISK_COMMENT")
                    RISK_COMMENT_JSON=$(jq -n --arg body "$RISK_COMMENT" '{body: $body}')

                    if [ -n "$COMMENT_ID" ]; then
                      # Update existing comment
                      echo "Updating existing risk analysis comment $COMMENT_ID"
                      curl -X PATCH \
                        -H "Authorization: token $GITHUB_TOKEN" \
                        -H "Accept: application/vnd.github.v3+json" \
                        "https://api.github.com/repos/${{ github.repository }}/issues/comments/$COMMENT_ID" \
                        -d "$RISK_COMMENT_JSON"
                    else
                      # Create new comment if none exists
                      echo "Posting new risk analysis comment to PR"
                      curl -X POST \
                        -H "Authorization: token $GITHUB_TOKEN" \
                        -H "Accept: application/vnd.github.v3+json" \
                        "https://api.github.com/repos/${{ github.repository }}/issues/${{ github.event.pull_request.number }}/comments" \
                        -d "$RISK_COMMENT_JSON"
                    fi
                  elif [ -n "$COMMENT_ID" ]; then
                    # No migrations to analyze but comment exists - delete it
                    echo "Deleting risk analysis comment (no migrations to analyze)"
                    curl -X DELETE \
                      -H "Authorization: token $GITHUB_TOKEN" \
                      -H "Accept: application/vnd.github.v3+json" \
                      "https://api.github.com/repos/${{ github.repository }}/issues/comments/$COMMENT_ID"
                  else
                    echo "No migrations to analyze and no existing comment"
                  fi

                  # Fail the job if there were blocked migrations
                  if [ $EXIT_CODE -ne 0 ]; then
                    exit $EXIT_CODE
                  fi

            - name: Upload migration analysis artifact
              if: always() && github.event_name == 'pull_request'
              uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6.0.0
              with:
                  name: migration-analysis
                  path: migration_analysis.md
                  if-no-files-found: ignore

            - name: Run migrations for this PR
              run: |
                  # Run Django migrations first (excluding managed=False models)
                  python manage.py migrate
                  # Then run persons migrations using sqlx
                  DATABASE_URL="postgres://posthog:posthog@localhost:5432/posthog_persons" \
                    sqlx migrate run --source rust/persons_migrations/

            - name: Dump migrated schema
              if: github.event_name == 'push'
              run: |
                  set -e
                  set -o pipefail
                  # Dump schema + django_migrations data so Django knows which migrations are applied
                  # Run pg_dump inside container to ensure version match (host has pg_dump 16, container has 15)
                  (docker compose -f docker-compose.dev.yml exec -T db pg_dump --schema-only --clean -U posthog posthog && \
                   docker compose -f docker-compose.dev.yml exec -T db pg_dump --data-only --table=django_migrations -U posthog posthog) | gzip > schema.sql.gz
                  # Verify the dump is valid
                  gunzip -t schema.sql.gz

            - name: Upload migrated schema artifact
              if: github.event_name == 'push'
              uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6.0.0
              with:
                  name: migrated-schema
                  path: schema.sql.gz
                  retention-days: 90

            - name: Check migrations
              # Skip migration safety check on master push (no migration_files from path filter)
              if: github.event_name != 'push'
              run: |
                  DATABASE_URL="postgres://posthog:posthog@localhost:5432/posthog_persons" \
                    sqlx migrate info --source rust/persons_migrations/
                  python manage.py makemigrations --check --dry-run
                  git fetch origin master
                  # Check migration safety using old SQL-based checker (still uses stdin from git diff)
                  echo "${{ needs.changes.outputs.migrations_files }}" | grep -v migrations/0001_ | grep -v 'rust/persons_migrations' | python manage.py test_migrations_are_safe

            - name: Check CH migrations
              run: |
                  # Same as above, except now for CH looking at files that were added in posthog/clickhouse/migrations/
                  git diff --name-status origin/master..HEAD | grep "A\sposthog/clickhouse/migrations/" | grep -v README | awk '{print $2}' | python manage.py test_ch_migrations_are_safe

            - name: Install pnpm
              uses: pnpm/action-setup@a7487c7e89a18df4991f7f222e4898a00d66ddda # v4

            - name: Set up Node.js
              uses: actions/setup-node@v4
              with:
                  node-version: 22.22.0
                  cache: pnpm

            - name: Install package.json dependencies with pnpm
              run: pnpm install --frozen-lockfile

            - name: Check if OpenAPI types are up to date
              run: |
                  ./bin/hogli build:openapi
                  if ! git diff --exit-code; then
                      echo ""
                      echo "::error::OpenAPI types are out of date!"
                      echo ""
                      echo "The TypeScript API types in products/*/frontend/generated/ are auto-generated"
                      echo "from Django serializers and views. When you modify the backend API, you need"
                      echo "to regenerate these types."
                      echo ""
                      echo "To fix, run locally:  hogli build:openapi"
                      echo "Then commit the updated generated files."
                      echo ""
                      echo "More info: https://posthog.com/handbook/engineering/type-system"
                      echo ""
                      echo "Questions? #team-devex on Slack"
                      exit 1
                  fi

    django:
        needs: [changes, detect-snapshot-mode]
        if: needs.changes.outputs.backend == 'true'
        # increase for tmate testing
        timeout-minutes: 30

        name: Django tests â€“ ${{ matrix.segment }} (persons-on-events ${{ matrix.person-on-events && 'on' || 'off' }}), Py ${{ matrix.python-version }}, ${{ matrix.clickhouse-server-image }} (${{matrix.group}}/${{ matrix.concurrency }})
        runs-on: depot-ubuntu-latest

        strategy:
            fail-fast: false
            matrix:
                python-version: ['3.12.12']
                clickhouse-server-image: ['clickhouse/clickhouse-server:25.8.12.129']
                segment: ['Core']
                person-on-events: [false]
                # :NOTE: Keep concurrency and groups in sync
                concurrency: [40]
                group:
                    [
                        1,
                        2,
                        3,
                        4,
                        5,
                        6,
                        7,
                        8,
                        9,
                        10,
                        11,
                        12,
                        13,
                        14,
                        15,
                        16,
                        17,
                        18,
                        19,
                        20,
                        21,
                        22,
                        23,
                        24,
                        25,
                        26,
                        27,
                        28,
                        29,
                        30,
                        31,
                        32,
                        33,
                        34,
                        35,
                        36,
                        37,
                        38,
                        39,
                        40,
                    ]
                include:
                    - segment: 'Core'
                      person-on-events: true
                      clickhouse-server-image: 'clickhouse/clickhouse-server:25.8.12.129'
                      python-version: '3.12.12'
                      concurrency: 10
                      group: 1
                    - segment: 'Core'
                      person-on-events: true
                      clickhouse-server-image: 'clickhouse/clickhouse-server:25.8.12.129'
                      python-version: '3.12.12'
                      concurrency: 10
                      group: 2
                    - segment: 'Core'
                      person-on-events: true
                      clickhouse-server-image: 'clickhouse/clickhouse-server:25.8.12.129'
                      python-version: '3.12.12'
                      concurrency: 10
                      group: 3
                    - segment: 'Core'
                      person-on-events: true
                      clickhouse-server-image: 'clickhouse/clickhouse-server:25.8.12.129'
                      python-version: '3.12.12'
                      concurrency: 10
                      group: 4
                    - segment: 'Core'
                      person-on-events: true
                      clickhouse-server-image: 'clickhouse/clickhouse-server:25.8.12.129'
                      python-version: '3.12.12'
                      concurrency: 10
                      group: 5
                    - segment: 'Core'
                      person-on-events: true
                      clickhouse-server-image: 'clickhouse/clickhouse-server:25.8.12.129'
                      python-version: '3.12.12'
                      concurrency: 10
                      group: 6
                    - segment: 'Core'
                      person-on-events: true
                      clickhouse-server-image: 'clickhouse/clickhouse-server:25.8.12.129'
                      python-version: '3.12.12'
                      concurrency: 10
                      group: 7
                    - segment: 'Core'
                      person-on-events: true
                      clickhouse-server-image: 'clickhouse/clickhouse-server:25.8.12.129'
                      python-version: '3.12.12'
                      concurrency: 10
                      group: 8
                    - segment: 'Core'
                      person-on-events: true
                      clickhouse-server-image: 'clickhouse/clickhouse-server:25.8.12.129'
                      python-version: '3.12.12'
                      concurrency: 10
                      group: 9
                    - segment: 'Core'
                      person-on-events: true
                      clickhouse-server-image: 'clickhouse/clickhouse-server:25.8.12.129'
                      python-version: '3.12.12'
                      concurrency: 10
                      group: 10
                    - segment: 'Temporal'
                      person-on-events: false
                      clickhouse-server-image: 'clickhouse/clickhouse-server:25.8.12.129'
                      python-version: '3.12.12'
                      concurrency: 10
                      group: 1
                    - segment: 'Temporal'
                      person-on-events: false
                      clickhouse-server-image: 'clickhouse/clickhouse-server:25.8.12.129'
                      python-version: '3.12.12'
                      concurrency: 10
                      group: 2
                    - segment: 'Temporal'
                      person-on-events: false
                      clickhouse-server-image: 'clickhouse/clickhouse-server:25.8.12.129'
                      python-version: '3.12.12'
                      concurrency: 10
                      group: 3
                    - segment: 'Temporal'
                      person-on-events: false
                      clickhouse-server-image: 'clickhouse/clickhouse-server:25.8.12.129'
                      python-version: '3.12.12'
                      concurrency: 10
                      group: 4
                    - segment: 'Temporal'
                      person-on-events: false
                      clickhouse-server-image: 'clickhouse/clickhouse-server:25.8.12.129'
                      python-version: '3.12.12'
                      concurrency: 10
                      group: 5
                    - segment: 'Temporal'
                      person-on-events: false
                      clickhouse-server-image: 'clickhouse/clickhouse-server:25.8.12.129'
                      python-version: '3.12.12'
                      concurrency: 10
                      group: 6
                    - segment: 'Temporal'
                      person-on-events: false
                      clickhouse-server-image: 'clickhouse/clickhouse-server:25.8.12.129'
                      python-version: '3.12.12'
                      concurrency: 10
                      group: 7
                    - segment: 'Temporal'
                      person-on-events: false
                      clickhouse-server-image: 'clickhouse/clickhouse-server:25.8.12.129'
                      python-version: '3.12.12'
                      concurrency: 10
                      group: 8
                    - segment: 'Temporal'
                      person-on-events: false
                      clickhouse-server-image: 'clickhouse/clickhouse-server:25.8.12.129'
                      python-version: '3.12.12'
                      concurrency: 10
                      group: 9
                    - segment: 'Temporal'
                      person-on-events: false
                      clickhouse-server-image: 'clickhouse/clickhouse-server:25.8.12.129'
                      python-version: '3.12.12'
                      concurrency: 10
                      group: 10

        steps:
            - uses: actions/checkout@v6
              with:
                  fetch-depth: 1
                  repository: ${{ github.event.pull_request.head.repo.full_name }}
                  ref: ${{ github.event.pull_request.head.ref }}
                  clean: false
            - name: Clean up data directories with container permissions
              run: |
                  # Use docker to clean up files created by containers
                  [ -d "data" ] && docker run --rm -v "$(pwd)/data:/data" alpine sh -c "rm -rf /data/seaweedfs /data/minio" || true
              continue-on-error: true

            - name: 'Safeguard: ensure no stray Python modules at product root'
              run: |
                  echo "Checking that products/* only contain backend/, frontend/, or shared/ as Python code roots..."
                  BAD_FILES=$(find products -maxdepth 2 -type f -name "*.py" ! -path "*/backend/*" ! -name "__init__.py" ! -name "conftest.py" -o -maxdepth 2 -type d -name "migrations" ! -path "*/backend/*")
                  if [ -n "$BAD_FILES" ]; then
                    echo "âŒ Found Python code or migrations outside backend/:"
                    echo "$BAD_FILES"
                    echo "Please move these into the appropriate backend/ folder."
                    exit 1
                  fi
                  echo "âœ… No stray Python files or migrations found at product roots."

            # Pre-tests

            # Copies the fully versioned UDF xml file for use in CI testing
            - name: Stop/Start stack with Docker Compose
              shell: bash
              run: |
                  export CLICKHOUSE_SERVER_IMAGE=${{ matrix.clickhouse-server-image }}
                  export DOCKER_REGISTRY_PREFIX="us-east1-docker.pkg.dev/posthog-301601/mirror/"
                  cp posthog/user_scripts/latest_user_defined_function.xml docker/clickhouse/user_defined_function.xml

                  # Start docker compose in background
                  (
                      max_attempts=3
                      attempt=1
                      delay=5

                      while [ $attempt -le $max_attempts ]; do
                          echo "Attempt $attempt of $max_attempts to start stack..."

                          if docker compose -f docker-compose.dev.yml --profile '*' down && \
                            docker compose -f docker-compose.dev.yml --profile batch-exports up -d; then
                              echo "Stack started successfully"
                              exit 0
                          fi

                          echo "Failed to start stack on attempt $attempt"

                          if [ $attempt -lt $max_attempts ]; then
                              sleep_time=$((delay * 2 ** (attempt - 1)))
                              echo "Waiting ${sleep_time} seconds before retry..."
                              sleep $sleep_time
                          fi

                          attempt=$((attempt + 1))
                      done

                      echo "Failed to start stack after $max_attempts attempts"
                      exit 1
                  ) &

            - name: Add Kafka and ClickHouse to /etc/hosts
              shell: bash
              run: echo "127.0.0.1 kafka clickhouse" | sudo tee -a /etc/hosts

            - name: Set up Python
              uses: actions/setup-python@83679a892e2d95755f2dac6acb0bfd1e9ac5d548 # v6.1.0
              with:
                  python-version: ${{ matrix.python-version }}

            - name: Install uv
              id: setup-uv-tests
              uses: astral-sh/setup-uv@61cb8a9741eeb8a550a1b8544337180c0fc8476b # v7.2.0
              with:
                  enable-cache: true
                  version: 0.9.9

            - name: Install SAML (python3-saml) dependencies
              if: ${{ needs.changes.outputs.backend == 'true' && steps.setup-uv-tests.outputs.cache-hit != 'true' }}
              shell: bash
              run: |
                  sudo apt-get update && sudo apt-get install libxml2-dev libxmlsec1-dev libxmlsec1-openssl

            - name: Install Rust
              if: needs.changes.outputs.backend == 'true'
              uses: dtolnay/rust-toolchain@0b1efabc08b657293548b77fb76cc02d26091c7e
              with:
                  toolchain: 1.91.1
                  components: cargo

            - name: Cache Rust dependencies
              if: needs.changes.outputs.backend == 'true'
              uses: Swatinem/rust-cache@779680da715d629ac1d338a641029a2f4372abb5 # v2.8.2

            - name: Install sqlx-cli
              if: needs.changes.outputs.backend == 'true'
              run: |
                  cargo install sqlx-cli --version 0.8.0 --features postgres --no-default-features --locked

            - name: Determine if hogql-parser has changed compared to master
              shell: bash
              id: hogql-parser-diff
              run: |
                  git fetch --no-tags --prune --depth=1 origin master
                  changed=$(git diff --quiet HEAD origin/master -- common/hogql_parser/ && echo "false" || echo "true")
                  echo "changed=$changed" >> $GITHUB_OUTPUT

            - name: Install pnpm
              uses: pnpm/action-setup@41ff72655975bd51cab0327fa583b6e92b6d3061 # v4.2.0

            - name: Set up Node.js
              uses: actions/setup-node@395ad3262231945c25e8478fd5baf05154b1d79f # v6.1.0
              with:
                  node-version: 22.22.0
                  cache: pnpm

            # tests would intermittently fail in GH actions
            # with exit code 134 _after passing_ all tests
            # this appears to fix it
            # absolute wild tbh https://stackoverflow.com/a/75503402
            - uses: tlambert03/setup-qt-libs@19e4ef2d781d81f5f067182e228b54ec90d23b76 # v1.8

            - name: Install plugin_transpiler
              shell: bash
              run: |
                  pnpm --filter=@posthog/plugin-transpiler... install --frozen-lockfile
                  bin/turbo --filter=@posthog/plugin-transpiler build

            - name: Install Python dependencies
              shell: bash
              run: |
                  UV_PROJECT_ENVIRONMENT=$pythonLocation uv sync --frozen --dev

            - name: Install the working version of hogql-parser
              if: ${{ needs.changes.outputs.backend == 'true' && steps.hogql-parser-diff.outputs.changed == 'true' }}
              shell: bash
              # This is not cached currently, as it's important to build the current HEAD version of hogql-parser if it has
              # changed (requirements.txt has the already-published version)
              run: |
                  sudo apt-get install libboost-all-dev unzip cmake curl uuid pkg-config
                  curl https://www.antlr.org/download/antlr4-cpp-runtime-4.13.1-source.zip --output antlr4-source.zip
                  # Check that the downloaded archive is the expected runtime - a security measure
                  anltr_known_md5sum="c875c148991aacd043f733827644a76f"
                  antlr_found_ms5sum="$(md5sum antlr4-source.zip | cut -d' ' -f1)"
                  if [[ "$anltr_known_md5sum" != "$antlr_found_ms5sum" ]]; then
                      echo "Unexpected MD5 sum of antlr4-source.zip!"
                      echo "Known: $anltr_known_md5sum"
                      echo "Found: $antlr_found_ms5sum"
                      exit 64
                  fi
                  unzip antlr4-source.zip -d antlr4-source && cd antlr4-source
                  cmake .
                  DESTDIR=out make install
                  sudo cp -r out/usr/local/include/antlr4-runtime /usr/include/
                  sudo cp out/usr/local/lib/libantlr4-runtime.so* /usr/lib/
                  sudo ldconfig
                  cd ..
                  pip install ./common/hogql_parser

            - name: Set up needed files
              shell: bash
              run: |
                  mkdir -p frontend/dist
                  touch frontend/dist/index.html
                  touch frontend/dist/layout.html
                  touch frontend/dist/exporter.html
                  ./bin/download-mmdb

            - name: Wait for services to be available
              shell: bash
              run: |
                  bin/check_kafka_clickhouse_up
                  bin/check_postgres_up

            - name: Wait for Temporal
              if: ${{ needs.changes.outputs.backend == 'true' && matrix.segment == 'Temporal' }}
              shell: bash
              run: |
                  bin/check_temporal_up

            - name: Determine if --snapshot-update should be on
              # UPDATE mode: human commits - update snapshots
              # CHECK mode: bot commits (after snapshot update) - verify snapshots match exactly
              # persons-on-events: always update (we ignore snapshot divergence there)
              if: ${{ needs.changes.outputs.backend == 'true' && (needs.detect-snapshot-mode.outputs.mode == 'update' || matrix.person-on-events) }}
              shell: bash
              run: echo "PYTEST_ARGS=--snapshot-update" >> $GITHUB_ENV

            # Tests
            - name: Run Core tests
              id: run-core-tests
              if: ${{ needs.changes.outputs.backend == 'true' && matrix.segment == 'Core' }}
              env:
                  PERSON_ON_EVENTS_V2_ENABLED: ${{ matrix.person-on-events && 'true' || 'false' }}
              shell: bash
              run: | # async_migrations covered in ci-async-migrations.yml
                  set +e
                  pytest ${{
                      matrix.person-on-events
                      && './posthog/clickhouse/ ./posthog/queries/ ./posthog/api/test/test_insight* ./posthog/api/test/dashboards/test_dashboard.py'
                      || 'posthog products'
                  }} ${{ matrix.person-on-events && 'ee/clickhouse/' || 'ee/' }} -m "not async_migrations" \
                      --ignore=posthog/temporal \
                      --ignore=posthog/dags \
                      --ignore=products/**/dags \
                      --ignore=products/batch_exports/backend/tests/temporal \
                      --ignore=common/hogvm/python/test \
                      ${{ matrix.person-on-events && '--ignore=posthog/hogql_queries' || '' }} \
                      ${{ matrix.person-on-events && '--ignore=posthog/hogql' || '' }} \
                      --splits ${{ matrix.concurrency }} --group ${{ matrix.group }} \
                      --durations=1000 --durations-min=1.0 --store-durations \
                      --splitting-algorithm=duration_based_chunks \
                      --reruns 2 --reruns-delay 1 \
                      $PYTEST_ARGS
                  exit_code=$?
                  set -e
                  if [ $exit_code -eq 5 ]; then
                      echo "No tests collected for this shard, this is expected when splitting tests"
                      exit 0
                  else
                      exit $exit_code
                  fi

            # Uncomment this code to create an ssh-able console so you can debug issues with github actions
            # (Consider changing the timeout in ci-backend.yml to have more time)
            # - name: Setup tmate session
            #   if: failure()
            #   uses: mxschmitt/action-tmate@v3

            - name: Run /decide read replica tests
              id: run-decide-read-replica-tests
              if: ${{ needs.changes.outputs.backend == 'true' && matrix.segment == 'Core' && matrix.group == 1 && !matrix.person-on-events }}
              env:
                  POSTHOG_DB_NAME: posthog
                  READ_REPLICA_OPT_IN: 'decide,PersonalAPIKey, local_evaluation'
                  POSTHOG_POSTGRES_READ_HOST: localhost
                  POSTHOG_DB_PASSWORD: posthog
                  POSTHOG_DB_USER: posthog
              shell: bash
              run: |
                  pytest posthog/api/test/test_decide.py::TestDecideUsesReadReplica \
                      --durations=1000 --durations-min=1.0 \
                      --reruns 2 --reruns-delay 1 \
                      $PYTEST_ARGS

            - name: Run Temporal tests
              id: run-temporal-tests
              if: ${{ needs.changes.outputs.backend == 'true' && matrix.segment == 'Temporal' }}
              shell: bash
              env:
                  AWS_S3_ALLOW_UNSAFE_RENAME: 'true'
                  MODAL_TOKEN_ID: ${{ needs.changes.outputs.tasks_temporal == 'true' && secrets.MODAL_TOKEN_ID || '' }}
                  MODAL_TOKEN_SECRET: ${{ needs.changes.outputs.tasks_temporal == 'true' && secrets.MODAL_TOKEN_SECRET || '' }}
              run: |
                  set +e
                  pytest posthog/temporal products/batch_exports/backend/tests/temporal products/tasks/backend/temporal -m "not async_migrations" \
                      --splits ${{ matrix.concurrency }} --group ${{ matrix.group }} \
                      --durations=100 --durations-min=1.0 --store-durations \
                      --splitting-algorithm=duration_based_chunks \
                      --reruns 2 --reruns-delay 1 \
                      $PYTEST_ARGS
                  exit_code=$?
                  set -e
                  if [ $exit_code -eq 5 ]; then
                      echo "No tests collected for this shard, this is expected when splitting tests"
                      exit 0
                  else
                      exit $exit_code
                  fi

            # Post tests
            - name: Show docker compose logs on failure
              if: failure() && (needs.changes.outputs.backend == 'true' && steps.run-core-tests.outcome != 'failure' && steps.run-decide-read-replica-tests.outcome != 'failure' && steps.run-temporal-tests.outcome != 'failure')
              shell: bash
              run: docker compose -f docker-compose.dev.yml logs

            - name: Upload updated timing data as artifacts
              uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6.0.0
              # Only upload timing data on main branch to avoid artifact bloat from PRs
              if: ${{ github.ref == 'refs/heads/master' && needs.changes.outputs.backend == 'true' && !matrix.person-on-events && matrix.clickhouse-server-image == 'clickhouse/clickhouse-server:25.8.12.129' }}
              with:
                  name: timing_data-${{ matrix.segment }}-${{ matrix.group }}
                  path: .test_durations
                  include-hidden-files: true
                  retention-days: 2

            - name: Verify new snapshots for flakiness
              # Only in UPDATE mode - CHECK mode doesn't update snapshots
              if: ${{ needs.detect-snapshot-mode.outputs.mode == 'update' && github.event.pull_request.head.repo.full_name == 'PostHog/posthog' && needs.changes.outputs.backend == 'true' && !matrix.person-on-events }}
              shell: bash
              run: |
                  .github/scripts/verify-new-snapshots.sh

            - name: Generate snapshot patch
              # Only in UPDATE mode - CHECK mode verifies snapshots match exactly
              if: ${{ needs.detect-snapshot-mode.outputs.mode == 'update' && github.event.pull_request.head.repo.full_name == 'PostHog/posthog' && needs.changes.outputs.backend == 'true' && !matrix.person-on-events }}
              shell: bash
              run: |
                  mkdir -p /tmp/patches
                  # Stage any new/modified .ambr files so they appear in git diff
                  git add -N '*.ambr' || true
                  # Generate patch if there are changes
                  if ! git diff --quiet '*.ambr' 2>/dev/null; then
                    git diff --binary --full-index '*.ambr' > /tmp/patches/backend-${{ matrix.segment }}-${{ matrix.group }}.patch
                    echo "Generated patch with $(wc -l < /tmp/patches/backend-${{ matrix.segment }}-${{ matrix.group }}.patch) lines"
                  else
                    echo "No snapshot changes to patch"
                  fi

            - name: Upload snapshot patch
              # Only in UPDATE mode
              if: ${{ needs.detect-snapshot-mode.outputs.mode == 'update' && github.event.pull_request.head.repo.full_name == 'PostHog/posthog' && needs.changes.outputs.backend == 'true' && !matrix.person-on-events }}
              uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6.0.0
              with:
                  name: snapshot-patch-${{ matrix.segment }}-${{ matrix.group }}
                  path: /tmp/patches/
                  if-no-files-found: ignore
                  retention-days: 1

            - name: Archive email renders
              uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6.0.0
              if: needs.changes.outputs.backend == 'true' && matrix.segment == 'Core' && !matrix.person-on-events
              with:
                  name: email_renders-${{ github.sha }}-${{ github.run_attempt }}-${{ matrix.segment }}-${{ matrix.person-on-events }}-${{ matrix.group }}
                  path: posthog/tasks/test/__emails__
                  retention-days: 1

    # Aggregate and commit snapshot changes from all matrix jobs
    handle-snapshots:
        name: Commit snapshot changes
        needs: [changes, detect-snapshot-mode, django]
        # Only in UPDATE mode - CHECK mode verifies snapshots match exactly
        # Run even if some matrix jobs failed (to commit snapshots from passing jobs)
        if: ${{ always() && needs.detect-snapshot-mode.outputs.mode == 'update' && needs.changes.outputs.backend == 'true' && github.event.pull_request.head.repo.full_name == 'PostHog/posthog' }}
        runs-on: ubuntu-latest
        permissions:
            contents: write
            pull-requests: write
        steps:
            # Use GitHub app token so Actions run after commiting updated snapshots
            - name: Get app token
              id: app-token
              uses: getsentry/action-github-app-token@d4b5da6c5e37703f8c3b3e43abb5705b46e159cc # v3.0.0
              with:
                  app_id: ${{ secrets.GH_APP_POSTHOG_TESTS_APP_ID }}
                  private_key: ${{ secrets.GH_APP_POSTHOG_TESTS_PRIVATE_KEY }}

            - name: Checkout
              uses: actions/checkout@v6
              with:
                  ref: ${{ github.event.pull_request.head.sha }}
                  repository: ${{ github.event.pull_request.head.repo.full_name }}
                  token: ${{ steps.app-token.outputs.token }}
                  fetch-depth: 1

            - name: Download all snapshot patches
              id: download-patches
              continue-on-error: true
              uses: actions/download-artifact@37930b1c2abaa49bbe596cd826c3c89aef350131 # v7.0.0
              with:
                  pattern: snapshot-patch-*
                  path: /tmp/snapshot-patches
                  merge-multiple: true

            - name: Check for patches
              id: check-patches
              run: |
                  if [ "${{ steps.download-patches.outcome }}" == "failure" ] || [ ! -d /tmp/snapshot-patches ]; then
                    echo "has-patches=false" >> $GITHUB_OUTPUT
                    echo "No snapshot patches found"
                    exit 0
                  fi

                  if [ -n "$(find /tmp/snapshot-patches -name '*.patch' -type f -size +0c 2>/dev/null)" ]; then
                    echo "has-patches=true" >> $GITHUB_OUTPUT
                    echo "Found patches:"
                    ls -la /tmp/snapshot-patches/
                  else
                    echo "has-patches=false" >> $GITHUB_OUTPUT
                    echo "Patch files empty or missing - no snapshot changes"
                  fi

            - name: Commit snapshots
              if: steps.check-patches.outputs.has-patches == 'true'
              uses: ./.github/actions/commit-snapshots
              with:
                  workflow-type: backend
                  patch-path: /tmp/snapshot-patches
                  snapshot-path: '.'
                  commit-message: 'test(backend): update query snapshots'
                  pr-number: ${{ github.event.pull_request.number }}
                  repository: ${{ github.repository }}
                  commit-sha: ${{ github.event.pull_request.head.sha }}
                  branch-name: ${{ github.event.pull_request.head.ref }}
                  github-token: ${{ steps.app-token.outputs.token }}

    # Job just to collate the status of the matrix jobs for requiring passing status
    # Must depend on handle-snapshots to prevent auto-merge before commits complete
    django_tests:
        needs: [django, check-migrations, async-migrations, handle-snapshots]
        name: Django Tests Pass
        runs-on: ubuntu-latest
        if: always()
        steps:
            - name: Check matrix outcome
              run: |
                  # The `needs.django.result` will be 'success' only if all jobs in the matrix succeeded.
                  # Otherwise, it will be 'failure'.
                  if [[ "${{ needs.django.result }}" != "success" && "${{ needs.django.result }}" != "skipped" ]]; then
                    echo "One or more jobs in the Django test matrix failed."
                    exit 1
                  fi

                  # Check migration validation - must pass for Django Tests to be green
                  if [[ "${{ needs.check-migrations.result }}" != "success" && "${{ needs.check-migrations.result }}" != "skipped" ]]; then
                    echo "Migration checks failed."
                    exit 1
                  fi

                  # Check async migrations - must pass for Django Tests to be green
                  if [[ "${{ needs.async-migrations.result }}" != "success" && "${{ needs.async-migrations.result }}" != "skipped" ]]; then
                    echo "Async migrations tests failed."
                    exit 1
                  fi

                  # Check handle-snapshots result (OK if skipped, but fail if it failed)
                  if [[ "${{ needs.handle-snapshots.result }}" == "failure" ]]; then
                    echo "Snapshot commit job failed."
                    exit 1
                  fi

                  echo "All checks passed."

    async-migrations:
        name: Async migrations tests -  ${{ matrix.clickhouse-server-image }}
        needs: [changes]
        strategy:
            fail-fast: false
            matrix:
                clickhouse-server-image: ['clickhouse/clickhouse-server:25.8.12.129']
        if: needs.changes.outputs.backend == 'true'
        runs-on: depot-ubuntu-latest
        steps:
            - name: 'Checkout repo'
              uses: actions/checkout@v6
              with:
                  fetch-depth: 1
                  clean: false
            - name: Clean up data directories with container permissions
              run: |
                  # Use docker to clean up files created by containers
                  [ -d "data" ] && docker run --rm -v "$(pwd)/data:/data" alpine sh -c "rm -rf /data/seaweedfs /data/minio" || true
              continue-on-error: true

            - name: Start stack with Docker Compose
              run: |
                  export CLICKHOUSE_SERVER_IMAGE_VERSION=${{ matrix.clickhouse-server-image }}
                  docker compose -f docker-compose.dev.yml down
                  docker compose -f docker-compose.dev.yml up -d &

            - name: Set up Python
              uses: actions/setup-python@83679a892e2d95755f2dac6acb0bfd1e9ac5d548 # v6.1.0
              with:
                  python-version-file: 'pyproject.toml'

            - name: Install uv
              id: setup-uv-async
              uses: astral-sh/setup-uv@61cb8a9741eeb8a550a1b8544337180c0fc8476b # v7.2.0
              with:
                  enable-cache: true
                  version: 0.9.9

            - name: Install SAML (python3-saml) dependencies
              if: steps.setup-uv-async.outputs.cache-hit != 'true'
              run: |
                  sudo apt-get update
                  sudo apt-get install libxml2-dev libxmlsec1-dev libxmlsec1-openssl

            - name: Install Rust
              uses: dtolnay/rust-toolchain@0b1efabc08b657293548b77fb76cc02d26091c7e
              with:
                  toolchain: 1.91.1
                  components: cargo

            - name: Cache Rust dependencies
              uses: Swatinem/rust-cache@779680da715d629ac1d338a641029a2f4372abb5 # v2.8.2

            - name: Install sqlx-cli
              run: |
                  cargo install sqlx-cli --version 0.8.0 --features postgres --no-default-features --locked

            - name: Install python dependencies
              shell: bash
              run: |
                  UV_PROJECT_ENVIRONMENT=$pythonLocation uv sync --frozen --dev

            - name: Add Kafka and ClickHouse to /etc/hosts
              run: sudo echo "127.0.0.1 kafka clickhouse" | sudo tee -a /etc/hosts

            - name: Set up needed files
              run: |
                  mkdir -p frontend/dist
                  touch frontend/dist/index.html
                  touch frontend/dist/layout.html
                  touch frontend/dist/exporter.html

            - name: Wait for services to be available
              shell: bash
              run: |
                  bin/check_kafka_clickhouse_up
                  bin/check_postgres_up

            - name: Run async migrations tests
              run: |
                  pytest -m "async_migrations" --reruns 2 --reruns-delay 1

    calculate-running-time:
        name: Calculate running time
        needs: [django_tests, async-migrations]
        runs-on: ubuntu-latest
        if: # Run on pull requests to PostHog/posthog + on PostHog/posthog outside of PRs - but never on forks
            always() && (
            (github.event_name == 'pull_request' && github.event.pull_request.head.repo.full_name == 'PostHog/posthog') ||
            (github.event_name != 'pull_request' && github.repository == 'PostHog/posthog'))
        steps:
            - name: Capture running time to PostHog
              uses: PostHog/posthog-github-action@eea1405eeb2d1d2259f0ee0b44d7b152869e6840 # v1.1.1
              with:
                  posthog-token: ${{ secrets.POSTHOG_API_TOKEN }}
                  event: 'posthog-ci-running-time'
                  capture-run-duration: true
                  capture-job-durations: true
                  github-token: ${{ secrets.GITHUB_TOKEN }}
                  status-job: 'Django Tests Pass'
                  runner: 'depot'

    llm-gateway:
        name: LLM Gateway Tests
        needs: changes
        if: needs.changes.outputs.llm_gateway == 'true'
        runs-on: ubuntu-latest
        timeout-minutes: 10
        defaults:
            run:
                working-directory: services/llm-gateway

        steps:
            - uses: actions/checkout@v6

            - name: Set up Python
              uses: actions/setup-python@83679a892e2d95755f2dac6acb0bfd1e9ac5d548 # v6.1.0
              with:
                  python-version: '3.12'

            - name: Install uv
              uses: astral-sh/setup-uv@61cb8a9741eeb8a550a1b8544337180c0fc8476b # v7.2.0

            - name: Install dependencies
              run: uv sync --frozen --dev

            - name: Run tests
              env:
                  OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
                  ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
              run: uv run pytest tests/ -v --tb=short
